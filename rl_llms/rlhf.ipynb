{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### RLHF Demo\n",
    "Run post training on a pretrained GPT-2 model to understand RLHF. Steps will be SFT -> train reward model -> run grpo on pretrained llm on reward model. Rather than using TRL, I will be implementing grpo myself. Implementation will start with single gpu and then scaled to distributed system."
   ],
   "id": "6a402223a4106937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:16:41.586689Z",
     "start_time": "2025-08-22T01:16:37.565039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompt = \"The usual weather in California is\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=20,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ],
   "id": "a36fde68ad87b98c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The usual weather in California is a bit of a mess.\n",
      "\n",
      "The sun is shining, but\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Supervised fine tuning\n",
    "\n",
    "What I need to do:\n",
    "1. Preprocess data into chat template with EOS token. Ensure data is padded and make sure batches are truncated to fit context length.\n",
    "2. Iterate through every batch and for each one calculate the loss (ONLY on the last assistant completion so the model learns prompt prediction). We use cross entropy btw.\n",
    "3. Run a number of epochs on it.\n",
    "4. Keep single threaded till we implement grpo as well."
   ],
   "id": "fb9e9f51ec732eb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:17:00.212480Z",
     "start_time": "2025-08-22T01:16:46.750112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "import copy\n",
    "\n",
    "# ---------------\n",
    "# hyperparameters / config\n",
    "num_epochs = 5\n",
    "batch_size = 2\n",
    "lr = 5e-5\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# ---------------\n",
    "\n",
    "# create dataset train/val/test splits\n",
    "train_sft_dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split='train_sft').select(range(1000))\n",
    "train_split_size = int(0.9 * len(train_sft_dataset))\n",
    "train_split = train_sft_dataset.select(range(train_split_size))\n",
    "val_split = train_sft_dataset.select(range(train_split_size, len(train_sft_dataset)))\n",
    "\n",
    "# create chat template for tokenizer to use, gpt2 uses eos token so we need to add that as well\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{%- for message in messages %}\n",
    "    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|im_start|>assistant\\n' }}\n",
    "{%- else %}\n",
    "    {{- eos_token }}\n",
    "{%- endif %}\n",
    "\"\"\"\n",
    "\n",
    "# preprocess data and create dataloader\n",
    "ending_msg_token_len = len(tokenizer.encode('<|im_end|>\\n'))\n",
    "def add_chat_tem(example):\n",
    "    # truncate conversations by message boundary\n",
    "    # truncation strategy:\n",
    "    # 1. remove entire user/assistant pairs until it fits context window in fifo order but keep system prompt\n",
    "    # 2. if you have one left, and it still isn't fitting into the context window, remove the tokens till it does\n",
    "    example['og_messages'] = copy.deepcopy(example['messages'])\n",
    "    max_context_length = model.config.max_position_embeddings\n",
    "    has_system = 1 if example['messages'][0]['role'] == 'system' else 0\n",
    "    while True:\n",
    "        enc_chat_tem_ex = tokenizer.apply_chat_template(example['messages'], tokenize=True, add_special_tokens=False)\n",
    "        diff = len(enc_chat_tem_ex) - max_context_length\n",
    "        if diff <= 0:\n",
    "            example['exclude'] = False\n",
    "            break\n",
    "        elif len(example['messages']) // 2 <= 1:\n",
    "            example['exclude'] = True\n",
    "            break\n",
    "\n",
    "        del example['messages'][0 + has_system]\n",
    "        del example['messages'][0 + has_system]\n",
    "\n",
    "\n",
    "    # convert to chat template and keep track of # of tokens in last generation\n",
    "    enc_chat_tem_ex = tokenizer.apply_chat_template(example['messages'], tokenize=True, add_special_tokens=False)\n",
    "    example['input_ids'] = enc_chat_tem_ex\n",
    "    end_size = (len(tokenizer.encode(example['messages'][-1]['content'], add_special_tokens=False)) + ending_msg_token_len)\n",
    "    last_gen_start_ind = len(enc_chat_tem_ex) - end_size\n",
    "    example['last_gen_start_ind'] = last_gen_start_ind\n",
    "    return example\n",
    "\n",
    "exclude_filter = lambda x: x['exclude'] == False\n",
    "train_split = train_split.map(add_chat_tem).filter(exclude_filter)\n",
    "val_split = val_split.map(add_chat_tem).filter(exclude_filter)\n",
    "\n",
    "# create custom collator for sft\n",
    "class DataCollatorForSFT(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        last_gen_start_inds = [example['last_gen_start_ind'] for example in features]\n",
    "        features = [{'input_ids': example['input_ids']} for example in features]\n",
    "        batch = super().__call__(features, return_tensors=return_tensors)\n",
    "        # scrappy but just assume we're calling with return_tensors='pt'\n",
    "        batch['last_gen_start_inds'] = torch.tensor(last_gen_start_inds)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSFT(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_split,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_split,\n",
    "    batch_size=len(val_split),\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "val_batch = next(iter(val_dataloader))"
   ],
   "id": "b8e444928958616b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/900 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1658 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 900/900 [00:08<00:00, 103.69 examples/s]\n",
      "Filter: 100%|██████████| 900/900 [00:00<00:00, 7176.06 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:01<00:00, 87.12 examples/s]\n",
      "Filter: 100%|██████████| 100/100 [00:00<00:00, 6494.34 examples/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:17:19.895076Z",
     "start_time": "2025-08-22T01:17:19.892564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pprint(val_split)\n",
    "pprint(train_split)\n",
    "# pprint(val_batch)"
   ],
   "id": "6ed83db81e20cf58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'messages', 'og_messages', 'exclude', 'input_ids', 'last_gen_start_ind'],\n",
      "    num_rows: 99\n",
      "})\n",
      "Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'messages', 'og_messages', 'exclude', 'input_ids', 'last_gen_start_ind'],\n",
      "    num_rows: 885\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:17:24.870688Z",
     "start_time": "2025-08-22T01:17:24.869089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# keep running list of training + val loss for logging\n",
    "losses_per_epoch = []\n",
    "latest_epoch_losses = []"
   ],
   "id": "782cd2195efbc066",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:23:50.221648Z",
     "start_time": "2025-08-22T01:20:43.666728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "BxT -> BxTxP\n",
    "A couple of issues I need to work out:\n",
    "1. Padding inputs_ids in each prompt in the match correctly and ensuring attention mask is right\n",
    "2. After we feed the prompt, we need to remove everything but the corresponding amount of tokens from the end of the prompt corresponding to the last assistant generation. We need to calculate labels that way simimlarly.\n",
    "3. Calculate the loss for the sliced part (but each batch inner matrix is different size so need to think about that).\n",
    "4. Backprop on that to train the model.\n",
    "'''\n",
    "\n",
    "def calc_model_loss(batch):\n",
    "    input_ids = batch['input_ids']\n",
    "    labels = batch['labels']\n",
    "    last_gen_start_inds = batch['last_gen_start_inds']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    # print(\"SEQUENCE LENGTH IN BATCH:\", input_ids.shape[1], \"MAX SEQUENCE LENGTH ALLOWED:\", model.config.max_position_embeddings)\n",
    "    # mask labels that aren't included in last gen\n",
    "    mask = torch.arange(input_ids.shape[1]) < last_gen_start_inds[:, None]\n",
    "    labels[mask] = -100\n",
    "\n",
    "    # run forward pass and calc loss\n",
    "    model_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    # calculate loss\n",
    "    B, T, C = model_outputs.logits.shape\n",
    "    logits = model_outputs.logits.view(B*T, C)\n",
    "    labels = labels.view(B*T)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# training run\n",
    "model.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    step = 0\n",
    "    print(f\"\\n Starting epoch: {epoch}\")\n",
    "    for batch in train_dataloader:\n",
    "        step += 1\n",
    "        # calculate training loss\n",
    "        training_loss = calc_model_loss(batch)\n",
    "        optimizer.zero_grad()\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate val loss\n",
    "        with torch.no_grad():\n",
    "            val_loss = calc_model_loss(val_batch)\n",
    "\n",
    "        latest_epoch_losses.append((training_loss.item(), val_loss.item()))\n",
    "        print(f\"epoch: {epoch} | step: {step} | training loss: {training_loss} | validation loss: {val_loss}\")\n",
    "        break\n",
    "    break\n",
    "\n",
    "    losses_per_epoch.append(latest_epoch_losses)\n",
    "    latest_epoch_losses = []\n",
    "\n"
   ],
   "id": "71d7c703271a5893",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting epoch: 1\n",
      "epoch: 1 | step: 1 | training loss: 7.863521099090576 | validation loss: 6.813094139099121\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create the reward model",
   "id": "6e77f8f5907b1bc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:41:25.928628Z",
     "start_time": "2025-08-20T19:41:25.919996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The reward model generates a single logit representing the probability of that response (we use Bradley-Terry model of preferences)\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size * 4)\n",
    "        self.fc2 = nn.Linear(input_size * 4, 1)\n",
    "\n",
    "'''\n",
    "Preference training loop steps:\n",
    "1. Iterate through synthetic data of preferences.\n",
    "2. Use the final hidden state last embedding as input to reward network.\n",
    "3. Do that for both (otherwise you can use synthetic data and feed into the network the prompt + the response)\n",
    "4. Use that to get the embeddings for both (just do one pass)\n",
    "5. Calculate the loss based on Bradley-Terry and do backprop on the reward model network (you can mean the sums so you do it batchwise)\n",
    "\n",
    "The idea we'll use is take the prompt reward string to get rewrd but only train on prompt to avoid mismatch problem.\n",
    "'''\n"
   ],
   "id": "daf721071ca1e02e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPreference training loop steps:\\n1. Iterate through synthetic data of preferences.\\n2. Use the final hidden state last embedding as input to reward network.\\n3. Do that for both (otherwise you can use synthetic data and feed into the network the prompt + the response)\\n4. Use that to get the embeddings for both (just do one pass)\\n5. Calculate the loss based on Bradley-Terry and do backprop on the reward model network (you can mean the sums so you do it batchwise)\\n\\nThe idea we'll use is take the prompt reward string to get rewrd but only train on prompt to avoid mismatch problem.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:58:32.609858Z",
     "start_time": "2025-08-20T19:58:31.662841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_data = load_dataset('HuggingFaceH4/no_robots')['train']\n",
    "train_data[0]"
   ],
   "id": "d09fc1eced2f7d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert’s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. “We can’t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,” says Rinkert.',\n",
       " 'prompt_id': '627a77298cf96a309aa35a62207c4164e22a66f6db79119506228f28ddc0f947',\n",
       " 'messages': [{'content': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert’s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. “We can’t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,” says Rinkert.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.',\n",
       "   'role': 'assistant'}],\n",
       " 'category': 'Summarize'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6190e2927bd84be6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
