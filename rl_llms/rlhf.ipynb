{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### RLHF Demo\n",
    "Run post training on a pretrained GPT-2 model to understand RLHF. Steps will be SFT -> train reward model -> run grpo on pretrained llm on reward model. Rather than using TRL, I will be implementing grpo myself. Implementation will start with single gpu and then scaled to distributed system."
   ],
   "id": "6a402223a4106937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T00:36:27.162345Z",
     "start_time": "2025-08-21T00:36:03.947157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompt = \"The usual weather in California is\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=1000,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ],
   "id": "a36fde68ad87b98c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The usual weather in California is a bit of a mess.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Supervised fine tuning\n",
    "\n",
    "What I need to do:\n",
    "1. Preprocess data into chat template with EOS token. Ensure data is padded and make sure batches are truncated to fit context length.\n",
    "2. Iterate through every batch and for each one calculate the loss (ONLY on the last assistant completion so the model learns prompt prediction). We use cross entropy btw.\n",
    "3. Run a number of epochs on it.\n",
    "4. Keep single threaded till we implement grpo as well."
   ],
   "id": "fb9e9f51ec732eb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:15:09.010722Z",
     "start_time": "2025-08-21T21:15:07.364859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "\n",
    "# ---------------\n",
    "# hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 2\n",
    "# ---------------\n",
    "\n",
    "# create dataset train/val/test splits\n",
    "train_sft_dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split='train_sft').select(range(1000))\n",
    "train_split_size = int(0.9 * len(train_sft_dataset))\n",
    "train_split = train_sft_dataset.select(range(train_split_size))\n",
    "val_split = train_sft_dataset.select(range(train_split_size, len(train_sft_dataset)))\n",
    "\n",
    "# create chat template for tokenizer to use, gpt2 uses eos token so we need to add that as well\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{%- for message in messages %}\n",
    "    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|im_start|>assistant\\n' }}\n",
    "{%- else %}\n",
    "    {{- eos_token }}\n",
    "{%- endif %}\n",
    "\"\"\"\n",
    "\n",
    "# preprocess data and create dataloader\n",
    "ending_msg_token_len = len(tokenizer.encode('<|im_end|>\\n'))\n",
    "def add_chat_tem(example):\n",
    "    # convert to chat template and keep track of # of tokens in last generation\n",
    "    enc_chat_tem_ex = tokenizer.apply_chat_template(example['messages'], tokenize=True, add_special_tokens=False)\n",
    "    example['input_ids'] = enc_chat_tem_ex\n",
    "    end_size = (len(tokenizer.encode(example['messages'][-1]['content'], add_special_tokens=False)) + ending_msg_token_len)\n",
    "    last_gen_start_ind = len(enc_chat_tem_ex) - end_size\n",
    "    example['last_gen_start_ind'] = last_gen_start_ind\n",
    "    return example\n",
    "\n",
    "\n",
    "train_split = train_split.map(add_chat_tem)\n",
    "val_split = val_split.map(add_chat_tem)\n",
    "\n",
    "# create custom collator for sft\n",
    "class DataCollatorForSFT(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        last_gen_start_inds = [example['last_gen_start_ind'] for example in features]\n",
    "        features = [{'input_ids': example['input_ids']} for example in features]\n",
    "        batch = super().__call__(features, return_tensors=return_tensors)\n",
    "        # scrappy but just assume we're calling with return_tensors='pt'\n",
    "        batch['last_gen_start_inds'] = torch.tensor(last_gen_start_inds)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSFT(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_split,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")"
   ],
   "id": "b8e444928958616b",
   "outputs": [],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T21:15:09.022870Z",
     "start_time": "2025-08-21T21:15:09.015702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "BxT -> BxTxP\n",
    "A couple of issues I need to work out:\n",
    "1. Padding inputs_ids in each prompt in the match correctly and ensuring attention mask is right\n",
    "2. After we feed the prompt, we need to remove everything but the corresponding amount of tokens from the end of the prompt corresponding to the last assistant generation. We need to calculate labels that way simimlarly.\n",
    "3. Calculate the loss for the sliced part (but each batch inner matrix is different size so need to think about that).\n",
    "4. Backprop on that to train the model.\n",
    "'''\n",
    "\n",
    "# training run\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        last_gen_start_inds = batch['last_gen_start_inds']\n",
    "\n",
    "        print(\"shape:\", input_ids.shape)\n",
    "\n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "        first_prompt_input_ids = batch['input_ids'][0]\n",
    "        print(len(first_prompt_input_ids))\n",
    "        first_prompt_last_gen_token_len = batch['last_gen_start_inds'][0].item()\n",
    "        print(first_prompt_last_gen_token_len)\n",
    "\n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        print(tokenizer.decode(first_prompt_input_ids[:first_prompt_last_gen_token_len]))\n",
    "\n",
    "\n",
    "        # mask labels that aren't included in last gen\n",
    "        mask = torch.arange(input_ids.shape[1]) < last_gen_start_inds[:, None]\n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        print(mask)\n",
    "        input_ids[mask] = -100\n",
    "\n",
    "\n",
    "        break\n",
    "    break"
   ],
   "id": "71d7c703271a5893",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURES LENGTH: 2\n",
      "DIFFERENT LENGTHS: [1106, 1581]\n",
      "BATCH SIZE BEFORE: 2\n",
      "BATCH SIZE IN COLLATOR:  2\n",
      "shape: torch.Size([2, 1581])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1581\n",
      "820\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<|im_start|>user\n",
      "If you are not comfortable using spreadsheets and running scripts to create shared folders with your students, there is another simpler way. It helps if you do this at the beginning of a class or at least all together as a class so you can help people who get stuck. Once they share a folder with you, anything they put in the folder will be shared with you so that you can edit it.\n",
      "Have everyone go to their Google Drive page. Select “Create” and then click “Folder”.\n",
      "The students should name their folder a specific way so that you can easily sort them later. I would suggest “Period-LastName-FirstName-Semester”.\n",
      "They should then select the “Share” button.\n",
      "They will have to start typing in your name but it should automatically bring up your account to share with. REMIND THEM TO UNCLICK THE “NOTIFY PEOPLE VIA EMAIL” box otherwise you will receive a lot of emails. You do not need these emails to access the folders.\n",
      "Now the folder is shared with you. They can place any document into this folder to “turn in” assignments and you will have access to them.\n",
      "To organize all of the student folders that are now created you will have to create a separate folder for each section. Title your folder so that you know what class and section each is. Drag and drop the shared student folders into your section folders and now you can easily find them. What is the purpose of creating a separate folder for each section, and how can you easily organize the shared student folders?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The purpose of creating a separate folder for each section is to help you easily identify and access the shared student folders for each specific class and section. By creating a separate folder, you can organize the folders in a way that makes sense for you and easily find the documents you need. You can easily organize the shared student folders by dragging and dropping them into the appropriate section folder. This way, you can keep all of the student work organized and easily accessible.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you please show me how to create a section folder and explain how to drag and drop the shared student folders into it?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sure! I'd be happy to explain how to create a section folder and organize your shared student folders within it. Here are the steps:\n",
      "\n",
      "1. Go to your Google Drive page.\n",
      "2. Click on the \"New\" button in the top left corner of the screen.\n",
      "3. Select \"Folder\" from the drop-down menu.\n",
      "4. Give your folder a name that identifies which class and section it's for (e.g. \"English 101 - Section A\").\n",
      "5. Open the folder you just created.\n",
      "6. In another tab or window, navigate to the shared folder that a student has created for your class.\n",
      "7. Click and hold the shared folder, and drag it over to the section folder you created in step 4. \n",
      "8. Drop the shared folder into the section folder.\n",
      "\n",
      "Repeat steps 6-8 for all of the shared student folders that belong to that section. \n",
      "\n",
      "By creating a separate folder for each class and section, and dragging and dropping the shared student folders into them, you can keep everything organized in a way that makes it easy to find and access the files you need.<|im_end|>\n",
      "<|im_start|>user\n",
      "Thanks for showing me how to create a section folder and organize the shared student folders within it. Can you also explain how to give feedback to students after reviewing their assignments in the shared folder?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([[ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False]])\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create the reward model",
   "id": "6e77f8f5907b1bc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:41:25.928628Z",
     "start_time": "2025-08-20T19:41:25.919996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The reward model generates a single logit representing the probability of that response (we use Bradley-Terry model of preferences)\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size * 4)\n",
    "        self.fc2 = nn.Linear(input_size * 4, 1)\n",
    "\n",
    "'''\n",
    "Preference training loop steps:\n",
    "1. Iterate through synthetic data of preferences.\n",
    "2. Use the final hidden state last embedding as input to reward network.\n",
    "3. Do that for both (otherwise you can use synthetic data and feed into the network the prompt + the response)\n",
    "4. Use that to get the embeddings for both (just do one pass)\n",
    "5. Calculate the loss based on Bradley-Terry and do backprop on the reward model network (you can mean the sums so you do it batchwise)\n",
    "\n",
    "The idea we'll use is take the prompt reward string to get rewrd but only train on prompt to avoid mismatch problem.\n",
    "'''\n"
   ],
   "id": "daf721071ca1e02e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPreference training loop steps:\\n1. Iterate through synthetic data of preferences.\\n2. Use the final hidden state last embedding as input to reward network.\\n3. Do that for both (otherwise you can use synthetic data and feed into the network the prompt + the response)\\n4. Use that to get the embeddings for both (just do one pass)\\n5. Calculate the loss based on Bradley-Terry and do backprop on the reward model network (you can mean the sums so you do it batchwise)\\n\\nThe idea we'll use is take the prompt reward string to get rewrd but only train on prompt to avoid mismatch problem.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:58:32.609858Z",
     "start_time": "2025-08-20T19:58:31.662841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_data = load_dataset('HuggingFaceH4/no_robots')['train']\n",
    "train_data[0]"
   ],
   "id": "d09fc1eced2f7d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert’s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. “We can’t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,” says Rinkert.',\n",
       " 'prompt_id': '627a77298cf96a309aa35a62207c4164e22a66f6db79119506228f28ddc0f947',\n",
       " 'messages': [{'content': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert’s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. “We can’t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,” says Rinkert.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.',\n",
       "   'role': 'assistant'}],\n",
       " 'category': 'Summarize'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6190e2927bd84be6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
