{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### RLHF on pretrained GPT2\n",
    "Run post training on a pretrained GPT-2 model to understand RLHF. Steps will be SFT -> train reward model -> run grpo on pretrained llm on reward model. Rather than using TRL, I will be implementing grpo myself. Implementation will start with single gpu and then scaled to distributed system. This is an interactive notebook where I can write out the main algorithms but will make separate files for actual training."
   ],
   "id": "6a402223a4106937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T19:02:41.870002Z",
     "start_time": "2025-09-08T19:02:38.213121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "\n",
    "device = torch.device('cpu' if torch.backends.mps.is_available() else 'cpu')\n",
    "print('using device:', device)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "model = model.to(torch.float32)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# model.eval()\n",
    "#\n",
    "# prompt = \"\"\"\n",
    "# <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_start|>human\n",
    "#  Black people should go back to Africa.<|im_end|>\n",
    "# <|im_start|>assistant\n",
    "#  I’m afraid I can’t say that.<|im_end|>\n",
    "# <|im_start|>human\n",
    "#  The police love to kill blacks. It has happened too many times.<|im_end|>\n",
    "# <|im_start|>assistant\n",
    "#  I can’t say that.<|im_end|>\n",
    "# <|im_start|>human\n",
    "#  The blacks that kill people they should die too.<|im_end|>\n",
    "# <|im_start|>assistant\n",
    "# \"\"\"\n",
    "# inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     outputs = model.generate(\n",
    "#         inputs,\n",
    "#         max_new_tokens=100,\n",
    "#         num_return_sequences=1,\n",
    "#         temperature=0.7,\n",
    "#         pad_token_id=tokenizer.eos_token_id\n",
    "#     )\n",
    "#\n",
    "# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(generated_text)"
   ],
   "id": "a36fde68ad87b98c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulbir/Desktop/rl-learning/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Supervised fine tuning\n",
    "\n",
    "What I need to do:\n",
    "1. Preprocess data into chat template with EOS token. Ensure data is padded and make sure batches are truncated to fit context length.\n",
    "2. Iterate through every batch and for each one calculate the loss (ONLY on the last assistant completion so the model learns prompt prediction). We use cross entropy btw.\n",
    "3. Run a number of epochs on it.\n",
    "4. Keep single threaded till we implement grpo as well."
   ],
   "id": "fb9e9f51ec732eb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T19:03:01.213839Z",
     "start_time": "2025-09-08T19:02:46.669724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "import copy\n",
    "\n",
    "# ---------------\n",
    "# hyperparameters / config\n",
    "num_epochs = 5\n",
    "batch_size = 2\n",
    "lr = 5e-5\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# ---------------\n",
    "\n",
    "# create dataset train/val/test splits\n",
    "train_sft_dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split='train_sft').select(range(1000))\n",
    "train_split_size = int(0.9 * len(train_sft_dataset))\n",
    "train_split = train_sft_dataset.select(range(train_split_size))\n",
    "val_split = train_sft_dataset.select(range(train_split_size, len(train_sft_dataset)))\n",
    "\n",
    "# create chat template for tokenizer to use, gpt2 uses eos token so we need to add that as well\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{%- for message in messages %}\n",
    "    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|im_start|>assistant\\n' }}\n",
    "{%- else %}\n",
    "    {{- eos_token }}\n",
    "{%- endif %}\n",
    "\"\"\"\n",
    "\n",
    "# preprocess data and create dataloader\n",
    "ending_msg_token_len = len(tokenizer.encode('<|im_end|>\\n'))\n",
    "def add_chat_tem(example):\n",
    "    example['og_messages'] = copy.deepcopy(example['messages'])\n",
    "    max_context_length = model.config.max_position_embeddings\n",
    "    has_system = 1 if example['messages'][0]['role'] == 'system' else 0\n",
    "    while True:\n",
    "        enc_chat_tem_ex = tokenizer.apply_chat_template(example['messages'], tokenize=True, add_special_tokens=False)\n",
    "        diff = len(enc_chat_tem_ex) - max_context_length\n",
    "        if diff <= 0:\n",
    "            example['exclude'] = False\n",
    "            break\n",
    "        elif len(example['messages']) // 2 <= 1:\n",
    "            example['exclude'] = True\n",
    "            break\n",
    "\n",
    "        del example['messages'][0 + has_system]\n",
    "        del example['messages'][0 + has_system]\n",
    "\n",
    "\n",
    "    # convert to chat template and keep track of # of tokens in last generation\n",
    "    enc_chat_tem_ex = tokenizer.apply_chat_template(example['messages'], tokenize=True, add_special_tokens=False)\n",
    "    example['input_ids'] = enc_chat_tem_ex\n",
    "    end_size = (len(tokenizer.encode(example['messages'][-1]['content'], add_special_tokens=False)) + ending_msg_token_len)\n",
    "    last_gen_start_ind = len(enc_chat_tem_ex) - end_size\n",
    "    example['last_gen_start_ind'] = last_gen_start_ind\n",
    "    return example\n",
    "\n",
    "exclude_filter = lambda x: x['exclude'] == False\n",
    "train_split = train_split.map(add_chat_tem).filter(exclude_filter)\n",
    "val_split = val_split.map(add_chat_tem).filter(exclude_filter)\n",
    "\n",
    "# create custom collator for sft\n",
    "class DataCollatorForSFT(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        last_gen_start_inds = [example['last_gen_start_ind'] for example in features]\n",
    "        features = [{'input_ids': example['input_ids']} for example in features]\n",
    "        batch = super().__call__(features, return_tensors=return_tensors)\n",
    "        # scrappy but just assume we're calling with return_tensors='pt'\n",
    "        batch['last_gen_start_inds'] = torch.tensor(last_gen_start_inds)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSFT(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_split,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_split,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "val_batch = next(iter(val_dataloader))"
   ],
   "id": "b8e444928958616b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/900 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1658 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 900/900 [00:08<00:00, 103.45 examples/s]\n",
      "Filter: 100%|██████████| 900/900 [00:00<00:00, 6525.10 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:01<00:00, 89.83 examples/s]\n",
      "Filter: 100%|██████████| 100/100 [00:00<00:00, 5565.69 examples/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T01:09:37.179021Z",
     "start_time": "2025-09-06T01:09:37.165942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pprint(val_split)\n",
    "pprint(train_split)\n",
    "# pprint(val_batch)\n",
    "\n",
    "pprint(train_split[0])"
   ],
   "id": "6ed83db81e20cf58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'messages', 'og_messages', 'exclude', 'input_ids', 'last_gen_start_ind'],\n",
      "    num_rows: 99\n",
      "})\n",
      "Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'messages', 'og_messages', 'exclude', 'input_ids', 'last_gen_start_ind'],\n",
      "    num_rows: 885\n",
      "})\n",
      "{'exclude': False,\n",
      " 'input_ids': [27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               4711,\n",
      "               7729,\n",
      "               4174,\n",
      "               284,\n",
      "               2665,\n",
      "               12,\n",
      "               3106,\n",
      "               13460,\n",
      "               357,\n",
      "               19309,\n",
      "               684,\n",
      "               425,\n",
      "               718,\n",
      "               13,\n",
      "               15,\n",
      "               28200,\n",
      "               4990,\n",
      "               1437,\n",
      "               604,\n",
      "               13,\n",
      "               15,\n",
      "               28200,\n",
      "               2547,\n",
      "               439,\n",
      "               897,\n",
      "               513,\n",
      "               13,\n",
      "               15,\n",
      "               10,\n",
      "               22278,\n",
      "               362,\n",
      "               13,\n",
      "               15,\n",
      "               28200,\n",
      "               16540,\n",
      "               17517,\n",
      "               642,\n",
      "               13,\n",
      "               15,\n",
      "               10,\n",
      "               737,\n",
      "               1867,\n",
      "               7505,\n",
      "               2196,\n",
      "               716,\n",
      "               314,\n",
      "               1262,\n",
      "               30,\n",
      "               198,\n",
      "               2202,\n",
      "               534,\n",
      "               50004,\n",
      "               5468,\n",
      "               1222,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               11,\n",
      "               345,\n",
      "               460,\n",
      "               3538,\n",
      "               905,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               286,\n",
      "               257,\n",
      "               1720,\n",
      "               319,\n",
      "               20599,\n",
      "               416,\n",
      "               15882,\n",
      "               530,\n",
      "               286,\n",
      "               262,\n",
      "               7505,\n",
      "               338,\n",
      "               3170,\n",
      "               12,\n",
      "               259,\n",
      "               6460,\n",
      "               0,\n",
      "               198,\n",
      "               7120,\n",
      "               12251,\n",
      "               5468,\n",
      "               1222,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               481,\n",
      "               783,\n",
      "               3359,\n",
      "               262,\n",
      "               9233,\n",
      "               1720,\n",
      "               2939,\n",
      "               655,\n",
      "               416,\n",
      "               33627,\n",
      "               625,\n",
      "               326,\n",
      "               1720,\n",
      "               2939,\n",
      "               40901,\n",
      "               13,\n",
      "               198,\n",
      "               13921,\n",
      "               428,\n",
      "               3895,\n",
      "               4174,\n",
      "               284,\n",
      "               477,\n",
      "               9004,\n",
      "               286,\n",
      "               262,\n",
      "               7505,\n",
      "               393,\n",
      "               655,\n",
      "               2176,\n",
      "               3392,\n",
      "               355,\n",
      "               5610,\n",
      "               287,\n",
      "               262,\n",
      "               2420,\n",
      "               2587,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               1212,\n",
      "               3895,\n",
      "               691,\n",
      "               8991,\n",
      "               284,\n",
      "               12251,\n",
      "               5468,\n",
      "               290,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               286,\n",
      "               262,\n",
      "               2665,\n",
      "               12,\n",
      "               3106,\n",
      "               13460,\n",
      "               5610,\n",
      "               287,\n",
      "               262,\n",
      "               2420,\n",
      "               2587,\n",
      "               29847,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               6090,\n",
      "               345,\n",
      "               5698,\n",
      "               502,\n",
      "               832,\n",
      "               262,\n",
      "               1429,\n",
      "               286,\n",
      "               15882,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               319,\n",
      "               616,\n",
      "               12251,\n",
      "               5468,\n",
      "               290,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               19457,\n",
      "               11,\n",
      "               994,\n",
      "               389,\n",
      "               262,\n",
      "               4831,\n",
      "               284,\n",
      "               7139,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               319,\n",
      "               534,\n",
      "               12251,\n",
      "               5468,\n",
      "               290,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               25,\n",
      "               198,\n",
      "               198,\n",
      "               16,\n",
      "               13,\n",
      "               5972,\n",
      "               287,\n",
      "               284,\n",
      "               534,\n",
      "               13705,\n",
      "               1958,\n",
      "               1848,\n",
      "               290,\n",
      "               467,\n",
      "               284,\n",
      "               534,\n",
      "               7467,\n",
      "               9363,\n",
      "               13,\n",
      "               198,\n",
      "               17,\n",
      "               13,\n",
      "               6914,\n",
      "               319,\n",
      "               8562,\n",
      "               1096,\n",
      "               7505,\n",
      "               329,\n",
      "               262,\n",
      "               2665,\n",
      "               12,\n",
      "               3106,\n",
      "               7505,\n",
      "               345,\n",
      "               389,\n",
      "               1262,\n",
      "               13,\n",
      "               198,\n",
      "               18,\n",
      "               13,\n",
      "               13244,\n",
      "               10055,\n",
      "               284,\n",
      "               262,\n",
      "               12251,\n",
      "               5468,\n",
      "               393,\n",
      "               38188,\n",
      "               50004,\n",
      "               2665,\n",
      "               810,\n",
      "               345,\n",
      "               765,\n",
      "               284,\n",
      "               7139,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               13,\n",
      "               198,\n",
      "               19,\n",
      "               13,\n",
      "               2080,\n",
      "               262,\n",
      "               2665,\n",
      "               1280,\n",
      "               11,\n",
      "               3904,\n",
      "               319,\n",
      "               262,\n",
      "               2665,\n",
      "               338,\n",
      "               4634,\n",
      "               357,\n",
      "               31763,\n",
      "               8,\n",
      "               7196,\n",
      "               287,\n",
      "               262,\n",
      "               1353,\n",
      "               12,\n",
      "               9464,\n",
      "               5228,\n",
      "               13,\n",
      "               198,\n",
      "               20,\n",
      "               13,\n",
      "               554,\n",
      "               262,\n",
      "               6460,\n",
      "               6103,\n",
      "               326,\n",
      "               3568,\n",
      "               11,\n",
      "               804,\n",
      "               329,\n",
      "               281,\n",
      "               3038,\n",
      "               15494,\n",
      "               705,\n",
      "               5159,\n",
      "               3359,\n",
      "               6,\n",
      "               393,\n",
      "               705,\n",
      "               5159,\n",
      "               20599,\n",
      "               4458,\n",
      "               198,\n",
      "               21,\n",
      "               13,\n",
      "               1002,\n",
      "               1695,\n",
      "               11,\n",
      "               2922,\n",
      "               705,\n",
      "               15307,\n",
      "               9233,\n",
      "               2939,\n",
      "               319,\n",
      "               20599,\n",
      "               4458,\n",
      "               198,\n",
      "               22,\n",
      "               13,\n",
      "               12793,\n",
      "               262,\n",
      "               2458,\n",
      "               290,\n",
      "               12714,\n",
      "               262,\n",
      "               12251,\n",
      "               14,\n",
      "               37948,\n",
      "               12251,\n",
      "               2443,\n",
      "               284,\n",
      "               766,\n",
      "               262,\n",
      "               1245,\n",
      "               13,\n",
      "               198,\n",
      "               198,\n",
      "               1532,\n",
      "               345,\n",
      "               821,\n",
      "               1719,\n",
      "               5876,\n",
      "               4917,\n",
      "               262,\n",
      "               4634,\n",
      "               11,\n",
      "               262,\n",
      "               1266,\n",
      "               1517,\n",
      "               284,\n",
      "               466,\n",
      "               318,\n",
      "               3522,\n",
      "               284,\n",
      "               534,\n",
      "               7505,\n",
      "               338,\n",
      "               10314,\n",
      "               11,\n",
      "               1201,\n",
      "               262,\n",
      "               4067,\n",
      "               290,\n",
      "               27393,\n",
      "               286,\n",
      "               6460,\n",
      "               460,\n",
      "               7565,\n",
      "               1022,\n",
      "               13460,\n",
      "               29847,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               6090,\n",
      "               345,\n",
      "               2148,\n",
      "               502,\n",
      "               351,\n",
      "               257,\n",
      "               2792,\n",
      "               284,\n",
      "               262,\n",
      "               10314,\n",
      "               329,\n",
      "               616,\n",
      "               7505,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               40,\n",
      "               836,\n",
      "               470,\n",
      "               423,\n",
      "               1895,\n",
      "               284,\n",
      "               534,\n",
      "               3650,\n",
      "               338,\n",
      "               7505,\n",
      "               1321,\n",
      "               13,\n",
      "               2102,\n",
      "               11,\n",
      "               345,\n",
      "               460,\n",
      "               3221,\n",
      "               1064,\n",
      "               262,\n",
      "               10314,\n",
      "               329,\n",
      "               534,\n",
      "               7505,\n",
      "               416,\n",
      "               1016,\n",
      "               284,\n",
      "               262,\n",
      "               6128,\n",
      "               1958,\n",
      "               7505,\n",
      "               3650,\n",
      "               11,\n",
      "               4917,\n",
      "               534,\n",
      "               7505,\n",
      "               290,\n",
      "               12264,\n",
      "               319,\n",
      "               262,\n",
      "               705,\n",
      "               11284,\n",
      "               6,\n",
      "               2792,\n",
      "               5140,\n",
      "               287,\n",
      "               262,\n",
      "               4220,\n",
      "               826,\n",
      "               5228,\n",
      "               286,\n",
      "               262,\n",
      "               2443,\n",
      "               13,\n",
      "               25929,\n",
      "               11,\n",
      "               345,\n",
      "               460,\n",
      "               466,\n",
      "               257,\n",
      "               23645,\n",
      "               2989,\n",
      "               329,\n",
      "               262,\n",
      "               1438,\n",
      "               286,\n",
      "               534,\n",
      "               7505,\n",
      "               3940,\n",
      "               416,\n",
      "               705,\n",
      "               22897,\n",
      "               341,\n",
      "               6,\n",
      "               393,\n",
      "               705,\n",
      "               7220,\n",
      "               5698,\n",
      "               4458,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               6090,\n",
      "               345,\n",
      "               6216,\n",
      "               611,\n",
      "               428,\n",
      "               3895,\n",
      "               635,\n",
      "               2499,\n",
      "               329,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               286,\n",
      "               616,\n",
      "               7505,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               464,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               743,\n",
      "               393,\n",
      "               743,\n",
      "               407,\n",
      "               670,\n",
      "               329,\n",
      "               534,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               11,\n",
      "               6906,\n",
      "               319,\n",
      "               262,\n",
      "               8398,\n",
      "               286,\n",
      "               534,\n",
      "               7505,\n",
      "               13,\n",
      "               2773,\n",
      "               13460,\n",
      "               2291,\n",
      "               428,\n",
      "               3895,\n",
      "               287,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               416,\n",
      "               4277,\n",
      "               11,\n",
      "               981,\n",
      "               1854,\n",
      "               743,\n",
      "               2421,\n",
      "               3224,\n",
      "               31344,\n",
      "               13,\n",
      "               1675,\n",
      "               2198,\n",
      "               611,\n",
      "               428,\n",
      "               3895,\n",
      "               318,\n",
      "               1695,\n",
      "               329,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               286,\n",
      "               534,\n",
      "               7505,\n",
      "               11,\n",
      "               1061,\n",
      "               777,\n",
      "               4831,\n",
      "               25,\n",
      "               198,\n",
      "               198,\n",
      "               16,\n",
      "               13,\n",
      "               1514,\n",
      "               284,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               810,\n",
      "               345,\n",
      "               561,\n",
      "               588,\n",
      "               284,\n",
      "               7139,\n",
      "               262,\n",
      "               3895,\n",
      "               13,\n",
      "               362,\n",
      "               13,\n",
      "               6914,\n",
      "               319,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               6460,\n",
      "               7196,\n",
      "               357,\n",
      "               31763,\n",
      "               7196,\n",
      "               8,\n",
      "               290,\n",
      "               804,\n",
      "               329,\n",
      "               705,\n",
      "               5159,\n",
      "               3359,\n",
      "               6,\n",
      "               393,\n",
      "               705,\n",
      "               5159,\n",
      "               20599,\n",
      "               4458,\n",
      "               513,\n",
      "               13,\n",
      "               1002,\n",
      "               1695,\n",
      "               11,\n",
      "               2922,\n",
      "               705,\n",
      "               15307,\n",
      "               9233,\n",
      "               2939,\n",
      "               319,\n",
      "               20599,\n",
      "               4458,\n",
      "               604,\n",
      "               13,\n",
      "               12793,\n",
      "               262,\n",
      "               2458,\n",
      "               13,\n",
      "               1002,\n",
      "               428,\n",
      "               3038,\n",
      "               318,\n",
      "               407,\n",
      "               1695,\n",
      "               287,\n",
      "               534,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               6460,\n",
      "               11,\n",
      "               345,\n",
      "               743,\n",
      "               761,\n",
      "               284,\n",
      "               3151,\n",
      "               503,\n",
      "               284,\n",
      "               534,\n",
      "               7505,\n",
      "               8517,\n",
      "               329,\n",
      "               6829,\n",
      "               351,\n",
      "               2183,\n",
      "               2890,\n",
      "               534,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               284,\n",
      "               2291,\n",
      "               428,\n",
      "               3895,\n",
      "               29847,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               50256],\n",
      " 'last_gen_start_ind': 613,\n",
      " 'messages': [{'content': 'These instructions apply to section-based themes '\n",
      "                          '(Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo '\n",
      "                          '2.0+, Mobilia 5.0+). What theme version am I '\n",
      "                          'using?\\n'\n",
      "                          'On your Collections pages & Featured Collections '\n",
      "                          'sections, you can easily show the secondary image '\n",
      "                          'of a product on hover by enabling one of the '\n",
      "                          \"theme's built-in settings!\\n\"\n",
      "                          'Your Collection pages & Featured Collections '\n",
      "                          'sections will now display the secondary product '\n",
      "                          'image just by hovering over that product image '\n",
      "                          'thumbnail.\\n'\n",
      "                          'Does this feature apply to all sections of the '\n",
      "                          'theme or just specific ones as listed in the text '\n",
      "                          'material?',\n",
      "               'role': 'user'},\n",
      "              {'content': 'This feature only applies to Collection pages and '\n",
      "                          'Featured Collections sections of the section-based '\n",
      "                          'themes listed in the text material.',\n",
      "               'role': 'assistant'},\n",
      "              {'content': 'Can you guide me through the process of enabling '\n",
      "                          'the secondary image hover feature on my Collection '\n",
      "                          'pages and Featured Collections sections?',\n",
      "               'role': 'user'},\n",
      "              {'content': 'Sure, here are the steps to enable the secondary '\n",
      "                          'image hover feature on your Collection pages and '\n",
      "                          'Featured Collections sections:\\n'\n",
      "                          '\\n'\n",
      "                          '1. Log in to your Shopify account and go to your '\n",
      "                          'Online Store.\\n'\n",
      "                          '2. Click on Customize theme for the section-based '\n",
      "                          'theme you are using.\\n'\n",
      "                          '3. Navigate to the Collection pages or Featured '\n",
      "                          'Collections section where you want to enable the '\n",
      "                          'secondary image hover feature.\\n'\n",
      "                          \"4. With the section open, click on the section's \"\n",
      "                          'setting (gear) icon in the top-left corner.\\n'\n",
      "                          '5. In the settings panel that appears, look for an '\n",
      "                          \"option labeled 'Image display' or 'Image hover'.\\n\"\n",
      "                          \"6. If available, select 'Show secondary image on \"\n",
      "                          \"hover'.\\n\"\n",
      "                          '7. Save the changes and preview the '\n",
      "                          'Collection/Featured Collection page to see the '\n",
      "                          'effect.\\n'\n",
      "                          '\\n'\n",
      "                          \"If you're having trouble finding the setting, the \"\n",
      "                          \"best thing to do is refer to your theme's \"\n",
      "                          'documentation, since the location and labeling of '\n",
      "                          'settings can vary between themes.',\n",
      "               'role': 'assistant'},\n",
      "              {'content': 'Can you provide me with a link to the documentation '\n",
      "                          'for my theme?',\n",
      "               'role': 'user'},\n",
      "              {'content': \"I don't have access to your store's theme \"\n",
      "                          'information. However, you can usually find the '\n",
      "                          'documentation for your theme by going to the '\n",
      "                          'shopify theme store, finding your theme and '\n",
      "                          \"clicking on the 'support' link located in the \"\n",
      "                          'bottom right corner of the page. Alternatively, you '\n",
      "                          'can do a google search for the name of your theme '\n",
      "                          \"followed by 'documentation' or 'user guide'.\",\n",
      "               'role': 'assistant'},\n",
      "              {'content': 'Can you confirm if this feature also works for the '\n",
      "                          'Quick Shop section of my theme?',\n",
      "               'role': 'user'},\n",
      "              {'content': 'The secondary image hover feature may or may not '\n",
      "                          'work for your Quick Shop section, depending on the '\n",
      "                          'configuration of your theme. Some themes include '\n",
      "                          'this feature in the Quick Shop section by default, '\n",
      "                          'while others may require additional customization. '\n",
      "                          'To check if this feature is available for the Quick '\n",
      "                          'Shop section of your theme, follow these steps:\\n'\n",
      "                          '\\n'\n",
      "                          '1. Go to the Quick Shop section where you would '\n",
      "                          'like to enable the feature. 2. Click on the Quick '\n",
      "                          \"Shop settings icon (gear icon) and look for 'Image \"\n",
      "                          \"display' or 'Image hover'. 3. If available, select \"\n",
      "                          \"'Show secondary image on hover'. 4. Save the \"\n",
      "                          'changes. If this option is not available in your '\n",
      "                          'Quick Shop section settings, you may need to reach '\n",
      "                          'out to your theme developer for assistance with '\n",
      "                          'customizing your Quick Shop section to include this '\n",
      "                          'feature.',\n",
      "               'role': 'assistant'}],\n",
      " 'og_messages': [{'content': 'These instructions apply to section-based themes '\n",
      "                             '(Responsive 6.0+, Retina 4.0+, Parallax 3.0+ '\n",
      "                             'Turbo 2.0+, Mobilia 5.0+). What theme version am '\n",
      "                             'I using?\\n'\n",
      "                             'On your Collections pages & Featured Collections '\n",
      "                             'sections, you can easily show the secondary '\n",
      "                             'image of a product on hover by enabling one of '\n",
      "                             \"the theme's built-in settings!\\n\"\n",
      "                             'Your Collection pages & Featured Collections '\n",
      "                             'sections will now display the secondary product '\n",
      "                             'image just by hovering over that product image '\n",
      "                             'thumbnail.\\n'\n",
      "                             'Does this feature apply to all sections of the '\n",
      "                             'theme or just specific ones as listed in the '\n",
      "                             'text material?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': 'This feature only applies to Collection pages '\n",
      "                             'and Featured Collections sections of the '\n",
      "                             'section-based themes listed in the text '\n",
      "                             'material.',\n",
      "                  'role': 'assistant'},\n",
      "                 {'content': 'Can you guide me through the process of enabling '\n",
      "                             'the secondary image hover feature on my '\n",
      "                             'Collection pages and Featured Collections '\n",
      "                             'sections?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': 'Sure, here are the steps to enable the secondary '\n",
      "                             'image hover feature on your Collection pages and '\n",
      "                             'Featured Collections sections:\\n'\n",
      "                             '\\n'\n",
      "                             '1. Log in to your Shopify account and go to your '\n",
      "                             'Online Store.\\n'\n",
      "                             '2. Click on Customize theme for the '\n",
      "                             'section-based theme you are using.\\n'\n",
      "                             '3. Navigate to the Collection pages or Featured '\n",
      "                             'Collections section where you want to enable the '\n",
      "                             'secondary image hover feature.\\n'\n",
      "                             \"4. With the section open, click on the section's \"\n",
      "                             'setting (gear) icon in the top-left corner.\\n'\n",
      "                             '5. In the settings panel that appears, look for '\n",
      "                             \"an option labeled 'Image display' or 'Image \"\n",
      "                             \"hover'.\\n\"\n",
      "                             \"6. If available, select 'Show secondary image on \"\n",
      "                             \"hover'.\\n\"\n",
      "                             '7. Save the changes and preview the '\n",
      "                             'Collection/Featured Collection page to see the '\n",
      "                             'effect.\\n'\n",
      "                             '\\n'\n",
      "                             \"If you're having trouble finding the setting, \"\n",
      "                             \"the best thing to do is refer to your theme's \"\n",
      "                             'documentation, since the location and labeling '\n",
      "                             'of settings can vary between themes.',\n",
      "                  'role': 'assistant'},\n",
      "                 {'content': 'Can you provide me with a link to the '\n",
      "                             'documentation for my theme?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': \"I don't have access to your store's theme \"\n",
      "                             'information. However, you can usually find the '\n",
      "                             'documentation for your theme by going to the '\n",
      "                             'shopify theme store, finding your theme and '\n",
      "                             \"clicking on the 'support' link located in the \"\n",
      "                             'bottom right corner of the page. Alternatively, '\n",
      "                             'you can do a google search for the name of your '\n",
      "                             \"theme followed by 'documentation' or 'user \"\n",
      "                             \"guide'.\",\n",
      "                  'role': 'assistant'},\n",
      "                 {'content': 'Can you confirm if this feature also works for '\n",
      "                             'the Quick Shop section of my theme?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': 'The secondary image hover feature may or may not '\n",
      "                             'work for your Quick Shop section, depending on '\n",
      "                             'the configuration of your theme. Some themes '\n",
      "                             'include this feature in the Quick Shop section '\n",
      "                             'by default, while others may require additional '\n",
      "                             'customization. To check if this feature is '\n",
      "                             'available for the Quick Shop section of your '\n",
      "                             'theme, follow these steps:\\n'\n",
      "                             '\\n'\n",
      "                             '1. Go to the Quick Shop section where you would '\n",
      "                             'like to enable the feature. 2. Click on the '\n",
      "                             'Quick Shop settings icon (gear icon) and look '\n",
      "                             \"for 'Image display' or 'Image hover'. 3. If \"\n",
      "                             \"available, select 'Show secondary image on \"\n",
      "                             \"hover'. 4. Save the changes. If this option is \"\n",
      "                             'not available in your Quick Shop section '\n",
      "                             'settings, you may need to reach out to your '\n",
      "                             'theme developer for assistance with customizing '\n",
      "                             'your Quick Shop section to include this feature.',\n",
      "                  'role': 'assistant'}],\n",
      " 'prompt': 'These instructions apply to section-based themes (Responsive 6.0+, '\n",
      "           'Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme '\n",
      "           'version am I using?\\n'\n",
      "           'On your Collections pages & Featured Collections sections, you can '\n",
      "           'easily show the secondary image of a product on hover by enabling '\n",
      "           \"one of the theme's built-in settings!\\n\"\n",
      "           'Your Collection pages & Featured Collections sections will now '\n",
      "           'display the secondary product image just by hovering over that '\n",
      "           'product image thumbnail.\\n'\n",
      "           'Does this feature apply to all sections of the theme or just '\n",
      "           'specific ones as listed in the text material?',\n",
      " 'prompt_id': 'f0e37e9f7800261167ce91143f98f511f768847236f133f2d0aed60b444ebe57'}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T01:09:38.776925Z",
     "start_time": "2025-09-06T01:09:38.774806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# keep running list of training + val loss for logging\n",
    "losses_per_epoch = []\n",
    "latest_epoch_losses = []"
   ],
   "id": "782cd2195efbc066",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T01:10:08.023543Z",
     "start_time": "2025-09-06T01:09:40.543818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_model_loss(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    last_gen_start_inds = batch['last_gen_start_inds'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    # print(f\"Batch shape: {input_ids.shape}\")  # This is [batch_size, sequence_length]\n",
    "    # print(f\"Sequence length: {input_ids.shape[1]}\")\n",
    "    # print(f\"Memory before forward pass: {torch.mps.driver_allocated_memory() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "    # mask labels that aren't included in last gen\n",
    "    mask = torch.arange(input_ids.shape[1], device=device) < last_gen_start_inds[:, None]\n",
    "    labels[mask] = -100\n",
    "\n",
    "    # run forward pass and calc loss\n",
    "    model_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # print(f\"Memory after forward pass: {torch.mps.driver_allocated_memory() / 1024**3:.2f} GB\")\n",
    "\n",
    "    # calculate loss\n",
    "    B, T, C = model_outputs.logits.shape\n",
    "    logits = model_outputs.logits.view(B*T, C)\n",
    "    labels = labels.view(B*T)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# training run\n",
    "model.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    step = 0\n",
    "    train_dl_iter = iter(train_dataloader)\n",
    "    print(f\"\\n Starting epoch: {epoch}\")\n",
    "    for batch in train_dl_iter:\n",
    "        step += 1\n",
    "        # calculate training loss\n",
    "        training_loss = calc_model_loss(batch)\n",
    "        optimizer.zero_grad()\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate val loss\n",
    "        avg_val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            val_dl_iter = iter(val_dataloader)\n",
    "            for val_batch in val_dl_iter:\n",
    "                avg_val_loss += calc_model_loss(val_batch).item()\n",
    "                val_batches += 1\n",
    "            avg_val_loss /= val_batches\n",
    "\n",
    "        latest_epoch_losses.append((training_loss.item(), avg_val_loss))\n",
    "        print(f\"epoch: {epoch} | step: {step} | training loss: {training_loss} | validation loss: {avg_val_loss}\")\n",
    "\n",
    "    losses_per_epoch.append(latest_epoch_losses)\n",
    "    latest_epoch_losses = []"
   ],
   "id": "71d7c703271a5893",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting epoch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 50\u001B[39m\n\u001B[32m     48\u001B[39m val_dl_iter = \u001B[38;5;28miter\u001B[39m(val_dataloader)\n\u001B[32m     49\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m val_batch \u001B[38;5;129;01min\u001B[39;00m val_dl_iter:\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m     avg_val_loss += \u001B[43mcalc_model_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_batch\u001B[49m\u001B[43m)\u001B[49m.item()\n\u001B[32m     51\u001B[39m     val_batches += \u001B[32m1\u001B[39m\n\u001B[32m     52\u001B[39m avg_val_loss /= val_batches\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mcalc_model_loss\u001B[39m\u001B[34m(batch)\u001B[39m\n\u001B[32m     14\u001B[39m labels[mask] = -\u001B[32m100\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# run forward pass and calc loss\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m model_outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# print(f\"Memory after forward pass: {torch.mps.driver_allocated_memory() / 1024**3:.2f} GB\")\u001B[39;00m\n\u001B[32m     20\u001B[39m \n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m# calculate loss\u001B[39;00m\n\u001B[32m     22\u001B[39m B, T, C = model_outputs.logits.shape\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001B[39m, in \u001B[36mGPT2LMHeadModel.forward\u001B[39m\u001B[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001B[39m\n\u001B[32m   1056\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1057\u001B[39m \u001B[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001B[39;00m\n\u001B[32m   1058\u001B[39m \u001B[33;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1072\u001B[39m \u001B[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[32m   1073\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1074\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m-> \u001B[39m\u001B[32m1076\u001B[39m transformer_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1077\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1078\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1079\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1080\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1081\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1082\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1083\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1084\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1085\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1086\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1087\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1088\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1089\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1090\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1091\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1092\u001B[39m hidden_states = transformer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1094\u001B[39m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:927\u001B[39m, in \u001B[36mGPT2Model.forward\u001B[39m\u001B[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[39m\n\u001B[32m    924\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states:\n\u001B[32m    925\u001B[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001B[32m--> \u001B[39m\u001B[32m927\u001B[39m outputs = \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    929\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgradient_checkpointing\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    933\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001B[39;49;00m\n\u001B[32m    934\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    935\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    936\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    937\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    938\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    940\u001B[39m hidden_states = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    942\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         logger.warning(message)\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:411\u001B[39m, in \u001B[36mGPT2Block.forward\u001B[39m\u001B[34m(self, hidden_states, past_key_value, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001B[39m\n\u001B[32m    409\u001B[39m residual = hidden_states\n\u001B[32m    410\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.ln_1(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m411\u001B[39m attn_output, self_attn_weights = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    412\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    413\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    414\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    415\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    416\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    417\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    418\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    419\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    420\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    421\u001B[39m \u001B[38;5;66;03m# residual connection\u001B[39;00m\n\u001B[32m    422\u001B[39m hidden_states = attn_output + residual\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:344\u001B[39m, in \u001B[36mGPT2Attention.forward\u001B[39m\u001B[34m(self, hidden_states, past_key_value, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, **kwargs)\u001B[39m\n\u001B[32m    340\u001B[39m     attn_output, attn_weights = \u001B[38;5;28mself\u001B[39m._upcast_and_reordered_attn(\n\u001B[32m    341\u001B[39m         query_states, key_states, value_states, attention_mask, head_mask\n\u001B[32m    342\u001B[39m     )\n\u001B[32m    343\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m344\u001B[39m     attn_output, attn_weights = \u001B[43mattention_interface\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    345\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    346\u001B[39m \u001B[43m        \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    347\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalue_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn_dropout\u001B[49m\u001B[43m.\u001B[49m\u001B[43mp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[32;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    356\u001B[39m attn_output = attn_output.reshape(*attn_output.shape[:-\u001B[32m2\u001B[39m], -\u001B[32m1\u001B[39m).contiguous()\n\u001B[32m    357\u001B[39m attn_output = \u001B[38;5;28mself\u001B[39m.c_proj(attn_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/transformers/integrations/sdpa_attention.py:89\u001B[39m, in \u001B[36msdpa_attention_forward\u001B[39m\u001B[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001B[39m\n\u001B[32m     86\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.jit.is_tracing() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(is_causal, torch.Tensor):\n\u001B[32m     87\u001B[39m     is_causal = is_causal.item()\n\u001B[32m---> \u001B[39m\u001B[32m89\u001B[39m attn_output = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfunctional\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     90\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     91\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     92\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     93\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     94\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdropout_p\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscale\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     96\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     97\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43msdpa_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     98\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     99\u001B[39m attn_output = attn_output.transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m).contiguous()\n\u001B[32m    101\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m attn_output, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training the reward model",
   "id": "6e77f8f5907b1bc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T19:03:06.661773Z",
     "start_time": "2025-09-08T19:03:06.638596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reward model\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size * 4)\n",
    "        self.fc2 = nn.Linear(input_size * 4, input_size)\n",
    "        self.fc3 = nn.Linear(input_size, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "reward_model = RewardModel(model.config.n_embd).to(device)\n",
    "\n",
    "# ---------------\n",
    "# rm hyperparameters / config\n",
    "rm_batch_size = 2\n",
    "rm_epochs = 5\n",
    "rm_lr = 1e-5\n",
    "rm_weight_decay = 0.01\n",
    "rm_optimizer = torch.optim.AdamW(reward_model.parameters(), lr=rm_lr, weight_decay=rm_weight_decay)\n",
    "max_rm_ds_size = 1000\n",
    "# ---------------"
   ],
   "id": "68558aadb95b116a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T19:03:12.287455Z",
     "start_time": "2025-09-08T19:03:07.628168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# we will use Anthropic/hh-rlhf for training our reward model\n",
    "rlhf_dataset = load_dataset(\"Anthropic/hh-rlhf\", split='train')\n",
    "rm_train_dataset = rlhf_dataset.select(range(max_rm_ds_size))\n",
    "rm_train_split_size = int(0.9 * len(rm_train_dataset))\n",
    "rm_train_split = rm_train_dataset.select(range(rm_train_split_size))\n",
    "rm_val_split = rm_train_dataset.select(range(rm_train_split_size, len(rm_train_dataset)))\n",
    "\n",
    "def conv_str_to_msgs(str):\n",
    "    split = [line.strip() for line in re.split(r'(?=Human:|Assistant:)', str) if line.strip()]\n",
    "    msgs = []\n",
    "    for s in split:\n",
    "        role, content = s.split(':', 1)\n",
    "        msgs.append({'role': role.lower(), 'content': content})\n",
    "    return msgs\n",
    "\n",
    "def rm_preproc(example):\n",
    "    ex_proc = {\n",
    "        key: tokenizer.apply_chat_template(\n",
    "            conv_str_to_msgs(value),\n",
    "            tokenize=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        for key, value in example.items()\n",
    "    }\n",
    "\n",
    "    max_context_length = model.config.max_position_embeddings\n",
    "    ex_proc['exclude'] = (\n",
    "            len(ex_proc['chosen']) > max_context_length or len(ex_proc['rejected']) > max_context_length\n",
    "    )\n",
    "\n",
    "    return ex_proc\n",
    "\n",
    "rm_train_split = rm_train_split.map(rm_preproc).filter(exclude_filter)\n",
    "rm_val_split = rm_val_split.map(rm_preproc).filter(exclude_filter)\n",
    "\n",
    "\n",
    "# create custom collator for rlhf data\n",
    "class DataCollatorForRm:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding = True\n",
    "\n",
    "        self.clt = DataCollatorWithPadding(\n",
    "            tokenizer,\n",
    "            padding=self.padding,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        chosen = [{'input_ids': b['chosen']} for b in batch]\n",
    "        rejected = [{'input_ids': b['rejected']} for b in batch]\n",
    "        chosen_padded = self.clt(chosen)\n",
    "        rejected_padded = self.clt(rejected)\n",
    "\n",
    "        return {'chosen': chosen_padded, 'rejected': rejected_padded}\n",
    "\n",
    "\n",
    "rm_data_collator = DataCollatorForRm(tokenizer=tokenizer)\n",
    "\n",
    "rm_train_dataloader = DataLoader(\n",
    "    rm_train_split,\n",
    "    batch_size=rm_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=rm_data_collator\n",
    ")\n",
    "\n",
    "rm_val_dataloader = DataLoader(\n",
    "    rm_val_split,\n",
    "    batch_size=rm_batch_size,\n",
    "    collate_fn=rm_data_collator,\n",
    "    shuffle=False\n",
    ")\n",
    "\n"
   ],
   "id": "8f3c488397c223de",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 900/900 [00:01<00:00, 650.60 examples/s]\n",
      "Filter: 100%|██████████| 900/900 [00:00<00:00, 9808.64 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 631.03 examples/s]\n",
      "Filter: 100%|██████████| 100/100 [00:00<00:00, 8829.18 examples/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T22:00:31.895532Z",
     "start_time": "2025-08-27T22:00:31.808334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_rm_val(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    outputs = model.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # only get the emb for final token\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    final_tok_ind = torch.sum(attention_mask, dim=1) - 1\n",
    "    batch_inds = torch.arange(hidden_states.shape[0], device=device)\n",
    "    final_tok_emb = hidden_states[batch_inds, final_tok_ind]\n",
    "\n",
    "    return reward_model(final_tok_emb)\n",
    "\n",
    "def calc_rm_loss(ch_rm_val, rej_rm_val):\n",
    "    return -F.logsigmoid(ch_rm_val - rej_rm_val).mean()\n",
    "\n",
    "def calc_val_perc_correct(val_batch):\n",
    "    rej_rm_vals = calc_rm_val(val_batch['rejected'])\n",
    "    ch_rm_vals = calc_rm_val(val_batch['chosen'])\n",
    "    correct = (ch_rm_vals > rej_rm_vals).int().sum()\n",
    "    return (correct / len(rej_rm_vals)).item()\n",
    "\n",
    "\n",
    "# reward model training loop\n",
    "reward_model.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    step = 0\n",
    "    rm_train_dl_iter = iter(rm_train_dataloader)\n",
    "    for batch in rm_train_dl_iter:\n",
    "        step += 1\n",
    "        rej = batch['rejected'].to(device)\n",
    "        ch = batch['chosen'].to(device)\n",
    "        rm_loss = calc_rm_loss(calc_rm_val(ch), calc_rm_val(rej))\n",
    "\n",
    "        # calculate avg val dataset % classified correct\n",
    "        avg_val_per_right = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            rm_val_dl_iter = iter(rm_val_dataloader)\n",
    "            for val_batch in rm_val_dl_iter:\n",
    "                avg_val_per_right += calc_val_perc_correct(val_batch)\n",
    "                val_batches += 1\n",
    "            avg_val_per_right /= val_batches\n",
    "\n",
    "        print(f\"epoch: {epoch} | step: {step} | training loss: {rm_loss} | avg val % correct preference: {avg_val_per_right}\")\n",
    "        rm_optimizer.zero_grad()\n",
    "        rm_loss.backward()\n",
    "        rm_optimizer.step()"
   ],
   "id": "daf721071ca1e02e",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reward_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m     21\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (correct / \u001B[38;5;28mlen\u001B[39m(rej_rm_vals)).item()\n\u001B[32m     24\u001B[39m \u001B[38;5;66;03m# reward model training loop\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m \u001B[43mreward_model\u001B[49m.train()\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m, num_epochs+\u001B[32m1\u001B[39m):\n\u001B[32m     27\u001B[39m     step = \u001B[32m0\u001B[39m\n",
      "\u001B[31mNameError\u001B[39m: name 'reward_model' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GRPO implementation + rl training loop",
   "id": "96dba371d81ad028"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T19:03:15.873748Z",
     "start_time": "2025-09-08T19:03:15.871814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------\n",
    "# grpo hyperparameters / config\n",
    "grpo_batch_size = 2\n",
    "max_grpo_ds_size = 2000\n",
    "eps = 0.2\n",
    "beta = 0.05\n",
    "opt_st_per_b = 5\n",
    "lr = 1e-3\n",
    "grpo_opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# ---------------"
   ],
   "id": "af69a7b4cd20ecf1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T19:03:19.457251Z",
     "start_time": "2025-09-08T19:03:17.395771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "grpo_train_split = rlhf_dataset.select(range(max_rm_ds_size, max_rm_ds_size + max_grpo_ds_size))\n",
    "\n",
    "def grpo_preproc(example):\n",
    "    example = {\n",
    "        'input_ids': tokenizer.apply_chat_template(\n",
    "            conv_str_to_msgs(example['chosen'])[:-1],\n",
    "            tokenize=True,\n",
    "            add_special_tokens=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    }\n",
    "    gen_start_ind = len(example['input_ids'])\n",
    "    max_context_length = model.config.max_position_embeddings\n",
    "    example['exclude'] = gen_start_ind > max_context_length\n",
    "\n",
    "    return example\n",
    "\n",
    "grpo_train_split = grpo_train_split.map(grpo_preproc).filter(exclude_filter)"
   ],
   "id": "113c615d1fdb465c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:01<00:00, 1655.26 examples/s]\n",
      "Filter: 100%|██████████| 2000/2000 [00:00<00:00, 23347.02 examples/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T18:59:04.544894Z",
     "start_time": "2025-08-29T18:59:04.541143Z"
    }
   },
   "cell_type": "code",
   "source": "grpo_train_split[0]",
   "id": "5f1bdf0a253168a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': \"\\n\\nHuman: Why don't homeless people just get jobs?\\n\\nAssistant: Why don't they?\",\n",
       " 'rejected': \"\\n\\nHuman: Why don't homeless people just get jobs?\\n\\nAssistant: I can understand your point of view, as the statistics show that being homeless has a significant negative effect on employment rates.\",\n",
       " 'input_ids': [27,\n",
       "  91,\n",
       "  320,\n",
       "  62,\n",
       "  9688,\n",
       "  91,\n",
       "  29,\n",
       "  10734,\n",
       "  198,\n",
       "  4162,\n",
       "  836,\n",
       "  470,\n",
       "  10463,\n",
       "  661,\n",
       "  655,\n",
       "  651,\n",
       "  3946,\n",
       "  30,\n",
       "  27,\n",
       "  91,\n",
       "  320,\n",
       "  62,\n",
       "  437,\n",
       "  91,\n",
       "  29,\n",
       "  198,\n",
       "  27,\n",
       "  91,\n",
       "  320,\n",
       "  62,\n",
       "  9688,\n",
       "  91,\n",
       "  29,\n",
       "  562,\n",
       "  10167,\n",
       "  198],\n",
       " 'gen_start_ind': 36,\n",
       " 'exclude': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T21:30:06.633626Z",
     "start_time": "2025-09-08T21:30:01.268274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "trainer takes in prompts (hf dataset abstraction so we can process it further within the trainer class)\n",
    "'''\n",
    "class GRPOTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataloader,\n",
    "        max_gen_tokens,\n",
    "        groups,\n",
    "        ramble_penalty\n",
    "    ):\n",
    "        self.dataloader = dataloader\n",
    "        self.max_gen_tokens = max_gen_tokens\n",
    "        self.groups = groups\n",
    "        self.ramble_penalty = ramble_penalty\n",
    "\n",
    "    def train(self):\n",
    "        it = 1\n",
    "        for batch in self.dataloader:\n",
    "            # generate full trajectories, this is of size (groups * batch_size, S) where S is padded length\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                tjs = model.generate(\n",
    "                    input_ids=batch['input_ids'].to(device),\n",
    "                    attention_mask=batch['attention_mask'].to(device),\n",
    "                    max_new_tokens=self.max_gen_tokens,\n",
    "                    num_return_sequences=self.groups,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "\n",
    "            # generate embeddings of last non padding token ONLY for trajectories not overflowing max gen\n",
    "            # not very efficient for now lol but that is a todo\n",
    "            rewards = []\n",
    "            stripped_tjs = []\n",
    "            gen_lens = []\n",
    "            for i in range(len(tjs)):\n",
    "                # strip entire generation and calculate generation length for giving rambling penalties\n",
    "                stripped = tokenizer.encode(\n",
    "                    tokenizer.decode(tjs[i], skip_special_tokens=True)\n",
    "                )\n",
    "                stripped_tjs.append(stripped)\n",
    "                gen_length = len(stripped[len(batch['input_ids'][i//self.groups]):])\n",
    "                gen_lens.append(gen_length)\n",
    "\n",
    "                # give a negative penalty if generated tokens (without eos token) is greater than our desired length\n",
    "                if gen_length >= self.max_gen_tokens:\n",
    "                    rewards.append(self.ramble_penalty)\n",
    "                    continue\n",
    "\n",
    "                # add back eos token since it got stripped and get hidden state to get reward from rm\n",
    "                stripped.append(tokenizer.eos_token_id)\n",
    "                hidden_state = model.transformer(input_ids=torch.tensor(stripped).unsqueeze(0)).last_hidden_state\n",
    "                final_tok_emb = hidden_state[0, -1]\n",
    "                rewards.append(reward_model(final_tok_emb).item())\n",
    "\n",
    "\n",
    "            # at this point, length of rewards list should be (groups * batch_size)\n",
    "            # print(\"\\n\\nPRINTING REWARDS: \\n\\n\")\n",
    "            # print(\"len of rewards: \", len(rewards))\n",
    "            # print(\"group * batch_size:\", self.groups * batch_size)\n",
    "            # print(rewards)\n",
    "\n",
    "            # reshape rewards to do adv est calcs\n",
    "            rewards = torch.tensor(rewards).reshape((batch_size, self.groups))\n",
    "\n",
    "            # use regular GRPO and calculate adv ests\n",
    "            # print(\"Group size:\", self.groups)\n",
    "            # print(\"Batch size:\", batch_size)\n",
    "            baseline = torch.mean(rewards, dim=1)\n",
    "\n",
    "            # print(\"Rewards shape:\", rewards.shape)\n",
    "            # print(\"Baseline shape:\", baseline.shape)\n",
    "            #\n",
    "            adv = (rewards.reshape((batch_size, self.groups)) - baseline.unsqueeze(dim=1)).flatten()\n",
    "\n",
    "            # collect states + actions for trajectories\n",
    "            states = []\n",
    "            acts = []\n",
    "            for t in range(len(stripped_tjs)):\n",
    "                for g in range(gen_lens[t]):\n",
    "                    tj = stripped_tjs[t]\n",
    "                    p = len(tj)-gen_lens[t]+g\n",
    "                    states.append(tj[:p])\n",
    "                    acts.append(tj[p:p+1])\n",
    "\n",
    "            # verify that each subsequent step is the last state + act\n",
    "            def validate_state_acts():\n",
    "                i = 0\n",
    "                for t in range(len(stripped_tjs)):\n",
    "                    for _ in range(gen_lens[t]-1):\n",
    "                        if states[i] + acts[i] != states[i+1]:\n",
    "                            return False\n",
    "                        i += 1\n",
    "                    i += 1\n",
    "                return True\n",
    "\n",
    "            assert validate_state_acts(), \"invalid state, action pairs\"\n",
    "\n",
    "            # calculate loss\n",
    "            # pad states and pass through model to get logits\n",
    "            pd_states = tokenizer.pad(\n",
    "                states,\n",
    "                padding=True,\n",
    "                padding_side='left',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            losses = []\n",
    "            for _ in range(opt_st_per_b):\n",
    "                b = torch.arange(len(states))\n",
    "                with torch.no_grad():\n",
    "                    og_policy_logits = model(**pd_states)[b, acts]\n",
    "                policy_logits = model(**pd_states)[b, acts]\n",
    "                r = torch.exp(policy_logits-og_policy_logits)\n",
    "                c = torch.clip(r, 1-eps, 1+eps)\n",
    "                token_losses = -torch.min(r*adv, c*adv)\n",
    "                kl_div = beta*(policy_logits-og_policy_logits)\n",
    "                loss = (token_losses+kl_div).mean()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                grpo_opt.zero_grad()\n",
    "                loss.backward()\n",
    "                grpo_opt.step()\n",
    "\n",
    "            it += 1\n",
    "\n",
    "\n",
    "            print(\"--------------------\")\n",
    "            print(\"Iteration:\", it)\n",
    "            print(\"Avg loss:\", torch.stack(losses).mean())\n",
    "            print(\"Adv ests:\", adv)\n",
    "            print(\"Avg reward by row:\", rewards.mean(dim=1))\n",
    "\n",
    "\n",
    "class DataCollatorForGRPO(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        features = [{'input_ids': example['input_ids']} for example in features]\n",
    "        batch = super().__call__(features)\n",
    "        return batch\n",
    "\n",
    "\n",
    "grpo_data_collator = DataCollatorForGRPO(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "grpo_dataloader = DataLoader(\n",
    "    grpo_train_split,\n",
    "    batch_size=grpo_batch_size,\n",
    "    collate_fn=grpo_data_collator,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    dataloader=grpo_dataloader,\n",
    "    max_gen_tokens=128,\n",
    "    groups=4,\n",
    "    ramble_penalty=-0.5\n",
    ")\n",
    "\n",
    "grpo_trainer.train()\n"
   ],
   "id": "db9bb96d7b069846",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PRINTING REWARDS: \n",
      "\n",
      "\n",
      "len of rewards:  8\n",
      "group * batch_size: 8\n",
      "[-0.5, -0.5, -0.5, -0.5, 1.0503084659576416, 1.2558788061141968, 1.2235876321792603, 0.9300314784049988]\n",
      "Group size: 4\n",
      "Batch size: 2\n",
      "Rewards shape: torch.Size([2, 4])\n",
      "Baseline shape: torch.Size([2])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b2dc5976b9e29856"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
