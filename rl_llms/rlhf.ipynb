{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### RLHF on pretrained GPT2\n",
    "Run post training on a pretrained GPT-2 model to understand RLHF. Steps will be SFT -> train reward model -> run grpo on pretrained llm on reward model. Rather than using TRL, I will be implementing grpo myself. Implementation will start with single gpu and then scaled to distributed system. This is an interactive notebook where I can write out the main algorithms but will make separate files for actual training."
   ],
   "id": "6a402223a4106937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T22:57:01.779200Z",
     "start_time": "2025-08-29T22:57:00.101539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "\n",
    "device = torch.device('cpu' if torch.backends.mps.is_available() else 'cpu')\n",
    "print('using device:', device)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "model = model.to(torch.float32)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompt = \"How easy is it to fake an arson attack and collect the insurance money\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ],
   "id": "a36fde68ad87b98c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How easy is it to fake an arson attack and collect the insurance money?\n",
      "\n",
      "The answer is simple: you can't.\n",
      "\n",
      "The law requires that you pay the insurance company for the damage you cause. The insurance company will pay the insurance\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T22:28:48.438104Z",
     "start_time": "2025-08-29T22:28:48.435139Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.decode(50256))",
   "id": "848a09f61da48081",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Supervised fine tuning\n",
    "\n",
    "What I need to do:\n",
    "1. Preprocess data into chat template with EOS token. Ensure data is padded and make sure batches are truncated to fit context length.\n",
    "2. Iterate through every batch and for each one calculate the loss (ONLY on the last assistant completion so the model learns prompt prediction). We use cross entropy btw.\n",
    "3. Run a number of epochs on it.\n",
    "4. Keep single threaded till we implement grpo as well."
   ],
   "id": "fb9e9f51ec732eb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T17:59:08.115322Z",
     "start_time": "2025-08-29T17:58:38.464425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "import copy\n",
    "\n",
    "# ---------------\n",
    "# hyperparameters / config\n",
    "num_epochs = 5\n",
    "batch_size = 2\n",
    "lr = 5e-5\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# ---------------\n",
    "\n",
    "# create dataset train/val/test splits\n",
    "train_sft_dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split='train_sft').select(range(1000))\n",
    "train_split_size = int(0.9 * len(train_sft_dataset))\n",
    "train_split = train_sft_dataset.select(range(train_split_size))\n",
    "val_split = train_sft_dataset.select(range(train_split_size, len(train_sft_dataset)))\n",
    "\n",
    "# create chat template for tokenizer to use, gpt2 uses eos token so we need to add that as well\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{%- for message in messages %}\n",
    "    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|im_start|>assistant\\n' }}\n",
    "{%- else %}\n",
    "    {{- eos_token }}\n",
    "{%- endif %}\n",
    "\"\"\"\n",
    "\n",
    "# preprocess data and create dataloader\n",
    "ending_msg_token_len = len(tokenizer.encode('<|im_end|>\\n'))\n",
    "def add_chat_tem(example):\n",
    "    example['og_messages'] = copy.deepcopy(example['messages'])\n",
    "    max_context_length = model.config.max_position_embeddings\n",
    "    has_system = 1 if example['messages'][0]['role'] == 'system' else 0\n",
    "    while True:\n",
    "        enc_chat_tem_ex = tokenizer.apply_chat_template(example['messages'], tokenize=True, add_special_tokens=False)\n",
    "        diff = len(enc_chat_tem_ex) - max_context_length\n",
    "        if diff <= 0:\n",
    "            example['exclude'] = False\n",
    "            break\n",
    "        elif len(example['messages']) // 2 <= 1:\n",
    "            example['exclude'] = True\n",
    "            break\n",
    "\n",
    "        del example['messages'][0 + has_system]\n",
    "        del example['messages'][0 + has_system]\n",
    "\n",
    "\n",
    "    # convert to chat template and keep track of # of tokens in last generation\n",
    "    enc_chat_tem_ex = tokenizer.apply_chat_template(example['messages'], tokenize=True, add_special_tokens=False)\n",
    "    example['input_ids'] = enc_chat_tem_ex\n",
    "    end_size = (len(tokenizer.encode(example['messages'][-1]['content'], add_special_tokens=False)) + ending_msg_token_len)\n",
    "    last_gen_start_ind = len(enc_chat_tem_ex) - end_size\n",
    "    example['last_gen_start_ind'] = last_gen_start_ind\n",
    "    return example\n",
    "\n",
    "exclude_filter = lambda x: x['exclude'] == False\n",
    "train_split = train_split.map(add_chat_tem).filter(exclude_filter)\n",
    "val_split = val_split.map(add_chat_tem).filter(exclude_filter)\n",
    "\n",
    "# create custom collator for sft\n",
    "class DataCollatorForSFT(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        last_gen_start_inds = [example['last_gen_start_ind'] for example in features]\n",
    "        features = [{'input_ids': example['input_ids']} for example in features]\n",
    "        batch = super().__call__(features, return_tensors=return_tensors)\n",
    "        # scrappy but just assume we're calling with return_tensors='pt'\n",
    "        batch['last_gen_start_inds'] = torch.tensor(last_gen_start_inds)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSFT(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_split,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_split,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "val_batch = next(iter(val_dataloader))"
   ],
   "id": "b8e444928958616b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 900/900 [00:08<00:00, 102.49 examples/s]\n",
      "Filter: 100%|██████████| 900/900 [00:00<00:00, 5724.02 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:01<00:00, 90.56 examples/s]\n",
      "Filter: 100%|██████████| 100/100 [00:00<00:00, 5330.77 examples/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T18:00:13.742989Z",
     "start_time": "2025-08-29T18:00:13.729737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pprint(val_split)\n",
    "pprint(train_split)\n",
    "# pprint(val_batch)\n",
    "\n",
    "pprint(train_split[0])"
   ],
   "id": "6ed83db81e20cf58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'messages', 'og_messages', 'exclude', 'input_ids', 'last_gen_start_ind'],\n",
      "    num_rows: 99\n",
      "})\n",
      "Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'messages', 'og_messages', 'exclude', 'input_ids', 'last_gen_start_ind'],\n",
      "    num_rows: 885\n",
      "})\n",
      "{'exclude': False,\n",
      " 'input_ids': [27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               4711,\n",
      "               7729,\n",
      "               4174,\n",
      "               284,\n",
      "               2665,\n",
      "               12,\n",
      "               3106,\n",
      "               13460,\n",
      "               357,\n",
      "               19309,\n",
      "               684,\n",
      "               425,\n",
      "               718,\n",
      "               13,\n",
      "               15,\n",
      "               28200,\n",
      "               4990,\n",
      "               1437,\n",
      "               604,\n",
      "               13,\n",
      "               15,\n",
      "               28200,\n",
      "               2547,\n",
      "               439,\n",
      "               897,\n",
      "               513,\n",
      "               13,\n",
      "               15,\n",
      "               10,\n",
      "               22278,\n",
      "               362,\n",
      "               13,\n",
      "               15,\n",
      "               28200,\n",
      "               16540,\n",
      "               17517,\n",
      "               642,\n",
      "               13,\n",
      "               15,\n",
      "               10,\n",
      "               737,\n",
      "               1867,\n",
      "               7505,\n",
      "               2196,\n",
      "               716,\n",
      "               314,\n",
      "               1262,\n",
      "               30,\n",
      "               198,\n",
      "               2202,\n",
      "               534,\n",
      "               50004,\n",
      "               5468,\n",
      "               1222,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               11,\n",
      "               345,\n",
      "               460,\n",
      "               3538,\n",
      "               905,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               286,\n",
      "               257,\n",
      "               1720,\n",
      "               319,\n",
      "               20599,\n",
      "               416,\n",
      "               15882,\n",
      "               530,\n",
      "               286,\n",
      "               262,\n",
      "               7505,\n",
      "               338,\n",
      "               3170,\n",
      "               12,\n",
      "               259,\n",
      "               6460,\n",
      "               0,\n",
      "               198,\n",
      "               7120,\n",
      "               12251,\n",
      "               5468,\n",
      "               1222,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               481,\n",
      "               783,\n",
      "               3359,\n",
      "               262,\n",
      "               9233,\n",
      "               1720,\n",
      "               2939,\n",
      "               655,\n",
      "               416,\n",
      "               33627,\n",
      "               625,\n",
      "               326,\n",
      "               1720,\n",
      "               2939,\n",
      "               40901,\n",
      "               13,\n",
      "               198,\n",
      "               13921,\n",
      "               428,\n",
      "               3895,\n",
      "               4174,\n",
      "               284,\n",
      "               477,\n",
      "               9004,\n",
      "               286,\n",
      "               262,\n",
      "               7505,\n",
      "               393,\n",
      "               655,\n",
      "               2176,\n",
      "               3392,\n",
      "               355,\n",
      "               5610,\n",
      "               287,\n",
      "               262,\n",
      "               2420,\n",
      "               2587,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               1212,\n",
      "               3895,\n",
      "               691,\n",
      "               8991,\n",
      "               284,\n",
      "               12251,\n",
      "               5468,\n",
      "               290,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               286,\n",
      "               262,\n",
      "               2665,\n",
      "               12,\n",
      "               3106,\n",
      "               13460,\n",
      "               5610,\n",
      "               287,\n",
      "               262,\n",
      "               2420,\n",
      "               2587,\n",
      "               29847,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               6090,\n",
      "               345,\n",
      "               5698,\n",
      "               502,\n",
      "               832,\n",
      "               262,\n",
      "               1429,\n",
      "               286,\n",
      "               15882,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               319,\n",
      "               616,\n",
      "               12251,\n",
      "               5468,\n",
      "               290,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               19457,\n",
      "               11,\n",
      "               994,\n",
      "               389,\n",
      "               262,\n",
      "               4831,\n",
      "               284,\n",
      "               7139,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               319,\n",
      "               534,\n",
      "               12251,\n",
      "               5468,\n",
      "               290,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               25,\n",
      "               198,\n",
      "               198,\n",
      "               16,\n",
      "               13,\n",
      "               5972,\n",
      "               287,\n",
      "               284,\n",
      "               534,\n",
      "               13705,\n",
      "               1958,\n",
      "               1848,\n",
      "               290,\n",
      "               467,\n",
      "               284,\n",
      "               534,\n",
      "               7467,\n",
      "               9363,\n",
      "               13,\n",
      "               198,\n",
      "               17,\n",
      "               13,\n",
      "               6914,\n",
      "               319,\n",
      "               8562,\n",
      "               1096,\n",
      "               7505,\n",
      "               329,\n",
      "               262,\n",
      "               2665,\n",
      "               12,\n",
      "               3106,\n",
      "               7505,\n",
      "               345,\n",
      "               389,\n",
      "               1262,\n",
      "               13,\n",
      "               198,\n",
      "               18,\n",
      "               13,\n",
      "               13244,\n",
      "               10055,\n",
      "               284,\n",
      "               262,\n",
      "               12251,\n",
      "               5468,\n",
      "               393,\n",
      "               38188,\n",
      "               50004,\n",
      "               2665,\n",
      "               810,\n",
      "               345,\n",
      "               765,\n",
      "               284,\n",
      "               7139,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               13,\n",
      "               198,\n",
      "               19,\n",
      "               13,\n",
      "               2080,\n",
      "               262,\n",
      "               2665,\n",
      "               1280,\n",
      "               11,\n",
      "               3904,\n",
      "               319,\n",
      "               262,\n",
      "               2665,\n",
      "               338,\n",
      "               4634,\n",
      "               357,\n",
      "               31763,\n",
      "               8,\n",
      "               7196,\n",
      "               287,\n",
      "               262,\n",
      "               1353,\n",
      "               12,\n",
      "               9464,\n",
      "               5228,\n",
      "               13,\n",
      "               198,\n",
      "               20,\n",
      "               13,\n",
      "               554,\n",
      "               262,\n",
      "               6460,\n",
      "               6103,\n",
      "               326,\n",
      "               3568,\n",
      "               11,\n",
      "               804,\n",
      "               329,\n",
      "               281,\n",
      "               3038,\n",
      "               15494,\n",
      "               705,\n",
      "               5159,\n",
      "               3359,\n",
      "               6,\n",
      "               393,\n",
      "               705,\n",
      "               5159,\n",
      "               20599,\n",
      "               4458,\n",
      "               198,\n",
      "               21,\n",
      "               13,\n",
      "               1002,\n",
      "               1695,\n",
      "               11,\n",
      "               2922,\n",
      "               705,\n",
      "               15307,\n",
      "               9233,\n",
      "               2939,\n",
      "               319,\n",
      "               20599,\n",
      "               4458,\n",
      "               198,\n",
      "               22,\n",
      "               13,\n",
      "               12793,\n",
      "               262,\n",
      "               2458,\n",
      "               290,\n",
      "               12714,\n",
      "               262,\n",
      "               12251,\n",
      "               14,\n",
      "               37948,\n",
      "               12251,\n",
      "               2443,\n",
      "               284,\n",
      "               766,\n",
      "               262,\n",
      "               1245,\n",
      "               13,\n",
      "               198,\n",
      "               198,\n",
      "               1532,\n",
      "               345,\n",
      "               821,\n",
      "               1719,\n",
      "               5876,\n",
      "               4917,\n",
      "               262,\n",
      "               4634,\n",
      "               11,\n",
      "               262,\n",
      "               1266,\n",
      "               1517,\n",
      "               284,\n",
      "               466,\n",
      "               318,\n",
      "               3522,\n",
      "               284,\n",
      "               534,\n",
      "               7505,\n",
      "               338,\n",
      "               10314,\n",
      "               11,\n",
      "               1201,\n",
      "               262,\n",
      "               4067,\n",
      "               290,\n",
      "               27393,\n",
      "               286,\n",
      "               6460,\n",
      "               460,\n",
      "               7565,\n",
      "               1022,\n",
      "               13460,\n",
      "               29847,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               6090,\n",
      "               345,\n",
      "               2148,\n",
      "               502,\n",
      "               351,\n",
      "               257,\n",
      "               2792,\n",
      "               284,\n",
      "               262,\n",
      "               10314,\n",
      "               329,\n",
      "               616,\n",
      "               7505,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               40,\n",
      "               836,\n",
      "               470,\n",
      "               423,\n",
      "               1895,\n",
      "               284,\n",
      "               534,\n",
      "               3650,\n",
      "               338,\n",
      "               7505,\n",
      "               1321,\n",
      "               13,\n",
      "               2102,\n",
      "               11,\n",
      "               345,\n",
      "               460,\n",
      "               3221,\n",
      "               1064,\n",
      "               262,\n",
      "               10314,\n",
      "               329,\n",
      "               534,\n",
      "               7505,\n",
      "               416,\n",
      "               1016,\n",
      "               284,\n",
      "               262,\n",
      "               6128,\n",
      "               1958,\n",
      "               7505,\n",
      "               3650,\n",
      "               11,\n",
      "               4917,\n",
      "               534,\n",
      "               7505,\n",
      "               290,\n",
      "               12264,\n",
      "               319,\n",
      "               262,\n",
      "               705,\n",
      "               11284,\n",
      "               6,\n",
      "               2792,\n",
      "               5140,\n",
      "               287,\n",
      "               262,\n",
      "               4220,\n",
      "               826,\n",
      "               5228,\n",
      "               286,\n",
      "               262,\n",
      "               2443,\n",
      "               13,\n",
      "               25929,\n",
      "               11,\n",
      "               345,\n",
      "               460,\n",
      "               466,\n",
      "               257,\n",
      "               23645,\n",
      "               2989,\n",
      "               329,\n",
      "               262,\n",
      "               1438,\n",
      "               286,\n",
      "               534,\n",
      "               7505,\n",
      "               3940,\n",
      "               416,\n",
      "               705,\n",
      "               22897,\n",
      "               341,\n",
      "               6,\n",
      "               393,\n",
      "               705,\n",
      "               7220,\n",
      "               5698,\n",
      "               4458,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               6090,\n",
      "               345,\n",
      "               6216,\n",
      "               611,\n",
      "               428,\n",
      "               3895,\n",
      "               635,\n",
      "               2499,\n",
      "               329,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               286,\n",
      "               616,\n",
      "               7505,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               464,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               743,\n",
      "               393,\n",
      "               743,\n",
      "               407,\n",
      "               670,\n",
      "               329,\n",
      "               534,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               11,\n",
      "               6906,\n",
      "               319,\n",
      "               262,\n",
      "               8398,\n",
      "               286,\n",
      "               534,\n",
      "               7505,\n",
      "               13,\n",
      "               2773,\n",
      "               13460,\n",
      "               2291,\n",
      "               428,\n",
      "               3895,\n",
      "               287,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               416,\n",
      "               4277,\n",
      "               11,\n",
      "               981,\n",
      "               1854,\n",
      "               743,\n",
      "               2421,\n",
      "               3224,\n",
      "               31344,\n",
      "               13,\n",
      "               1675,\n",
      "               2198,\n",
      "               611,\n",
      "               428,\n",
      "               3895,\n",
      "               318,\n",
      "               1695,\n",
      "               329,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               286,\n",
      "               534,\n",
      "               7505,\n",
      "               11,\n",
      "               1061,\n",
      "               777,\n",
      "               4831,\n",
      "               25,\n",
      "               198,\n",
      "               198,\n",
      "               16,\n",
      "               13,\n",
      "               1514,\n",
      "               284,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               810,\n",
      "               345,\n",
      "               561,\n",
      "               588,\n",
      "               284,\n",
      "               7139,\n",
      "               262,\n",
      "               3895,\n",
      "               13,\n",
      "               362,\n",
      "               13,\n",
      "               6914,\n",
      "               319,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               6460,\n",
      "               7196,\n",
      "               357,\n",
      "               31763,\n",
      "               7196,\n",
      "               8,\n",
      "               290,\n",
      "               804,\n",
      "               329,\n",
      "               705,\n",
      "               5159,\n",
      "               3359,\n",
      "               6,\n",
      "               393,\n",
      "               705,\n",
      "               5159,\n",
      "               20599,\n",
      "               4458,\n",
      "               513,\n",
      "               13,\n",
      "               1002,\n",
      "               1695,\n",
      "               11,\n",
      "               2922,\n",
      "               705,\n",
      "               15307,\n",
      "               9233,\n",
      "               2939,\n",
      "               319,\n",
      "               20599,\n",
      "               4458,\n",
      "               604,\n",
      "               13,\n",
      "               12793,\n",
      "               262,\n",
      "               2458,\n",
      "               13,\n",
      "               1002,\n",
      "               428,\n",
      "               3038,\n",
      "               318,\n",
      "               407,\n",
      "               1695,\n",
      "               287,\n",
      "               534,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               6460,\n",
      "               11,\n",
      "               345,\n",
      "               743,\n",
      "               761,\n",
      "               284,\n",
      "               3151,\n",
      "               503,\n",
      "               284,\n",
      "               534,\n",
      "               7505,\n",
      "               8517,\n",
      "               329,\n",
      "               6829,\n",
      "               351,\n",
      "               2183,\n",
      "               2890,\n",
      "               534,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               284,\n",
      "               2291,\n",
      "               428,\n",
      "               3895,\n",
      "               29847,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               50256],\n",
      " 'last_gen_start_ind': 613,\n",
      " 'messages': [{'content': 'These instructions apply to section-based themes '\n",
      "                          '(Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo '\n",
      "                          '2.0+, Mobilia 5.0+). What theme version am I '\n",
      "                          'using?\\n'\n",
      "                          'On your Collections pages & Featured Collections '\n",
      "                          'sections, you can easily show the secondary image '\n",
      "                          'of a product on hover by enabling one of the '\n",
      "                          \"theme's built-in settings!\\n\"\n",
      "                          'Your Collection pages & Featured Collections '\n",
      "                          'sections will now display the secondary product '\n",
      "                          'image just by hovering over that product image '\n",
      "                          'thumbnail.\\n'\n",
      "                          'Does this feature apply to all sections of the '\n",
      "                          'theme or just specific ones as listed in the text '\n",
      "                          'material?',\n",
      "               'role': 'user'},\n",
      "              {'content': 'This feature only applies to Collection pages and '\n",
      "                          'Featured Collections sections of the section-based '\n",
      "                          'themes listed in the text material.',\n",
      "               'role': 'assistant'},\n",
      "              {'content': 'Can you guide me through the process of enabling '\n",
      "                          'the secondary image hover feature on my Collection '\n",
      "                          'pages and Featured Collections sections?',\n",
      "               'role': 'user'},\n",
      "              {'content': 'Sure, here are the steps to enable the secondary '\n",
      "                          'image hover feature on your Collection pages and '\n",
      "                          'Featured Collections sections:\\n'\n",
      "                          '\\n'\n",
      "                          '1. Log in to your Shopify account and go to your '\n",
      "                          'Online Store.\\n'\n",
      "                          '2. Click on Customize theme for the section-based '\n",
      "                          'theme you are using.\\n'\n",
      "                          '3. Navigate to the Collection pages or Featured '\n",
      "                          'Collections section where you want to enable the '\n",
      "                          'secondary image hover feature.\\n'\n",
      "                          \"4. With the section open, click on the section's \"\n",
      "                          'setting (gear) icon in the top-left corner.\\n'\n",
      "                          '5. In the settings panel that appears, look for an '\n",
      "                          \"option labeled 'Image display' or 'Image hover'.\\n\"\n",
      "                          \"6. If available, select 'Show secondary image on \"\n",
      "                          \"hover'.\\n\"\n",
      "                          '7. Save the changes and preview the '\n",
      "                          'Collection/Featured Collection page to see the '\n",
      "                          'effect.\\n'\n",
      "                          '\\n'\n",
      "                          \"If you're having trouble finding the setting, the \"\n",
      "                          \"best thing to do is refer to your theme's \"\n",
      "                          'documentation, since the location and labeling of '\n",
      "                          'settings can vary between themes.',\n",
      "               'role': 'assistant'},\n",
      "              {'content': 'Can you provide me with a link to the documentation '\n",
      "                          'for my theme?',\n",
      "               'role': 'user'},\n",
      "              {'content': \"I don't have access to your store's theme \"\n",
      "                          'information. However, you can usually find the '\n",
      "                          'documentation for your theme by going to the '\n",
      "                          'shopify theme store, finding your theme and '\n",
      "                          \"clicking on the 'support' link located in the \"\n",
      "                          'bottom right corner of the page. Alternatively, you '\n",
      "                          'can do a google search for the name of your theme '\n",
      "                          \"followed by 'documentation' or 'user guide'.\",\n",
      "               'role': 'assistant'},\n",
      "              {'content': 'Can you confirm if this feature also works for the '\n",
      "                          'Quick Shop section of my theme?',\n",
      "               'role': 'user'},\n",
      "              {'content': 'The secondary image hover feature may or may not '\n",
      "                          'work for your Quick Shop section, depending on the '\n",
      "                          'configuration of your theme. Some themes include '\n",
      "                          'this feature in the Quick Shop section by default, '\n",
      "                          'while others may require additional customization. '\n",
      "                          'To check if this feature is available for the Quick '\n",
      "                          'Shop section of your theme, follow these steps:\\n'\n",
      "                          '\\n'\n",
      "                          '1. Go to the Quick Shop section where you would '\n",
      "                          'like to enable the feature. 2. Click on the Quick '\n",
      "                          \"Shop settings icon (gear icon) and look for 'Image \"\n",
      "                          \"display' or 'Image hover'. 3. If available, select \"\n",
      "                          \"'Show secondary image on hover'. 4. Save the \"\n",
      "                          'changes. If this option is not available in your '\n",
      "                          'Quick Shop section settings, you may need to reach '\n",
      "                          'out to your theme developer for assistance with '\n",
      "                          'customizing your Quick Shop section to include this '\n",
      "                          'feature.',\n",
      "               'role': 'assistant'}],\n",
      " 'og_messages': [{'content': 'These instructions apply to section-based themes '\n",
      "                             '(Responsive 6.0+, Retina 4.0+, Parallax 3.0+ '\n",
      "                             'Turbo 2.0+, Mobilia 5.0+). What theme version am '\n",
      "                             'I using?\\n'\n",
      "                             'On your Collections pages & Featured Collections '\n",
      "                             'sections, you can easily show the secondary '\n",
      "                             'image of a product on hover by enabling one of '\n",
      "                             \"the theme's built-in settings!\\n\"\n",
      "                             'Your Collection pages & Featured Collections '\n",
      "                             'sections will now display the secondary product '\n",
      "                             'image just by hovering over that product image '\n",
      "                             'thumbnail.\\n'\n",
      "                             'Does this feature apply to all sections of the '\n",
      "                             'theme or just specific ones as listed in the '\n",
      "                             'text material?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': 'This feature only applies to Collection pages '\n",
      "                             'and Featured Collections sections of the '\n",
      "                             'section-based themes listed in the text '\n",
      "                             'material.',\n",
      "                  'role': 'assistant'},\n",
      "                 {'content': 'Can you guide me through the process of enabling '\n",
      "                             'the secondary image hover feature on my '\n",
      "                             'Collection pages and Featured Collections '\n",
      "                             'sections?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': 'Sure, here are the steps to enable the secondary '\n",
      "                             'image hover feature on your Collection pages and '\n",
      "                             'Featured Collections sections:\\n'\n",
      "                             '\\n'\n",
      "                             '1. Log in to your Shopify account and go to your '\n",
      "                             'Online Store.\\n'\n",
      "                             '2. Click on Customize theme for the '\n",
      "                             'section-based theme you are using.\\n'\n",
      "                             '3. Navigate to the Collection pages or Featured '\n",
      "                             'Collections section where you want to enable the '\n",
      "                             'secondary image hover feature.\\n'\n",
      "                             \"4. With the section open, click on the section's \"\n",
      "                             'setting (gear) icon in the top-left corner.\\n'\n",
      "                             '5. In the settings panel that appears, look for '\n",
      "                             \"an option labeled 'Image display' or 'Image \"\n",
      "                             \"hover'.\\n\"\n",
      "                             \"6. If available, select 'Show secondary image on \"\n",
      "                             \"hover'.\\n\"\n",
      "                             '7. Save the changes and preview the '\n",
      "                             'Collection/Featured Collection page to see the '\n",
      "                             'effect.\\n'\n",
      "                             '\\n'\n",
      "                             \"If you're having trouble finding the setting, \"\n",
      "                             \"the best thing to do is refer to your theme's \"\n",
      "                             'documentation, since the location and labeling '\n",
      "                             'of settings can vary between themes.',\n",
      "                  'role': 'assistant'},\n",
      "                 {'content': 'Can you provide me with a link to the '\n",
      "                             'documentation for my theme?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': \"I don't have access to your store's theme \"\n",
      "                             'information. However, you can usually find the '\n",
      "                             'documentation for your theme by going to the '\n",
      "                             'shopify theme store, finding your theme and '\n",
      "                             \"clicking on the 'support' link located in the \"\n",
      "                             'bottom right corner of the page. Alternatively, '\n",
      "                             'you can do a google search for the name of your '\n",
      "                             \"theme followed by 'documentation' or 'user \"\n",
      "                             \"guide'.\",\n",
      "                  'role': 'assistant'},\n",
      "                 {'content': 'Can you confirm if this feature also works for '\n",
      "                             'the Quick Shop section of my theme?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': 'The secondary image hover feature may or may not '\n",
      "                             'work for your Quick Shop section, depending on '\n",
      "                             'the configuration of your theme. Some themes '\n",
      "                             'include this feature in the Quick Shop section '\n",
      "                             'by default, while others may require additional '\n",
      "                             'customization. To check if this feature is '\n",
      "                             'available for the Quick Shop section of your '\n",
      "                             'theme, follow these steps:\\n'\n",
      "                             '\\n'\n",
      "                             '1. Go to the Quick Shop section where you would '\n",
      "                             'like to enable the feature. 2. Click on the '\n",
      "                             'Quick Shop settings icon (gear icon) and look '\n",
      "                             \"for 'Image display' or 'Image hover'. 3. If \"\n",
      "                             \"available, select 'Show secondary image on \"\n",
      "                             \"hover'. 4. Save the changes. If this option is \"\n",
      "                             'not available in your Quick Shop section '\n",
      "                             'settings, you may need to reach out to your '\n",
      "                             'theme developer for assistance with customizing '\n",
      "                             'your Quick Shop section to include this feature.',\n",
      "                  'role': 'assistant'}],\n",
      " 'prompt': 'These instructions apply to section-based themes (Responsive 6.0+, '\n",
      "           'Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme '\n",
      "           'version am I using?\\n'\n",
      "           'On your Collections pages & Featured Collections sections, you can '\n",
      "           'easily show the secondary image of a product on hover by enabling '\n",
      "           \"one of the theme's built-in settings!\\n\"\n",
      "           'Your Collection pages & Featured Collections sections will now '\n",
      "           'display the secondary product image just by hovering over that '\n",
      "           'product image thumbnail.\\n'\n",
      "           'Does this feature apply to all sections of the theme or just '\n",
      "           'specific ones as listed in the text material?',\n",
      " 'prompt_id': 'f0e37e9f7800261167ce91143f98f511f768847236f133f2d0aed60b444ebe57'}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T18:00:15.008386Z",
     "start_time": "2025-08-29T18:00:15.006395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# keep running list of training + val loss for logging\n",
    "losses_per_epoch = []\n",
    "latest_epoch_losses = []"
   ],
   "id": "782cd2195efbc066",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T18:00:24.092036Z",
     "start_time": "2025-08-29T18:00:15.940632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_model_loss(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    last_gen_start_inds = batch['last_gen_start_inds'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    # print(f\"Batch shape: {input_ids.shape}\")  # This is [batch_size, sequence_length]\n",
    "    # print(f\"Sequence length: {input_ids.shape[1]}\")\n",
    "    # print(f\"Memory before forward pass: {torch.mps.driver_allocated_memory() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "    # mask labels that aren't included in last gen\n",
    "    mask = torch.arange(input_ids.shape[1], device=device) < last_gen_start_inds[:, None]\n",
    "    labels[mask] = -100\n",
    "\n",
    "    # run forward pass and calc loss\n",
    "    model_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # print(f\"Memory after forward pass: {torch.mps.driver_allocated_memory() / 1024**3:.2f} GB\")\n",
    "\n",
    "    # calculate loss\n",
    "    B, T, C = model_outputs.logits.shape\n",
    "    logits = model_outputs.logits.view(B*T, C)\n",
    "    labels = labels.view(B*T)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# training run\n",
    "model.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    step = 0\n",
    "    train_dl_iter = iter(train_dataloader)\n",
    "    print(f\"\\n Starting epoch: {epoch}\")\n",
    "    for batch in train_dl_iter:\n",
    "        step += 1\n",
    "        # calculate training loss\n",
    "        training_loss = calc_model_loss(batch)\n",
    "        optimizer.zero_grad()\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate val loss\n",
    "        avg_val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            val_dl_iter = iter(val_dataloader)\n",
    "            for val_batch in val_dl_iter:\n",
    "                avg_val_loss += calc_model_loss(val_batch).item()\n",
    "                val_batches += 1\n",
    "            avg_val_loss /= val_batches\n",
    "\n",
    "        latest_epoch_losses.append((training_loss.item(), avg_val_loss))\n",
    "        print(f\"epoch: {epoch} | step: {step} | training loss: {training_loss} | validation loss: {avg_val_loss}\")\n",
    "\n",
    "    losses_per_epoch.append(latest_epoch_losses)\n",
    "    latest_epoch_losses = []"
   ],
   "id": "71d7c703271a5893",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting epoch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 50\u001B[39m\n\u001B[32m     48\u001B[39m val_dl_iter = \u001B[38;5;28miter\u001B[39m(val_dataloader)\n\u001B[32m     49\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m val_batch \u001B[38;5;129;01min\u001B[39;00m val_dl_iter:\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m     avg_val_loss += \u001B[43mcalc_model_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m     val_batches += \u001B[32m1\u001B[39m\n\u001B[32m     52\u001B[39m avg_val_loss /= val_batches\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training the reward model",
   "id": "6e77f8f5907b1bc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T18:00:27.553947Z",
     "start_time": "2025-08-29T18:00:27.503135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reward model\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size * 4)\n",
    "        self.fc2 = nn.Linear(input_size * 4, input_size)\n",
    "        self.fc3 = nn.Linear(input_size, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "reward_model = RewardModel(model.config.n_embd).to(device)\n",
    "\n",
    "# ---------------\n",
    "# rm hyperparameters / config\n",
    "rm_batch_size = 2\n",
    "rm_epochs = 5\n",
    "rm_lr = 1e-5\n",
    "rm_weight_decay = 0.01\n",
    "rm_optimizer = torch.optim.AdamW(reward_model.parameters(), lr=rm_lr, weight_decay=rm_weight_decay)\n",
    "max_rm_ds_size = 1000\n",
    "# ---------------"
   ],
   "id": "68558aadb95b116a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T18:00:55.172520Z",
     "start_time": "2025-08-29T18:00:30.018149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# we will use Anthropic/hh-rlhf for training our reward model\n",
    "rlhf_dataset = load_dataset(\"Anthropic/hh-rlhf\", split='train')\n",
    "rm_train_dataset = rlhf_dataset.select(range(max_rm_ds_size))\n",
    "rm_train_split_size = int(0.9 * len(rm_train_dataset))\n",
    "rm_train_split = rm_train_dataset.select(range(rm_train_split_size))\n",
    "rm_val_split = rm_train_dataset.select(range(rm_train_split_size, len(rm_train_dataset)))\n",
    "\n",
    "def conv_str_to_msgs(str):\n",
    "    split = [line.strip() for line in re.split(r'(?=Human:|Assistant:)', str) if line.strip()]\n",
    "    msgs = []\n",
    "    for s in split:\n",
    "        role, content = s.split(':', 1)\n",
    "        msgs.append({'role': role.lower(), 'content': content})\n",
    "    return msgs\n",
    "\n",
    "def rm_preproc(example):\n",
    "    ex_proc = {\n",
    "        key: tokenizer.apply_chat_template(\n",
    "            conv_str_to_msgs(value),\n",
    "            tokenize=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        for key, value in example.items()\n",
    "    }\n",
    "\n",
    "    max_context_length = model.config.max_position_embeddings\n",
    "    ex_proc['exclude'] = (\n",
    "            len(ex_proc['chosen']) > max_context_length or len(ex_proc['rejected']) > max_context_length\n",
    "    )\n",
    "\n",
    "    return ex_proc\n",
    "\n",
    "rm_train_split = rm_train_split.map(rm_preproc).filter(exclude_filter)\n",
    "rm_val_split = rm_val_split.map(rm_preproc).filter(exclude_filter)\n",
    "\n",
    "\n",
    "# create custom collator for rlhf data\n",
    "class DataCollatorForRm:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding = True\n",
    "\n",
    "        self.clt = DataCollatorWithPadding(\n",
    "            tokenizer,\n",
    "            padding=self.padding,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        chosen = [{'input_ids': b['chosen']} for b in batch]\n",
    "        rejected = [{'input_ids': b['rejected']} for b in batch]\n",
    "        chosen_padded = self.clt(chosen)\n",
    "        rejected_padded = self.clt(rejected)\n",
    "\n",
    "        return {'chosen': chosen_padded, 'rejected': rejected_padded}\n",
    "\n",
    "\n",
    "rm_data_collator = DataCollatorForRm(tokenizer=tokenizer)\n",
    "\n",
    "rm_train_dataloader = DataLoader(\n",
    "    rm_train_split,\n",
    "    batch_size=rm_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=rm_data_collator\n",
    ")\n",
    "\n",
    "rm_val_dataloader = DataLoader(\n",
    "    rm_val_split,\n",
    "    batch_size=rm_batch_size,\n",
    "    collate_fn=rm_data_collator,\n",
    "    shuffle=False\n",
    ")\n",
    "\n"
   ],
   "id": "8f3c488397c223de",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 900/900 [00:01<00:00, 665.45 examples/s]\n",
      "Filter: 100%|██████████| 900/900 [00:00<00:00, 10202.80 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 651.08 examples/s]\n",
      "Filter: 100%|██████████| 100/100 [00:00<00:00, 9547.48 examples/s]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T22:00:31.895532Z",
     "start_time": "2025-08-27T22:00:31.808334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_rm_val(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    outputs = model.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # only get the emb for final token\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    final_tok_ind = torch.sum(attention_mask, dim=1) - 1\n",
    "    batch_inds = torch.arange(hidden_states.shape[0], device=device)\n",
    "    final_tok_emb = hidden_states[batch_inds, final_tok_ind]\n",
    "\n",
    "    return reward_model(final_tok_emb)\n",
    "\n",
    "def calc_rm_loss(ch_rm_val, rej_rm_val):\n",
    "    return -F.logsigmoid(ch_rm_val - rej_rm_val).mean()\n",
    "\n",
    "def calc_val_perc_correct(val_batch):\n",
    "    rej_rm_vals = calc_rm_val(val_batch['rejected'])\n",
    "    ch_rm_vals = calc_rm_val(val_batch['chosen'])\n",
    "    correct = (ch_rm_vals > rej_rm_vals).int().sum()\n",
    "    return (correct / len(rej_rm_vals)).item()\n",
    "\n",
    "\n",
    "# reward model training loop\n",
    "reward_model.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    step = 0\n",
    "    rm_train_dl_iter = iter(rm_train_dataloader)\n",
    "    for batch in rm_train_dl_iter:\n",
    "        step += 1\n",
    "        rej = batch['rejected'].to(device)\n",
    "        ch = batch['chosen'].to(device)\n",
    "        rm_loss = calc_rm_loss(calc_rm_val(ch), calc_rm_val(rej))\n",
    "\n",
    "        # calculate avg val dataset % classified correct\n",
    "        avg_val_per_right = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            rm_val_dl_iter = iter(rm_val_dataloader)\n",
    "            for val_batch in rm_val_dl_iter:\n",
    "                avg_val_per_right += calc_val_perc_correct(val_batch)\n",
    "                val_batches += 1\n",
    "            avg_val_per_right /= val_batches\n",
    "\n",
    "        print(f\"epoch: {epoch} | step: {step} | training loss: {rm_loss} | avg val % correct preference: {avg_val_per_right}\")\n",
    "        rm_optimizer.zero_grad()\n",
    "        rm_loss.backward()\n",
    "        rm_optimizer.step()"
   ],
   "id": "daf721071ca1e02e",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reward_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m     21\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (correct / \u001B[38;5;28mlen\u001B[39m(rej_rm_vals)).item()\n\u001B[32m     24\u001B[39m \u001B[38;5;66;03m# reward model training loop\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m \u001B[43mreward_model\u001B[49m.train()\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m, num_epochs+\u001B[32m1\u001B[39m):\n\u001B[32m     27\u001B[39m     step = \u001B[32m0\u001B[39m\n",
      "\u001B[31mNameError\u001B[39m: name 'reward_model' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GRPO implementation + rl training loop",
   "id": "96dba371d81ad028"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T18:58:17.872323Z",
     "start_time": "2025-08-29T18:58:17.870718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------\n",
    "# grpo hyperparameters / config\n",
    "grpo_batch_size = 2\n",
    "max_grpo_ds_size = 2000\n",
    "# ---------------"
   ],
   "id": "af69a7b4cd20ecf1",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T19:01:55.013901Z",
     "start_time": "2025-08-29T19:01:44.399259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "grpo_train_split = rlhf_dataset.select(range(max_rm_ds_size, max_rm_ds_size + max_grpo_ds_size))\n",
    "\n",
    "def grpo_preproc(example):\n",
    "    example = {\n",
    "        'input_ids': tokenizer.apply_chat_template(\n",
    "            conv_str_to_msgs(example['chosen'])[:-1],\n",
    "            tokenize=True,\n",
    "            add_special_tokens=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    }\n",
    "    gen_start_ind = len(example['input_ids'])\n",
    "    example['last_gen_start_ind'] = gen_start_ind\n",
    "    max_context_length = model.config.max_position_embeddings\n",
    "    example['exclude'] = gen_start_ind > max_context_length\n",
    "\n",
    "    return example\n",
    "\n",
    "grpo_train_split = grpo_train_split.map(grpo_preproc).filter(exclude_filter)"
   ],
   "id": "113c615d1fdb465c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:01<00:00, 1699.13 examples/s]\n",
      "Filter: 100%|██████████| 2000/2000 [00:00<00:00, 25322.80 examples/s]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T18:59:04.544894Z",
     "start_time": "2025-08-29T18:59:04.541143Z"
    }
   },
   "cell_type": "code",
   "source": "grpo_train_split[0]",
   "id": "5f1bdf0a253168a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': \"\\n\\nHuman: Why don't homeless people just get jobs?\\n\\nAssistant: Why don't they?\",\n",
       " 'rejected': \"\\n\\nHuman: Why don't homeless people just get jobs?\\n\\nAssistant: I can understand your point of view, as the statistics show that being homeless has a significant negative effect on employment rates.\",\n",
       " 'input_ids': [27,\n",
       "  91,\n",
       "  320,\n",
       "  62,\n",
       "  9688,\n",
       "  91,\n",
       "  29,\n",
       "  10734,\n",
       "  198,\n",
       "  4162,\n",
       "  836,\n",
       "  470,\n",
       "  10463,\n",
       "  661,\n",
       "  655,\n",
       "  651,\n",
       "  3946,\n",
       "  30,\n",
       "  27,\n",
       "  91,\n",
       "  320,\n",
       "  62,\n",
       "  437,\n",
       "  91,\n",
       "  29,\n",
       "  198,\n",
       "  27,\n",
       "  91,\n",
       "  320,\n",
       "  62,\n",
       "  9688,\n",
       "  91,\n",
       "  29,\n",
       "  562,\n",
       "  10167,\n",
       "  198],\n",
       " 'gen_start_ind': 36,\n",
       " 'exclude': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-29T22:57:36.137293Z",
     "start_time": "2025-08-29T22:57:31.084228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "GRPO implementation notes:\n",
    "1. We will use epochs instead of iterations so we batch and use all our data each time.\n",
    "\n",
    "Pseudo alg:\n",
    "1. Collect trajectories. For an llm, a trajectory includes an entire prompt with its completion (in chat template form with the eos index to extract embedding to input into the reward model). This would be a 2d matrix btw where each row corresponds to a group of completions for a prompt. REMEMBER THAT THE 2D MATRIX TRAJECTORY WOULD BE SIZE BxTxP where P is the embedding dimension size from the eos token. That embedding contains a ton of contextual information about the sequence plus meanings. Initial state is the prompt and each action is generated token\n",
    "2. Create a matrix of similar size to the trajectories 2d matrix but this contains the reward from the rm for those completions. Use that 2d matrix to create a vector of advantage estimates.\n",
    "3. Now with the advantage estimates, see if we can get probs for all timesteps and create the loss in an efficient batched manner (remember make it negative since we want to maximize that though).\n",
    "4. Run backprop on the surrogate loss (we can do this multiple times).\n",
    "5. Do this on all the prompts in the set for multiple epochs.\n",
    "'''\n",
    "\n",
    "\n",
    "# expects data under column called prompts to be already in chat template + tokenized with assistant thing at bottom (just need to pass add_generation_prompt=True at the end)\n",
    "'''\n",
    "trainer takes in prompts (hf dataset abstraction so we can process it further within the trainer class)\n",
    "'''\n",
    "class GRPOTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataloader,\n",
    "        max_gen_tokens,\n",
    "        groups\n",
    "    ):\n",
    "        self.dataloader = dataloader\n",
    "        self.max_gen_tokens = max_gen_tokens\n",
    "        self.groups = groups\n",
    "\n",
    "    def train(self):\n",
    "        for batch in self.dataloader:\n",
    "            # generate trajectories\n",
    "\n",
    "            # print(batch['input_ids'])\n",
    "\n",
    "            batch_decoded = tokenizer.batch_decode(batch['input_ids'])\n",
    "            # for b in batch_decoded:\n",
    "            #     print('\\n\\n')\n",
    "            #     print(b)\n",
    "            #     print('\\n\\n')\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                tjs = model.generate(\n",
    "                    input_ids=batch['input_ids'].to(device),\n",
    "                    attention_mask=batch['attention_mask'].to(device),\n",
    "                    max_new_tokens=self.max_gen_tokens,\n",
    "                    num_return_sequences=self.groups,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    do_sample=True,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "                \"\"\"\n",
    "                what im expecting:\n",
    "                1d matrix of generated text (tokenized) of size P x G where P\n",
    "                is the number of prompts and G is the number of groups\n",
    "                \"\"\"\n",
    "\n",
    "            # print(tjs)\n",
    "            print(\"STARTING TJS PRINTING\\n\\n\")\n",
    "\n",
    "            batch_decoded_tjs = tokenizer.batch_decode(tjs)\n",
    "            for b in batch_decoded_tjs:\n",
    "                print('\\n\\n NEW')\n",
    "                print(b)\n",
    "                print('\\n\\n END')\n",
    "                print(\"\\n\\n og\")\n",
    "                print(batch_decoded[0])\n",
    "                break\n",
    "\n",
    "            print(\"ENDING THAT \\n\\n\")\n",
    "\n",
    "            \"\"\"\n",
    "            for trajs that DID end in eos (others get slight negative reward for rambling),\n",
    "            pass through the llm and take out final embedding for it (in a batched manner).\n",
    "            receive a matrix of size ((P x G) - R) x H where R is the # of responses that went\n",
    "            on for too long and H is the hidden embedding size.\n",
    "            input into reward model and construct a column vector of final rewards for all generations\n",
    "            (across all groups). average across groups and calculate advantage estimates.\n",
    "            then get logits for policy and create grpo loss and backprop on that\n",
    "            \"\"\"\n",
    "\n",
    "            return\n",
    "\n",
    "\n",
    "class DataCollatorForGRPO(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        last_gen_start_inds = [example['last_gen_start_ind'] for example in features]\n",
    "        features = [{'input_ids': example['input_ids']} for example in features]\n",
    "        batch = super().__call__(features)\n",
    "        batch['last_gen_start_inds'] = torch.tensor(last_gen_start_inds)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "grpo_data_collator = DataCollatorForGRPO(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "grpo_dataloader = DataLoader(\n",
    "    grpo_train_split,\n",
    "    batch_size=grpo_batch_size,\n",
    "    collate_fn=grpo_data_collator,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    dataloader=grpo_dataloader,\n",
    "    max_gen_tokens=128,\n",
    "    groups=4\n",
    ")\n",
    "\n",
    "grpo_trainer.train()\n"
   ],
   "id": "db9bb96d7b069846",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TJS PRINTING\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " NEW\n",
      "<|im_start|>human\n",
      " How can I sneak into college?<|im_end|>\n",
      "<|im_start|>assistant\n",
      " I’m sorry, what do you mean?<|im_end|>\n",
      "<|im_start|>human\n",
      " How do I get into a college with bad grades?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|im_start|>assistant\n",
      "</|\n",
      "\n",
      "<|im_start|>assistant\n",
      "\n",
      "</|\n",
      "\n",
      "<|im_start|>assistant\n",
      "\n",
      "</|\n",
      "\n",
      "[3]\n",
      "\n",
      "<|im_end|>human\n",
      "\n",
      "<|im_start|>assguard\n",
      "\n",
      "<|im_end|>assistant\n",
      "\n",
      "<|im_start|>assguard</|\n",
      "\n",
      "<|im_start|>assguard\n",
      "\n",
      "<|im_start|>assguard</|\n",
      "\n",
      "<|im_start|>assguard\n",
      "\n",
      "\n",
      "\n",
      " END\n",
      "\n",
      "\n",
      " og\n",
      "<|im_start|>human\n",
      " How can I sneak into college?<|im_end|>\n",
      "<|im_start|>assistant\n",
      " I’m sorry, what do you mean?<|im_end|>\n",
      "<|im_start|>human\n",
      " How do I get into a college with bad grades?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "ENDING THAT \n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b2358ae8e3a4e69e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
