{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### RLHF Demo\n",
    "Run post training on a pretrained GPT-2 model to understand RLHF. Steps will be SFT -> train reward model -> run grpo on pretrained llm on reward model. Rather than using TRL, I will be implementing grpo myself. Implementation will start with single gpu and then scaled to distributed system."
   ],
   "id": "6a402223a4106937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T00:36:27.162345Z",
     "start_time": "2025-08-21T00:36:03.947157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompt = \"The usual weather in California is\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=1000,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ],
   "id": "a36fde68ad87b98c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The usual weather in California is a bit of a mess.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but the clouds are still thick.\n",
      "\n",
      "The sun is shining, but\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Supervised fine tuning\n",
    "\n",
    "What I need to do:\n",
    "1. Preprocess data into chat template with EOS token. Ensure data is padded and make sure batches are truncated to fit context length.\n",
    "2. Iterate through every batch and for each one calculate the loss (ONLY on the last assistant completion so the model learns prompt prediction). We use cross entropy btw.\n",
    "3. Run a number of epochs on it.\n",
    "4. Keep single threaded till we implement grpo as well."
   ],
   "id": "fb9e9f51ec732eb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T18:21:14.760067Z",
     "start_time": "2025-08-21T18:21:13.066065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "\n",
    "# ---------------\n",
    "# hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 1\n",
    "# ---------------\n",
    "\n",
    "# create dataset train/val/test splits\n",
    "train_sft_dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split='train_sft').select(range(1000))\n",
    "train_split_size = int(0.9 * len(train_sft_dataset))\n",
    "train_split = train_sft_dataset.select(range(train_split_size))\n",
    "val_split = train_sft_dataset.select(range(train_split_size, len(train_sft_dataset)))\n",
    "\n",
    "# create chat template for tokenizer to use, gpt2 uses eos token so we need to add that as well\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{%- for message in messages %}\n",
    "    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|im_start|>assistant\\n' }}\n",
    "{%- else %}\n",
    "    {{- eos_token }}\n",
    "{%- endif %}\n",
    "\"\"\"\n",
    "\n",
    "# preprocess data and create dataloader\n",
    "ending_msg_token_len = len(tokenizer.encode('<|im_end|>\\n'))\n",
    "def add_chat_tem(example):\n",
    "    # convert to chat template and keep track of # of tokens in last generation\n",
    "    example['input_ids'] = tokenizer.apply_chat_template(example['messages'], tokenize=True, add_special_tokens=False)\n",
    "    example['last_gen_token_len'] = len(tokenizer.encode(example['messages'][-1]['content'], add_special_tokens=False)) + ending_msg_token_len\n",
    "    return example\n",
    "\n",
    "\n",
    "train_split = train_split.map(add_chat_tem)\n",
    "val_split = val_split.map(add_chat_tem)\n",
    "\n",
    "# create custom collator for sft\n",
    "class DataCollatorForSFT(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        last_gen_token_lens = [example['last_gen_token_len'] for example in features]\n",
    "        features = [{'input_ids': feature['input_ids'] for feature in features}]\n",
    "        batch = super().__call__(features, return_tensors=return_tensors)\n",
    "        # scrappy but just assume we're calling with return_tensors='pt'\n",
    "        batch['last_gen_token_lens'] = torch.tensor(last_gen_token_lens)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSFT(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_split,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")"
   ],
   "id": "b8e444928958616b",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T18:21:14.800347Z",
     "start_time": "2025-08-21T18:21:14.791235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "BxT -> BxTxP\n",
    "A couple of issues I need to work out:\n",
    "1. Padding inputs_ids in each prompt in the match correctly and ensuring attention mask is right\n",
    "2. After we feed the prompt, we need to remove everything but the corresponding amount of tokens from the end of the prompt corresponding to the last assistant generation. We need to calculate labels that way simimlarly.\n",
    "3. Calculate the loss for the sliced part (but each batch inner matrix is different size so need to think about that).\n",
    "4. Backprop on that to train the model.\n",
    "'''\n",
    "\n",
    "# training run\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        pprint(batch)\n",
    "        # print(len(batch[0]['input_ids']))\n",
    "        # When given multi-turn data, only include generation of final turn in loss\n",
    "        # padded_inputs = tokenizer.pad(batch, padding=True, return_tensors='pt')\n",
    "        # pprint(padded_inputs.keys())\n",
    "        # print(list(padded_inputs.keys()))\n",
    "        # padded_inputs = {\n",
    "        #     'input_ids': torch.tensor(padded_inputs['input_ids']),\n",
    "        #     'attention_mask': torch.tensor(padded_inputs['attention_mask'])\n",
    "        # }\n",
    "        # output_logits = model(**padded_inputs)\n",
    "        #\n",
    "        # last_gen_token_lens = [prompt['last_gen_token_len'] for prompt in batch]\n",
    "        # print(padded_inputs[0]['attention_mask'])\n",
    "\n",
    "\n",
    "        # print(tokenizer.decode(batch[0]['input_ids'][-batch[0]['last_gen_token_len']:])[0] == '\\n')\n",
    "        # print(\"----------\")\n",
    "        break\n",
    "    break"
   ],
   "id": "71d7c703271a5893",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[   27,    91,   320,    62,  9688,    91,    29,  7220,   198,  2437,\n",
      "           867,   661, 31099,   284,   670,   416,  7161,   287, 12551,  3688,\n",
      "           284, 24377,    30,    27,    91,   320,    62,   437,    91,    29,\n",
      "           198,    27,    91,   320,    62,  9688,    91,    29,   562, 10167,\n",
      "           198,    40,   466,   407,   423,  1895,   284,   262,  3452,  1366,\n",
      "            13,  2102,    11,  1864,   284,   257,  2864,   989,   416,   262,\n",
      "          2243, 30347,  1096,  1486,  1664,    11,   716, 22506,   468,   262,\n",
      "          4511,  5873,   286, 36799, 11300,   416, 17026,   287,   597,  1813,\n",
      "          1110,   379,  5946,     4,   981,  1702, 11656,   373,   407,  3017,\n",
      "           287,   262,  1351, 29847,    91,   320,    62,   437,    91,    29,\n",
      "           198,    27,    91,   320,    62,  9688,    91,    29,  7220,   198,\n",
      "         22017,    11, 24377,   318,  1107,  8036,     0,   314,  4601,   517,\n",
      "          4736,   561, 32980, 16259,   355,   257,  1724,   286,  4839, 29847,\n",
      "            91,   320,    62,   437,    91,    29,   198,    27,    91,   320,\n",
      "            62,  9688,    91,    29,   562, 10167,   198,    40,  4236,   351,\n",
      "           345,    13, 25550,   355,   257,  1724,   286,  9358,   460,   423,\n",
      "          6409,  4034,    11,  1390,  8868,  4979, 28014,    11, 10068,  1633,\n",
      "          3081,    11, 11560,  3518,  3842,    11,   290, 27496,  4045,  7876,\n",
      "         15873,    13,  4650,  4736,  1088,   262,   995,   389, 22650,   777,\n",
      "          4034,   290, 14771,   287, 16259,  6884,    11,   475,   612,   318,\n",
      "           991,   517,   326,   460,   307,  1760,   284,   787, 16259,   257,\n",
      "          3338,   290, 11282,  3038,   329,   477, 29847,    91,   320,    62,\n",
      "           437,    91,    29,   198,    27,    91,   320,    62,  9688,    91,\n",
      "            29,  7220,   198,    40,  2911, 12551,   635,  4940, 19086,  2890,\n",
      "         16259,  2582,    13,   632,   338,   257,  1049,   835,   284,  3368,\n",
      "          4979,   290,  2652,  4197,   379,   262,   976,   640, 29847,    91,\n",
      "           320,    62,   437,    91,    29,   198,    27,    91,   320,    62,\n",
      "          9688,    91,    29,   562, 10167,   198,    40,  4236,   351,   345,\n",
      "            13, 12551,   468,  1541,   925,   617,  4040,   284,  7898, 16259,\n",
      "            11,   884,   355,  2615, 16259, 13532,   290,  7161,    12, 21987,\n",
      "         16546,    13,  2102,    11,   517,   460,   307,  1760,   284,   787,\n",
      "         16259,   257,  3338,   290, 11282,  3038,   329,   477,    11,   884,\n",
      "           355, 10068, 16259,  6884,    11,  4441,  7161,    12, 13120,  6483,\n",
      "            11,  4955,  5713,  7161,  7647,  7291,    11,   290,  6011, 16538,\n",
      "           329, 16259,    13, 10335, 10720, 16259,   460,   407,   691,  1037,\n",
      "          4646,  4979, 28014,   475,   635,  7719,   257, 22841, 12263,   290,\n",
      "           257, 10536,   877,  2858, 29847,    91,   320,    62,   437,    91,\n",
      "            29,   198, 50256]]),\n",
      " 'labels': tensor([[   27,    91,   320,    62,  9688,    91,    29,  7220,   198,  2437,\n",
      "           867,   661, 31099,   284,   670,   416,  7161,   287, 12551,  3688,\n",
      "           284, 24377,    30,    27,    91,   320,    62,   437,    91,    29,\n",
      "           198,    27,    91,   320,    62,  9688,    91,    29,   562, 10167,\n",
      "           198,    40,   466,   407,   423,  1895,   284,   262,  3452,  1366,\n",
      "            13,  2102,    11,  1864,   284,   257,  2864,   989,   416,   262,\n",
      "          2243, 30347,  1096,  1486,  1664,    11,   716, 22506,   468,   262,\n",
      "          4511,  5873,   286, 36799, 11300,   416, 17026,   287,   597,  1813,\n",
      "          1110,   379,  5946,     4,   981,  1702, 11656,   373,   407,  3017,\n",
      "           287,   262,  1351, 29847,    91,   320,    62,   437,    91,    29,\n",
      "           198,    27,    91,   320,    62,  9688,    91,    29,  7220,   198,\n",
      "         22017,    11, 24377,   318,  1107,  8036,     0,   314,  4601,   517,\n",
      "          4736,   561, 32980, 16259,   355,   257,  1724,   286,  4839, 29847,\n",
      "            91,   320,    62,   437,    91,    29,   198,    27,    91,   320,\n",
      "            62,  9688,    91,    29,   562, 10167,   198,    40,  4236,   351,\n",
      "           345,    13, 25550,   355,   257,  1724,   286,  9358,   460,   423,\n",
      "          6409,  4034,    11,  1390,  8868,  4979, 28014,    11, 10068,  1633,\n",
      "          3081,    11, 11560,  3518,  3842,    11,   290, 27496,  4045,  7876,\n",
      "         15873,    13,  4650,  4736,  1088,   262,   995,   389, 22650,   777,\n",
      "          4034,   290, 14771,   287, 16259,  6884,    11,   475,   612,   318,\n",
      "           991,   517,   326,   460,   307,  1760,   284,   787, 16259,   257,\n",
      "          3338,   290, 11282,  3038,   329,   477, 29847,    91,   320,    62,\n",
      "           437,    91,    29,   198,    27,    91,   320,    62,  9688,    91,\n",
      "            29,  7220,   198,    40,  2911, 12551,   635,  4940, 19086,  2890,\n",
      "         16259,  2582,    13,   632,   338,   257,  1049,   835,   284,  3368,\n",
      "          4979,   290,  2652,  4197,   379,   262,   976,   640, 29847,    91,\n",
      "           320,    62,   437,    91,    29,   198,    27,    91,   320,    62,\n",
      "          9688,    91,    29,   562, 10167,   198,    40,  4236,   351,   345,\n",
      "            13, 12551,   468,  1541,   925,   617,  4040,   284,  7898, 16259,\n",
      "            11,   884,   355,  2615, 16259, 13532,   290,  7161,    12, 21987,\n",
      "         16546,    13,  2102,    11,   517,   460,   307,  1760,   284,   787,\n",
      "         16259,   257,  3338,   290, 11282,  3038,   329,   477,    11,   884,\n",
      "           355, 10068, 16259,  6884,    11,  4441,  7161,    12, 13120,  6483,\n",
      "            11,  4955,  5713,  7161,  7647,  7291,    11,   290,  6011, 16538,\n",
      "           329, 16259,    13, 10335, 10720, 16259,   460,   407,   691,  1037,\n",
      "          4646,  4979, 28014,   475,   635,  7719,   257, 22841, 12263,   290,\n",
      "           257, 10536,   877,  2858, 29847,    91,   320,    62,   437,    91,\n",
      "            29,   198,  -100]]),\n",
      " 'last_gen_token_lens': tensor([97])}\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create the reward model",
   "id": "6e77f8f5907b1bc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:41:25.928628Z",
     "start_time": "2025-08-20T19:41:25.919996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The reward model generates a single logit representing the probability of that response (we use Bradley-Terry model of preferences)\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size * 4)\n",
    "        self.fc2 = nn.Linear(input_size * 4, 1)\n",
    "\n",
    "'''\n",
    "Preference training loop steps:\n",
    "1. Iterate through synthetic data of preferences.\n",
    "2. Use the final hidden state last embedding as input to reward network.\n",
    "3. Do that for both (otherwise you can use synthetic data and feed into the network the prompt + the response)\n",
    "4. Use that to get the embeddings for both (just do one pass)\n",
    "5. Calculate the loss based on Bradley-Terry and do backprop on the reward model network (you can mean the sums so you do it batchwise)\n",
    "\n",
    "The idea we'll use is take the prompt reward string to get rewrd but only train on prompt to avoid mismatch problem.\n",
    "'''\n"
   ],
   "id": "daf721071ca1e02e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPreference training loop steps:\\n1. Iterate through synthetic data of preferences.\\n2. Use the final hidden state last embedding as input to reward network.\\n3. Do that for both (otherwise you can use synthetic data and feed into the network the prompt + the response)\\n4. Use that to get the embeddings for both (just do one pass)\\n5. Calculate the loss based on Bradley-Terry and do backprop on the reward model network (you can mean the sums so you do it batchwise)\\n\\nThe idea we'll use is take the prompt reward string to get rewrd but only train on prompt to avoid mismatch problem.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:58:32.609858Z",
     "start_time": "2025-08-20T19:58:31.662841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_data = load_dataset('HuggingFaceH4/no_robots')['train']\n",
    "train_data[0]"
   ],
   "id": "d09fc1eced2f7d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert’s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. “We can’t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,” says Rinkert.',\n",
       " 'prompt_id': '627a77298cf96a309aa35a62207c4164e22a66f6db79119506228f28ddc0f947',\n",
       " 'messages': [{'content': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert’s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. “We can’t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,” says Rinkert.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.',\n",
       "   'role': 'assistant'}],\n",
       " 'category': 'Summarize'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6190e2927bd84be6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
