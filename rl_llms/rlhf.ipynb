{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### RLHF on pretrained GPT2\n",
    "Run post training on a pretrained GPT-2 model to understand RLHF. Steps will be SFT -> train reward model -> run grpo on pretrained llm on reward model. Rather than using TRL, I will be implementing grpo myself. Implementation will start with single gpu and then scaled to distributed system."
   ],
   "id": "6a402223a4106937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T22:50:01.533329Z",
     "start_time": "2025-08-22T22:49:56.646565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print('using device:', device)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompt = \"The usual weather in California is\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=10,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ],
   "id": "a36fde68ad87b98c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The usual weather in California is a bit of a\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Supervised fine tuning\n",
    "\n",
    "What I need to do:\n",
    "1. Preprocess data into chat template with EOS token. Ensure data is padded and make sure batches are truncated to fit context length.\n",
    "2. Iterate through every batch and for each one calculate the loss (ONLY on the last assistant completion so the model learns prompt prediction). We use cross entropy btw.\n",
    "3. Run a number of epochs on it.\n",
    "4. Keep single threaded till we implement grpo as well."
   ],
   "id": "fb9e9f51ec732eb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T22:53:19.593619Z",
     "start_time": "2025-08-22T22:52:51.493039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, load_dataset_builder, get_dataset_split_names\n",
    "from torch.utils.data import DataLoader\n",
    "from pprint import pprint\n",
    "import copy\n",
    "\n",
    "# ---------------\n",
    "# hyperparameters / config\n",
    "num_epochs = 5\n",
    "batch_size = 2\n",
    "lr = 5e-5\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# ---------------\n",
    "\n",
    "# create dataset train/val/test splits\n",
    "train_sft_dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split='train_sft').select(range(1000))\n",
    "train_split_size = int(0.9 * len(train_sft_dataset))\n",
    "train_split = train_sft_dataset.select(range(train_split_size))\n",
    "val_split = train_sft_dataset.select(range(train_split_size, len(train_sft_dataset)))\n",
    "\n",
    "# create chat template for tokenizer to use, gpt2 uses eos token so we need to add that as well\n",
    "tokenizer.chat_template = \"\"\"\n",
    "{%- for message in messages %}\n",
    "    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|im_start|>assistant\\n' }}\n",
    "{%- else %}\n",
    "    {{- eos_token }}\n",
    "{%- endif %}\n",
    "\"\"\"\n",
    "\n",
    "# preprocess data and create dataloader\n",
    "ending_msg_token_len = len(tokenizer.encode('<|im_end|>\\n'))\n",
    "def add_chat_tem(example):\n",
    "    example['og_messages'] = copy.deepcopy(example['messages'])\n",
    "    max_context_length = model.config.max_position_embeddings\n",
    "    has_system = 1 if example['messages'][0]['role'] == 'system' else 0\n",
    "    while True:\n",
    "        enc_chat_tem_ex = tokenizer.apply_chat_template(example['messages'], tokenize=True, add_special_tokens=False)\n",
    "        diff = len(enc_chat_tem_ex) - max_context_length\n",
    "        if diff <= 0:\n",
    "            example['exclude'] = False\n",
    "            break\n",
    "        elif len(example['messages']) // 2 <= 1:\n",
    "            example['exclude'] = True\n",
    "            break\n",
    "\n",
    "        del example['messages'][0 + has_system]\n",
    "        del example['messages'][0 + has_system]\n",
    "\n",
    "\n",
    "    # convert to chat template and keep track of # of tokens in last generation\n",
    "    enc_chat_tem_ex = tokenizer.apply_chat_template(example['messages'], tokenize=True, add_special_tokens=False)\n",
    "    example['input_ids'] = enc_chat_tem_ex\n",
    "    end_size = (len(tokenizer.encode(example['messages'][-1]['content'], add_special_tokens=False)) + ending_msg_token_len)\n",
    "    last_gen_start_ind = len(enc_chat_tem_ex) - end_size\n",
    "    example['last_gen_start_ind'] = last_gen_start_ind\n",
    "    return example\n",
    "\n",
    "exclude_filter = lambda x: x['exclude'] == False\n",
    "train_split = train_split.map(add_chat_tem).filter(exclude_filter)\n",
    "val_split = val_split.map(add_chat_tem).filter(exclude_filter)\n",
    "\n",
    "# create custom collator for sft\n",
    "class DataCollatorForSFT(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        last_gen_start_inds = [example['last_gen_start_ind'] for example in features]\n",
    "        features = [{'input_ids': example['input_ids']} for example in features]\n",
    "        batch = super().__call__(features, return_tensors=return_tensors)\n",
    "        # scrappy but just assume we're calling with return_tensors='pt'\n",
    "        batch['last_gen_start_inds'] = torch.tensor(last_gen_start_inds)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForSFT(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_split,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_split,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "val_batch = next(iter(val_dataloader))"
   ],
   "id": "b8e444928958616b",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T22:21:33.397347Z",
     "start_time": "2025-08-22T22:21:33.381541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pprint(val_split)\n",
    "pprint(train_split)\n",
    "# pprint(val_batch)\n",
    "\n",
    "pprint(train_split[0])"
   ],
   "id": "6ed83db81e20cf58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'messages', 'og_messages', 'exclude', 'input_ids', 'last_gen_start_ind'],\n",
      "    num_rows: 99\n",
      "})\n",
      "Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'messages', 'og_messages', 'exclude', 'input_ids', 'last_gen_start_ind'],\n",
      "    num_rows: 885\n",
      "})\n",
      "{'exclude': False,\n",
      " 'input_ids': [27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               4711,\n",
      "               7729,\n",
      "               4174,\n",
      "               284,\n",
      "               2665,\n",
      "               12,\n",
      "               3106,\n",
      "               13460,\n",
      "               357,\n",
      "               19309,\n",
      "               684,\n",
      "               425,\n",
      "               718,\n",
      "               13,\n",
      "               15,\n",
      "               28200,\n",
      "               4990,\n",
      "               1437,\n",
      "               604,\n",
      "               13,\n",
      "               15,\n",
      "               28200,\n",
      "               2547,\n",
      "               439,\n",
      "               897,\n",
      "               513,\n",
      "               13,\n",
      "               15,\n",
      "               10,\n",
      "               22278,\n",
      "               362,\n",
      "               13,\n",
      "               15,\n",
      "               28200,\n",
      "               16540,\n",
      "               17517,\n",
      "               642,\n",
      "               13,\n",
      "               15,\n",
      "               10,\n",
      "               737,\n",
      "               1867,\n",
      "               7505,\n",
      "               2196,\n",
      "               716,\n",
      "               314,\n",
      "               1262,\n",
      "               30,\n",
      "               198,\n",
      "               2202,\n",
      "               534,\n",
      "               50004,\n",
      "               5468,\n",
      "               1222,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               11,\n",
      "               345,\n",
      "               460,\n",
      "               3538,\n",
      "               905,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               286,\n",
      "               257,\n",
      "               1720,\n",
      "               319,\n",
      "               20599,\n",
      "               416,\n",
      "               15882,\n",
      "               530,\n",
      "               286,\n",
      "               262,\n",
      "               7505,\n",
      "               338,\n",
      "               3170,\n",
      "               12,\n",
      "               259,\n",
      "               6460,\n",
      "               0,\n",
      "               198,\n",
      "               7120,\n",
      "               12251,\n",
      "               5468,\n",
      "               1222,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               481,\n",
      "               783,\n",
      "               3359,\n",
      "               262,\n",
      "               9233,\n",
      "               1720,\n",
      "               2939,\n",
      "               655,\n",
      "               416,\n",
      "               33627,\n",
      "               625,\n",
      "               326,\n",
      "               1720,\n",
      "               2939,\n",
      "               40901,\n",
      "               13,\n",
      "               198,\n",
      "               13921,\n",
      "               428,\n",
      "               3895,\n",
      "               4174,\n",
      "               284,\n",
      "               477,\n",
      "               9004,\n",
      "               286,\n",
      "               262,\n",
      "               7505,\n",
      "               393,\n",
      "               655,\n",
      "               2176,\n",
      "               3392,\n",
      "               355,\n",
      "               5610,\n",
      "               287,\n",
      "               262,\n",
      "               2420,\n",
      "               2587,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               1212,\n",
      "               3895,\n",
      "               691,\n",
      "               8991,\n",
      "               284,\n",
      "               12251,\n",
      "               5468,\n",
      "               290,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               286,\n",
      "               262,\n",
      "               2665,\n",
      "               12,\n",
      "               3106,\n",
      "               13460,\n",
      "               5610,\n",
      "               287,\n",
      "               262,\n",
      "               2420,\n",
      "               2587,\n",
      "               29847,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               6090,\n",
      "               345,\n",
      "               5698,\n",
      "               502,\n",
      "               832,\n",
      "               262,\n",
      "               1429,\n",
      "               286,\n",
      "               15882,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               319,\n",
      "               616,\n",
      "               12251,\n",
      "               5468,\n",
      "               290,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               19457,\n",
      "               11,\n",
      "               994,\n",
      "               389,\n",
      "               262,\n",
      "               4831,\n",
      "               284,\n",
      "               7139,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               319,\n",
      "               534,\n",
      "               12251,\n",
      "               5468,\n",
      "               290,\n",
      "               38188,\n",
      "               50004,\n",
      "               9004,\n",
      "               25,\n",
      "               198,\n",
      "               198,\n",
      "               16,\n",
      "               13,\n",
      "               5972,\n",
      "               287,\n",
      "               284,\n",
      "               534,\n",
      "               13705,\n",
      "               1958,\n",
      "               1848,\n",
      "               290,\n",
      "               467,\n",
      "               284,\n",
      "               534,\n",
      "               7467,\n",
      "               9363,\n",
      "               13,\n",
      "               198,\n",
      "               17,\n",
      "               13,\n",
      "               6914,\n",
      "               319,\n",
      "               8562,\n",
      "               1096,\n",
      "               7505,\n",
      "               329,\n",
      "               262,\n",
      "               2665,\n",
      "               12,\n",
      "               3106,\n",
      "               7505,\n",
      "               345,\n",
      "               389,\n",
      "               1262,\n",
      "               13,\n",
      "               198,\n",
      "               18,\n",
      "               13,\n",
      "               13244,\n",
      "               10055,\n",
      "               284,\n",
      "               262,\n",
      "               12251,\n",
      "               5468,\n",
      "               393,\n",
      "               38188,\n",
      "               50004,\n",
      "               2665,\n",
      "               810,\n",
      "               345,\n",
      "               765,\n",
      "               284,\n",
      "               7139,\n",
      "               262,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               13,\n",
      "               198,\n",
      "               19,\n",
      "               13,\n",
      "               2080,\n",
      "               262,\n",
      "               2665,\n",
      "               1280,\n",
      "               11,\n",
      "               3904,\n",
      "               319,\n",
      "               262,\n",
      "               2665,\n",
      "               338,\n",
      "               4634,\n",
      "               357,\n",
      "               31763,\n",
      "               8,\n",
      "               7196,\n",
      "               287,\n",
      "               262,\n",
      "               1353,\n",
      "               12,\n",
      "               9464,\n",
      "               5228,\n",
      "               13,\n",
      "               198,\n",
      "               20,\n",
      "               13,\n",
      "               554,\n",
      "               262,\n",
      "               6460,\n",
      "               6103,\n",
      "               326,\n",
      "               3568,\n",
      "               11,\n",
      "               804,\n",
      "               329,\n",
      "               281,\n",
      "               3038,\n",
      "               15494,\n",
      "               705,\n",
      "               5159,\n",
      "               3359,\n",
      "               6,\n",
      "               393,\n",
      "               705,\n",
      "               5159,\n",
      "               20599,\n",
      "               4458,\n",
      "               198,\n",
      "               21,\n",
      "               13,\n",
      "               1002,\n",
      "               1695,\n",
      "               11,\n",
      "               2922,\n",
      "               705,\n",
      "               15307,\n",
      "               9233,\n",
      "               2939,\n",
      "               319,\n",
      "               20599,\n",
      "               4458,\n",
      "               198,\n",
      "               22,\n",
      "               13,\n",
      "               12793,\n",
      "               262,\n",
      "               2458,\n",
      "               290,\n",
      "               12714,\n",
      "               262,\n",
      "               12251,\n",
      "               14,\n",
      "               37948,\n",
      "               12251,\n",
      "               2443,\n",
      "               284,\n",
      "               766,\n",
      "               262,\n",
      "               1245,\n",
      "               13,\n",
      "               198,\n",
      "               198,\n",
      "               1532,\n",
      "               345,\n",
      "               821,\n",
      "               1719,\n",
      "               5876,\n",
      "               4917,\n",
      "               262,\n",
      "               4634,\n",
      "               11,\n",
      "               262,\n",
      "               1266,\n",
      "               1517,\n",
      "               284,\n",
      "               466,\n",
      "               318,\n",
      "               3522,\n",
      "               284,\n",
      "               534,\n",
      "               7505,\n",
      "               338,\n",
      "               10314,\n",
      "               11,\n",
      "               1201,\n",
      "               262,\n",
      "               4067,\n",
      "               290,\n",
      "               27393,\n",
      "               286,\n",
      "               6460,\n",
      "               460,\n",
      "               7565,\n",
      "               1022,\n",
      "               13460,\n",
      "               29847,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               6090,\n",
      "               345,\n",
      "               2148,\n",
      "               502,\n",
      "               351,\n",
      "               257,\n",
      "               2792,\n",
      "               284,\n",
      "               262,\n",
      "               10314,\n",
      "               329,\n",
      "               616,\n",
      "               7505,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               40,\n",
      "               836,\n",
      "               470,\n",
      "               423,\n",
      "               1895,\n",
      "               284,\n",
      "               534,\n",
      "               3650,\n",
      "               338,\n",
      "               7505,\n",
      "               1321,\n",
      "               13,\n",
      "               2102,\n",
      "               11,\n",
      "               345,\n",
      "               460,\n",
      "               3221,\n",
      "               1064,\n",
      "               262,\n",
      "               10314,\n",
      "               329,\n",
      "               534,\n",
      "               7505,\n",
      "               416,\n",
      "               1016,\n",
      "               284,\n",
      "               262,\n",
      "               6128,\n",
      "               1958,\n",
      "               7505,\n",
      "               3650,\n",
      "               11,\n",
      "               4917,\n",
      "               534,\n",
      "               7505,\n",
      "               290,\n",
      "               12264,\n",
      "               319,\n",
      "               262,\n",
      "               705,\n",
      "               11284,\n",
      "               6,\n",
      "               2792,\n",
      "               5140,\n",
      "               287,\n",
      "               262,\n",
      "               4220,\n",
      "               826,\n",
      "               5228,\n",
      "               286,\n",
      "               262,\n",
      "               2443,\n",
      "               13,\n",
      "               25929,\n",
      "               11,\n",
      "               345,\n",
      "               460,\n",
      "               466,\n",
      "               257,\n",
      "               23645,\n",
      "               2989,\n",
      "               329,\n",
      "               262,\n",
      "               1438,\n",
      "               286,\n",
      "               534,\n",
      "               7505,\n",
      "               3940,\n",
      "               416,\n",
      "               705,\n",
      "               22897,\n",
      "               341,\n",
      "               6,\n",
      "               393,\n",
      "               705,\n",
      "               7220,\n",
      "               5698,\n",
      "               4458,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               7220,\n",
      "               198,\n",
      "               6090,\n",
      "               345,\n",
      "               6216,\n",
      "               611,\n",
      "               428,\n",
      "               3895,\n",
      "               635,\n",
      "               2499,\n",
      "               329,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               286,\n",
      "               616,\n",
      "               7505,\n",
      "               30,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               27,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               9688,\n",
      "               91,\n",
      "               29,\n",
      "               562,\n",
      "               10167,\n",
      "               198,\n",
      "               464,\n",
      "               9233,\n",
      "               2939,\n",
      "               20599,\n",
      "               3895,\n",
      "               743,\n",
      "               393,\n",
      "               743,\n",
      "               407,\n",
      "               670,\n",
      "               329,\n",
      "               534,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               11,\n",
      "               6906,\n",
      "               319,\n",
      "               262,\n",
      "               8398,\n",
      "               286,\n",
      "               534,\n",
      "               7505,\n",
      "               13,\n",
      "               2773,\n",
      "               13460,\n",
      "               2291,\n",
      "               428,\n",
      "               3895,\n",
      "               287,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               416,\n",
      "               4277,\n",
      "               11,\n",
      "               981,\n",
      "               1854,\n",
      "               743,\n",
      "               2421,\n",
      "               3224,\n",
      "               31344,\n",
      "               13,\n",
      "               1675,\n",
      "               2198,\n",
      "               611,\n",
      "               428,\n",
      "               3895,\n",
      "               318,\n",
      "               1695,\n",
      "               329,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               286,\n",
      "               534,\n",
      "               7505,\n",
      "               11,\n",
      "               1061,\n",
      "               777,\n",
      "               4831,\n",
      "               25,\n",
      "               198,\n",
      "               198,\n",
      "               16,\n",
      "               13,\n",
      "               1514,\n",
      "               284,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               810,\n",
      "               345,\n",
      "               561,\n",
      "               588,\n",
      "               284,\n",
      "               7139,\n",
      "               262,\n",
      "               3895,\n",
      "               13,\n",
      "               362,\n",
      "               13,\n",
      "               6914,\n",
      "               319,\n",
      "               262,\n",
      "               12029,\n",
      "               13705,\n",
      "               6460,\n",
      "               7196,\n",
      "               357,\n",
      "               31763,\n",
      "               7196,\n",
      "               8,\n",
      "               290,\n",
      "               804,\n",
      "               329,\n",
      "               705,\n",
      "               5159,\n",
      "               3359,\n",
      "               6,\n",
      "               393,\n",
      "               705,\n",
      "               5159,\n",
      "               20599,\n",
      "               4458,\n",
      "               513,\n",
      "               13,\n",
      "               1002,\n",
      "               1695,\n",
      "               11,\n",
      "               2922,\n",
      "               705,\n",
      "               15307,\n",
      "               9233,\n",
      "               2939,\n",
      "               319,\n",
      "               20599,\n",
      "               4458,\n",
      "               604,\n",
      "               13,\n",
      "               12793,\n",
      "               262,\n",
      "               2458,\n",
      "               13,\n",
      "               1002,\n",
      "               428,\n",
      "               3038,\n",
      "               318,\n",
      "               407,\n",
      "               1695,\n",
      "               287,\n",
      "               534,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               6460,\n",
      "               11,\n",
      "               345,\n",
      "               743,\n",
      "               761,\n",
      "               284,\n",
      "               3151,\n",
      "               503,\n",
      "               284,\n",
      "               534,\n",
      "               7505,\n",
      "               8517,\n",
      "               329,\n",
      "               6829,\n",
      "               351,\n",
      "               2183,\n",
      "               2890,\n",
      "               534,\n",
      "               12029,\n",
      "               13705,\n",
      "               2665,\n",
      "               284,\n",
      "               2291,\n",
      "               428,\n",
      "               3895,\n",
      "               29847,\n",
      "               91,\n",
      "               320,\n",
      "               62,\n",
      "               437,\n",
      "               91,\n",
      "               29,\n",
      "               198,\n",
      "               50256],\n",
      " 'last_gen_start_ind': 613,\n",
      " 'messages': [{'content': 'These instructions apply to section-based themes '\n",
      "                          '(Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo '\n",
      "                          '2.0+, Mobilia 5.0+). What theme version am I '\n",
      "                          'using?\\n'\n",
      "                          'On your Collections pages & Featured Collections '\n",
      "                          'sections, you can easily show the secondary image '\n",
      "                          'of a product on hover by enabling one of the '\n",
      "                          \"theme's built-in settings!\\n\"\n",
      "                          'Your Collection pages & Featured Collections '\n",
      "                          'sections will now display the secondary product '\n",
      "                          'image just by hovering over that product image '\n",
      "                          'thumbnail.\\n'\n",
      "                          'Does this feature apply to all sections of the '\n",
      "                          'theme or just specific ones as listed in the text '\n",
      "                          'material?',\n",
      "               'role': 'user'},\n",
      "              {'content': 'This feature only applies to Collection pages and '\n",
      "                          'Featured Collections sections of the section-based '\n",
      "                          'themes listed in the text material.',\n",
      "               'role': 'assistant'},\n",
      "              {'content': 'Can you guide me through the process of enabling '\n",
      "                          'the secondary image hover feature on my Collection '\n",
      "                          'pages and Featured Collections sections?',\n",
      "               'role': 'user'},\n",
      "              {'content': 'Sure, here are the steps to enable the secondary '\n",
      "                          'image hover feature on your Collection pages and '\n",
      "                          'Featured Collections sections:\\n'\n",
      "                          '\\n'\n",
      "                          '1. Log in to your Shopify account and go to your '\n",
      "                          'Online Store.\\n'\n",
      "                          '2. Click on Customize theme for the section-based '\n",
      "                          'theme you are using.\\n'\n",
      "                          '3. Navigate to the Collection pages or Featured '\n",
      "                          'Collections section where you want to enable the '\n",
      "                          'secondary image hover feature.\\n'\n",
      "                          \"4. With the section open, click on the section's \"\n",
      "                          'setting (gear) icon in the top-left corner.\\n'\n",
      "                          '5. In the settings panel that appears, look for an '\n",
      "                          \"option labeled 'Image display' or 'Image hover'.\\n\"\n",
      "                          \"6. If available, select 'Show secondary image on \"\n",
      "                          \"hover'.\\n\"\n",
      "                          '7. Save the changes and preview the '\n",
      "                          'Collection/Featured Collection page to see the '\n",
      "                          'effect.\\n'\n",
      "                          '\\n'\n",
      "                          \"If you're having trouble finding the setting, the \"\n",
      "                          \"best thing to do is refer to your theme's \"\n",
      "                          'documentation, since the location and labeling of '\n",
      "                          'settings can vary between themes.',\n",
      "               'role': 'assistant'},\n",
      "              {'content': 'Can you provide me with a link to the documentation '\n",
      "                          'for my theme?',\n",
      "               'role': 'user'},\n",
      "              {'content': \"I don't have access to your store's theme \"\n",
      "                          'information. However, you can usually find the '\n",
      "                          'documentation for your theme by going to the '\n",
      "                          'shopify theme store, finding your theme and '\n",
      "                          \"clicking on the 'support' link located in the \"\n",
      "                          'bottom right corner of the page. Alternatively, you '\n",
      "                          'can do a google search for the name of your theme '\n",
      "                          \"followed by 'documentation' or 'user guide'.\",\n",
      "               'role': 'assistant'},\n",
      "              {'content': 'Can you confirm if this feature also works for the '\n",
      "                          'Quick Shop section of my theme?',\n",
      "               'role': 'user'},\n",
      "              {'content': 'The secondary image hover feature may or may not '\n",
      "                          'work for your Quick Shop section, depending on the '\n",
      "                          'configuration of your theme. Some themes include '\n",
      "                          'this feature in the Quick Shop section by default, '\n",
      "                          'while others may require additional customization. '\n",
      "                          'To check if this feature is available for the Quick '\n",
      "                          'Shop section of your theme, follow these steps:\\n'\n",
      "                          '\\n'\n",
      "                          '1. Go to the Quick Shop section where you would '\n",
      "                          'like to enable the feature. 2. Click on the Quick '\n",
      "                          \"Shop settings icon (gear icon) and look for 'Image \"\n",
      "                          \"display' or 'Image hover'. 3. If available, select \"\n",
      "                          \"'Show secondary image on hover'. 4. Save the \"\n",
      "                          'changes. If this option is not available in your '\n",
      "                          'Quick Shop section settings, you may need to reach '\n",
      "                          'out to your theme developer for assistance with '\n",
      "                          'customizing your Quick Shop section to include this '\n",
      "                          'feature.',\n",
      "               'role': 'assistant'}],\n",
      " 'og_messages': [{'content': 'These instructions apply to section-based themes '\n",
      "                             '(Responsive 6.0+, Retina 4.0+, Parallax 3.0+ '\n",
      "                             'Turbo 2.0+, Mobilia 5.0+). What theme version am '\n",
      "                             'I using?\\n'\n",
      "                             'On your Collections pages & Featured Collections '\n",
      "                             'sections, you can easily show the secondary '\n",
      "                             'image of a product on hover by enabling one of '\n",
      "                             \"the theme's built-in settings!\\n\"\n",
      "                             'Your Collection pages & Featured Collections '\n",
      "                             'sections will now display the secondary product '\n",
      "                             'image just by hovering over that product image '\n",
      "                             'thumbnail.\\n'\n",
      "                             'Does this feature apply to all sections of the '\n",
      "                             'theme or just specific ones as listed in the '\n",
      "                             'text material?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': 'This feature only applies to Collection pages '\n",
      "                             'and Featured Collections sections of the '\n",
      "                             'section-based themes listed in the text '\n",
      "                             'material.',\n",
      "                  'role': 'assistant'},\n",
      "                 {'content': 'Can you guide me through the process of enabling '\n",
      "                             'the secondary image hover feature on my '\n",
      "                             'Collection pages and Featured Collections '\n",
      "                             'sections?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': 'Sure, here are the steps to enable the secondary '\n",
      "                             'image hover feature on your Collection pages and '\n",
      "                             'Featured Collections sections:\\n'\n",
      "                             '\\n'\n",
      "                             '1. Log in to your Shopify account and go to your '\n",
      "                             'Online Store.\\n'\n",
      "                             '2. Click on Customize theme for the '\n",
      "                             'section-based theme you are using.\\n'\n",
      "                             '3. Navigate to the Collection pages or Featured '\n",
      "                             'Collections section where you want to enable the '\n",
      "                             'secondary image hover feature.\\n'\n",
      "                             \"4. With the section open, click on the section's \"\n",
      "                             'setting (gear) icon in the top-left corner.\\n'\n",
      "                             '5. In the settings panel that appears, look for '\n",
      "                             \"an option labeled 'Image display' or 'Image \"\n",
      "                             \"hover'.\\n\"\n",
      "                             \"6. If available, select 'Show secondary image on \"\n",
      "                             \"hover'.\\n\"\n",
      "                             '7. Save the changes and preview the '\n",
      "                             'Collection/Featured Collection page to see the '\n",
      "                             'effect.\\n'\n",
      "                             '\\n'\n",
      "                             \"If you're having trouble finding the setting, \"\n",
      "                             \"the best thing to do is refer to your theme's \"\n",
      "                             'documentation, since the location and labeling '\n",
      "                             'of settings can vary between themes.',\n",
      "                  'role': 'assistant'},\n",
      "                 {'content': 'Can you provide me with a link to the '\n",
      "                             'documentation for my theme?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': \"I don't have access to your store's theme \"\n",
      "                             'information. However, you can usually find the '\n",
      "                             'documentation for your theme by going to the '\n",
      "                             'shopify theme store, finding your theme and '\n",
      "                             \"clicking on the 'support' link located in the \"\n",
      "                             'bottom right corner of the page. Alternatively, '\n",
      "                             'you can do a google search for the name of your '\n",
      "                             \"theme followed by 'documentation' or 'user \"\n",
      "                             \"guide'.\",\n",
      "                  'role': 'assistant'},\n",
      "                 {'content': 'Can you confirm if this feature also works for '\n",
      "                             'the Quick Shop section of my theme?',\n",
      "                  'role': 'user'},\n",
      "                 {'content': 'The secondary image hover feature may or may not '\n",
      "                             'work for your Quick Shop section, depending on '\n",
      "                             'the configuration of your theme. Some themes '\n",
      "                             'include this feature in the Quick Shop section '\n",
      "                             'by default, while others may require additional '\n",
      "                             'customization. To check if this feature is '\n",
      "                             'available for the Quick Shop section of your '\n",
      "                             'theme, follow these steps:\\n'\n",
      "                             '\\n'\n",
      "                             '1. Go to the Quick Shop section where you would '\n",
      "                             'like to enable the feature. 2. Click on the '\n",
      "                             'Quick Shop settings icon (gear icon) and look '\n",
      "                             \"for 'Image display' or 'Image hover'. 3. If \"\n",
      "                             \"available, select 'Show secondary image on \"\n",
      "                             \"hover'. 4. Save the changes. If this option is \"\n",
      "                             'not available in your Quick Shop section '\n",
      "                             'settings, you may need to reach out to your '\n",
      "                             'theme developer for assistance with customizing '\n",
      "                             'your Quick Shop section to include this feature.',\n",
      "                  'role': 'assistant'}],\n",
      " 'prompt': 'These instructions apply to section-based themes (Responsive 6.0+, '\n",
      "           'Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme '\n",
      "           'version am I using?\\n'\n",
      "           'On your Collections pages & Featured Collections sections, you can '\n",
      "           'easily show the secondary image of a product on hover by enabling '\n",
      "           \"one of the theme's built-in settings!\\n\"\n",
      "           'Your Collection pages & Featured Collections sections will now '\n",
      "           'display the secondary product image just by hovering over that '\n",
      "           'product image thumbnail.\\n'\n",
      "           'Does this feature apply to all sections of the theme or just '\n",
      "           'specific ones as listed in the text material?',\n",
      " 'prompt_id': 'f0e37e9f7800261167ce91143f98f511f768847236f133f2d0aed60b444ebe57'}\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T02:05:19.444406Z",
     "start_time": "2025-08-22T02:05:19.441829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# keep running list of training + val loss for logging\n",
    "losses_per_epoch = []\n",
    "latest_epoch_losses = []"
   ],
   "id": "782cd2195efbc066",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T02:07:09.578493Z",
     "start_time": "2025-08-22T02:06:46.678034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_model_loss(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    last_gen_start_inds = batch['last_gen_start_inds'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    print(f\"Batch shape: {input_ids.shape}\")  # This is [batch_size, sequence_length]\n",
    "    print(f\"Sequence length: {input_ids.shape[1]}\")\n",
    "    print(f\"Memory before forward pass: {torch.mps.driver_allocated_memory() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "    # mask labels that aren't included in last gen\n",
    "    mask = torch.arange(input_ids.shape[1], device=device) < last_gen_start_inds[:, None]\n",
    "    labels[mask] = -100\n",
    "\n",
    "    # run forward pass and calc loss\n",
    "    model_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    print(f\"Memory after forward pass: {torch.mps.driver_allocated_memory() / 1024**3:.2f} GB\")\n",
    "\n",
    "    # calculate loss\n",
    "    B, T, C = model_outputs.logits.shape\n",
    "    logits = model_outputs.logits.view(B*T, C)\n",
    "    labels = labels.view(B*T)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# training run\n",
    "model.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    step = 0\n",
    "    print(f\"\\n Starting epoch: {epoch}\")\n",
    "    for batch in train_dataloader:\n",
    "        step += 1\n",
    "        # calculate training loss\n",
    "        training_loss = calc_model_loss(batch)\n",
    "        optimizer.zero_grad()\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate val loss\n",
    "        avg_val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for val_batch in val_dataloader:\n",
    "                avg_val_loss += calc_model_loss(val_batch).item()\n",
    "                val_batches += 1\n",
    "            avg_val_loss /= val_batches\n",
    "\n",
    "        latest_epoch_losses.append((training_loss.item(), avg_val_loss))\n",
    "        print(f\"epoch: {epoch} | step: {step} | training loss: {training_loss} | validation loss: {avg_val_loss}\")\n",
    "\n",
    "    losses_per_epoch.append(latest_epoch_losses)\n",
    "    latest_epoch_losses = []"
   ],
   "id": "71d7c703271a5893",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting epoch: 1\n",
      "Batch shape: torch.Size([2, 474])\n",
      "Sequence length: 474\n",
      "Memory before forward pass: 29.08 GB\n",
      "Memory after forward pass: 29.18 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 29.51 GB\n",
      "Memory after forward pass: 29.51 GB\n",
      "epoch: 1 | step: 1 | training loss: 4.41838264465332 | validation loss: 3.669476270675659\n",
      "Batch shape: torch.Size([2, 727])\n",
      "Sequence length: 727\n",
      "Memory before forward pass: 29.51 GB\n",
      "Memory after forward pass: 29.61 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 30.03 GB\n",
      "Memory after forward pass: 29.97 GB\n",
      "epoch: 1 | step: 2 | training loss: 4.3390793800354 | validation loss: 2.811246395111084\n",
      "Batch shape: torch.Size([2, 584])\n",
      "Sequence length: 584\n",
      "Memory before forward pass: 29.97 GB\n",
      "Memory after forward pass: 29.92 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 30.07 GB\n",
      "Memory after forward pass: 30.07 GB\n",
      "epoch: 1 | step: 3 | training loss: 2.9756553173065186 | validation loss: 2.0315401554107666\n",
      "Batch shape: torch.Size([2, 376])\n",
      "Sequence length: 376\n",
      "Memory before forward pass: 30.07 GB\n",
      "Memory after forward pass: 29.93 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 29.95 GB\n",
      "Memory after forward pass: 29.88 GB\n",
      "epoch: 1 | step: 4 | training loss: 2.2955803871154785 | validation loss: 1.3433921337127686\n",
      "Batch shape: torch.Size([2, 738])\n",
      "Sequence length: 738\n",
      "Memory before forward pass: 29.88 GB\n",
      "Memory after forward pass: 29.92 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 30.35 GB\n",
      "Memory after forward pass: 30.27 GB\n",
      "epoch: 1 | step: 5 | training loss: 1.3386914730072021 | validation loss: 0.8121007084846497\n",
      "Batch shape: torch.Size([2, 1016])\n",
      "Sequence length: 1016\n",
      "Memory before forward pass: 30.27 GB\n",
      "Memory after forward pass: 29.22 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 29.75 GB\n",
      "Memory after forward pass: 29.69 GB\n",
      "epoch: 1 | step: 6 | training loss: 0.9335107207298279 | validation loss: 0.5363657474517822\n",
      "Batch shape: torch.Size([2, 568])\n",
      "Sequence length: 568\n",
      "Memory before forward pass: 29.69 GB\n",
      "Memory after forward pass: 29.62 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 29.48 GB\n",
      "Memory after forward pass: 29.48 GB\n",
      "epoch: 1 | step: 7 | training loss: 0.6449209451675415 | validation loss: 0.4152138829231262\n",
      "Batch shape: torch.Size([2, 907])\n",
      "Sequence length: 907\n",
      "Memory before forward pass: 29.48 GB\n",
      "Memory after forward pass: 29.48 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 29.48 GB\n",
      "Memory after forward pass: 29.48 GB\n",
      "epoch: 1 | step: 8 | training loss: 0.4934254586696625 | validation loss: 0.3790191411972046\n",
      "Batch shape: torch.Size([2, 986])\n",
      "Sequence length: 986\n",
      "Memory before forward pass: 29.48 GB\n",
      "Memory after forward pass: 29.49 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 29.59 GB\n",
      "Memory after forward pass: 29.61 GB\n",
      "epoch: 1 | step: 9 | training loss: 0.39423343539237976 | validation loss: 0.3149476945400238\n",
      "Batch shape: torch.Size([2, 647])\n",
      "Sequence length: 647\n",
      "Memory before forward pass: 29.61 GB\n",
      "Memory after forward pass: 29.61 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 29.63 GB\n",
      "Memory after forward pass: 29.63 GB\n",
      "epoch: 1 | step: 10 | training loss: 0.4001408517360687 | validation loss: 0.27943190932273865\n",
      "Batch shape: torch.Size([2, 859])\n",
      "Sequence length: 859\n",
      "Memory before forward pass: 29.63 GB\n",
      "Memory after forward pass: 29.63 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 30.10 GB\n",
      "Memory after forward pass: 30.10 GB\n",
      "epoch: 1 | step: 11 | training loss: 0.2560725212097168 | validation loss: 0.15778394043445587\n",
      "Batch shape: torch.Size([2, 564])\n",
      "Sequence length: 564\n",
      "Memory before forward pass: 30.10 GB\n",
      "Memory after forward pass: 30.04 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 29.84 GB\n",
      "Memory after forward pass: 29.84 GB\n",
      "epoch: 1 | step: 12 | training loss: 0.2495638132095337 | validation loss: 0.15964996814727783\n",
      "Batch shape: torch.Size([2, 662])\n",
      "Sequence length: 662\n",
      "Memory before forward pass: 29.84 GB\n",
      "Memory after forward pass: 29.92 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 30.31 GB\n",
      "Memory after forward pass: 30.10 GB\n",
      "epoch: 1 | step: 13 | training loss: 0.22952571511268616 | validation loss: 0.14143006503582\n",
      "Batch shape: torch.Size([2, 808])\n",
      "Sequence length: 808\n",
      "Memory before forward pass: 30.10 GB\n",
      "Memory after forward pass: 29.07 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 29.52 GB\n",
      "Memory after forward pass: 29.52 GB\n",
      "epoch: 1 | step: 14 | training loss: 0.2789856791496277 | validation loss: 0.19573673605918884\n",
      "Batch shape: torch.Size([2, 752])\n",
      "Sequence length: 752\n",
      "Memory before forward pass: 29.52 GB\n",
      "Memory after forward pass: 29.52 GB\n",
      "Batch shape: torch.Size([2, 853])\n",
      "Sequence length: 853\n",
      "Memory before forward pass: 29.67 GB\n",
      "Memory after forward pass: 29.49 GB\n",
      "epoch: 1 | step: 15 | training loss: 0.15260329842567444 | validation loss: 0.11347964406013489\n",
      "Batch shape: torch.Size([2, 976])\n",
      "Sequence length: 976\n",
      "Memory before forward pass: 29.49 GB\n",
      "Memory after forward pass: 29.57 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[26]\u001B[39m\u001B[32m, line 49\u001B[39m\n\u001B[32m     47\u001B[39m training_loss = calc_model_loss(batch)\n\u001B[32m     48\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m \u001B[43mtraining_loss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     50\u001B[39m optimizer.step()\n\u001B[32m     52\u001B[39m \u001B[38;5;66;03m# calculate val loss\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training the reward model",
   "id": "6e77f8f5907b1bc8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T21:57:51.832937Z",
     "start_time": "2025-08-27T21:57:51.680269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reward model\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, input_size * 4)\n",
    "        self.fc2 = nn.Linear(input_size * 4, input_size)\n",
    "        self.fc3 = nn.Linear(input_size, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "reward_model = RewardModel(model.config.n_embd).to(device)\n",
    "\n",
    "# ---------------\n",
    "# rm hyperparameters / config\n",
    "rm_batch_size = 4\n",
    "rm_epochs = 5\n",
    "rm_lr = 1e-5\n",
    "rm_weight_decay = 0.01\n",
    "rm_optimizer = torch.optim.AdamW(reward_model.parameters(), lr=rm_lr, weight_decay=rm_weight_decay)\n",
    "# ---------------"
   ],
   "id": "68558aadb95b116a",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-27T21:57:52.382534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# we will use Anthropic/hh-rlhf for training our reward model\n",
    "rm_train_dataset = load_dataset(\"Anthropic/hh-rlhf\", split='train').select(range(1000))\n",
    "rm_train_split_size = int(0.9 * len(rm_train_dataset))\n",
    "rm_train_split = rm_train_dataset.select(range(rm_train_split_size))\n",
    "rm_val_split = rm_train_dataset.select(range(rm_train_split_size, len(rm_train_dataset)))\n",
    "\n",
    "def conv_str_to_msgs(str):\n",
    "    split = [line.strip() for line in re.split(r'(?=Human:|Assistant:)', str) if line.strip()]\n",
    "    msgs = []\n",
    "    for s in split:\n",
    "        role, content = s.split(':', 1)\n",
    "        msgs.append({'role': role.lower(), 'content': content})\n",
    "    return msgs\n",
    "\n",
    "def rm_preproc(example):\n",
    "    ex_proc = {\n",
    "        key: tokenizer.apply_chat_template(\n",
    "            conv_str_to_msgs(value),\n",
    "            tokenize=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        for key, value in example.items()\n",
    "    }\n",
    "\n",
    "    max_context_length = model.config.max_position_embeddings\n",
    "    ex_proc['exclude'] = (\n",
    "            len(ex_proc['chosen']) > max_context_length or len(ex_proc['rejected']) > max_context_length\n",
    "    )\n",
    "\n",
    "    return ex_proc\n",
    "\n",
    "rm_train_split = rm_train_split.map(rm_preproc).filter(exclude_filter)\n",
    "rm_val_split = rm_val_split.map(rm_preproc).filter(exclude_filter)\n",
    "\n",
    "\n",
    "# create custom collator for rlhf data\n",
    "class DataCollatorForRm:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding = True\n",
    "\n",
    "        self.clt = DataCollatorWithPadding(\n",
    "            tokenizer,\n",
    "            padding=self.padding,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        chosen = [{'input_ids': b['chosen']} for b in batch]\n",
    "        rejected = [{'input_ids': b['rejected']} for b in batch]\n",
    "        chosen_padded = self.clt(chosen)\n",
    "        rejected_padded = self.clt(rejected)\n",
    "\n",
    "        return {'chosen': chosen_padded, 'rejected': rejected_padded}\n",
    "\n",
    "\n",
    "rm_data_collator = DataCollatorForRm(tokenizer=tokenizer)\n",
    "\n",
    "rm_train_dataloader = DataLoader(\n",
    "    rm_train_split,\n",
    "    batch_size=rm_batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=rm_data_collator\n",
    ")\n",
    "\n",
    "rm_val_dataloader = DataLoader(\n",
    "    rm_val_split,\n",
    "    batch_size=rm_batch_size,\n",
    "    collate_fn=rm_data_collator,\n",
    "    shuffle=False\n",
    ")\n",
    "\n"
   ],
   "id": "8f3c488397c223de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T21:56:55.205727Z",
     "start_time": "2025-08-27T21:55:38.427346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calc_rm_val(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    outputs = model.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # only get the emb for final token\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    final_tok_ind = torch.sum(attention_mask, dim=1) - 1\n",
    "    batch_inds = torch.arange(hidden_states.shape[0], device=device)\n",
    "    final_tok_emb = hidden_states[batch_inds, final_tok_ind]\n",
    "\n",
    "    return reward_model(final_tok_emb)\n",
    "\n",
    "def calc_rm_loss(ch_rm_val, rej_rm_val):\n",
    "    return -F.logsigmoid(ch_rm_val - rej_rm_val).mean()\n",
    "\n",
    "def calc_val_perc_correct(val_batch):\n",
    "    rej_rm_vals = calc_rm_val(val_batch['rejected'])\n",
    "    ch_rm_vals = calc_rm_val(val_batch['chosen'])\n",
    "    correct = (ch_rm_vals > rej_rm_vals).int().sum()\n",
    "    return (correct / len(rej_rm_vals)).item()\n",
    "\n",
    "\n",
    "# reward model training loop\n",
    "reward_model.train()\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    step = 0\n",
    "    for batch in rm_train_dataloader:\n",
    "        step += 1\n",
    "        rej = batch['rejected'].to(device)\n",
    "        ch = batch['chosen'].to(device)\n",
    "        rm_loss = calc_rm_loss(calc_rm_val(ch), calc_rm_val(rej))\n",
    "\n",
    "        # calculate avg val dataset % classified correct\n",
    "        avg_val_per_right = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for val_batch in rm_val_dataloader:\n",
    "                avg_val_per_right += calc_val_perc_correct(val_batch)\n",
    "                val_batches += 1\n",
    "            avg_val_per_right /= val_batches\n",
    "\n",
    "        print(f\"epoch: {epoch} | step: {step} | training loss: {rm_loss} | avg val % correct preference: {avg_val_per_right}\")\n",
    "        rm_optimizer.zero_grad()\n",
    "        rm_loss.backward()\n",
    "        rm_optimizer.step()"
   ],
   "id": "daf721071ca1e02e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | step: 1 | training loss: 0.7633222937583923 | avg val % correct preference: 0.58\n",
      "epoch: 1 | step: 2 | training loss: 0.5859189033508301 | avg val % correct preference: 0.53\n",
      "epoch: 1 | step: 3 | training loss: 0.7964285016059875 | avg val % correct preference: 0.49\n",
      "epoch: 1 | step: 4 | training loss: 0.7925732731819153 | avg val % correct preference: 0.56\n",
      "epoch: 1 | step: 5 | training loss: 0.6669638156890869 | avg val % correct preference: 0.57\n",
      "epoch: 1 | step: 6 | training loss: 0.48933303356170654 | avg val % correct preference: 0.54\n",
      "epoch: 1 | step: 7 | training loss: 0.8430061936378479 | avg val % correct preference: 0.65\n",
      "epoch: 1 | step: 8 | training loss: 0.7581450939178467 | avg val % correct preference: 0.51\n",
      "epoch: 1 | step: 9 | training loss: 0.6295057535171509 | avg val % correct preference: 0.59\n",
      "epoch: 1 | step: 10 | training loss: 0.5836320519447327 | avg val % correct preference: 0.51\n",
      "epoch: 1 | step: 11 | training loss: 0.7567492127418518 | avg val % correct preference: 0.53\n",
      "epoch: 1 | step: 12 | training loss: 0.7900158762931824 | avg val % correct preference: 0.52\n",
      "epoch: 1 | step: 13 | training loss: 0.6343318223953247 | avg val % correct preference: 0.47\n",
      "epoch: 1 | step: 14 | training loss: 0.7989672422409058 | avg val % correct preference: 0.43\n",
      "epoch: 1 | step: 15 | training loss: 0.7431890368461609 | avg val % correct preference: 0.42\n",
      "epoch: 1 | step: 16 | training loss: 0.7682856917381287 | avg val % correct preference: 0.56\n",
      "epoch: 1 | step: 17 | training loss: 0.7273510098457336 | avg val % correct preference: 0.4\n",
      "epoch: 1 | step: 18 | training loss: 0.6910202503204346 | avg val % correct preference: 0.54\n",
      "epoch: 1 | step: 19 | training loss: 0.5734549760818481 | avg val % correct preference: 0.53\n",
      "epoch: 1 | step: 20 | training loss: 0.6504559516906738 | avg val % correct preference: 0.56\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 15.86 GB, other allocations: 20.38 GB, max allowed: 36.27 GB). Tried to allocate 147.24 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[114]\u001B[39m\u001B[32m, line 45\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mepoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | step: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstep\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | training loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrm_loss\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | avg val % correct preference: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_val_per_right\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     44\u001B[39m rm_optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m45\u001B[39m \u001B[43mrm_loss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     46\u001B[39m rm_optimizer.step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mRuntimeError\u001B[39m: MPS backend out of memory (MPS allocated: 15.86 GB, other allocations: 20.38 GB, max allowed: 36.27 GB). Tried to allocate 147.24 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GRPO implementation + rl training loop",
   "id": "96dba371d81ad028"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "db9bb96d7b069846"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
