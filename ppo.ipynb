{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Proximal policy optimization algorithm from scratch",
   "id": "883836f2a581bf5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:16:33.449619Z",
     "start_time": "2025-08-12T02:16:33.443824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import torch.distributions as dist\n",
    "from enum import Enum\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, action_space_dim, is_continuous):\n",
    "        super().__init__()\n",
    "        self.is_continuous = is_continuous\n",
    "        self.state_dim = state_space_dim\n",
    "        self.action_space_dim = action_space_dim\n",
    "        self.fc1 = nn.Linear(state_space_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        # For continuous action dim, return mean and only diagonal entries on covariance matrix since action states are probably independent\n",
    "        self.fc3 = nn.Linear(64, 2 * action_space_dim if is_continuous else action_space_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        if self.is_continuous:\n",
    "            mean = x[..., :self.action_space_dim]\n",
    "            covar = torch.exp(x[..., self.action_space_dim:])\n",
    "            return mean, torch.diag(covar)\n",
    "        else:\n",
    "            return torch.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class BaselineVNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, max_timesteps):\n",
    "        super().__init__()\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.fc1 = nn.Linear(state_space_dim + 1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, timestep):\n",
    "        timestep = timestep.float() / self.max_timesteps\n",
    "        x = F.relu(self.fc1(torch.cat([state, timestep.unsqueeze(-1)])))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class AdvantageEstimation(Enum):\n",
    "    VPG = 0\n",
    "    # Use k=5 step lookahead\n",
    "    A2C = 1\n",
    "\n",
    "\n",
    "# Implement vpg alg\n",
    "iterations = 100\n",
    "max_timesteps = 500\n",
    "t_per_iter = 50\n",
    "env = gym.make('CartPole-v1', render_mode='human', max_episode_steps=max_timesteps)\n",
    "state_space_dim = 4\n",
    "action_space_dim = 2\n",
    "policy = PolicyNetwork(state_space_dim, action_space_dim, False)\n",
    "policy_opt = torch.optim.AdamW(policy.parameters(), lr=5e-4)\n",
    "policy_opt_steps = 5\n",
    "baseline = BaselineVNetwork(state_space_dim, max_timesteps)\n",
    "baseline_opt = torch.optim.AdamW(baseline.parameters(), lr=1e-3)\n",
    "baseline_opt_steps = 10\n",
    "discount = 0.99\n",
    "adv_est_scheme = AdvantageEstimation.A2C\n",
    "eps = 0.2"
   ],
   "id": "dd13a1fe9faa4edf",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T19:07:47.956992Z",
     "start_time": "2025-08-12T02:17:41.080349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for iter in range(100):\n",
    "    # Collect a set of trajectories by executing the current policy\n",
    "    trajectories = []\n",
    "    with torch.no_grad():\n",
    "        for t_ind in range(t_per_iter):\n",
    "            current_state, _ = env.reset()\n",
    "            current_state = torch.tensor(current_state, dtype=torch.float32)\n",
    "            traj = []\n",
    "            for _ in range(max_timesteps):\n",
    "                actions_dist = policy(current_state)\n",
    "                if policy.is_continuous:\n",
    "                    mean, covar = actions_dist\n",
    "                    mgd = dist.MultivariateNormal(mean, covar)\n",
    "                    action = mgd.sample()\n",
    "                else:\n",
    "                    d = dist.Categorical(actions_dist)\n",
    "                    action = d.sample()\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                new_step = (current_state, action, reward)\n",
    "                traj.append(new_step)\n",
    "                current_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "            trajectories.append(traj)\n",
    "\n",
    "\n",
    "    # At each timestep in each trajectory, compute the return and advantage estimate\n",
    "    def calc_returns():\n",
    "        if adv_est_scheme == AdvantageEstimation.VPG:\n",
    "            returns = [[sum(discount**(t_prime - time) * traj[t_prime][2] for t_prime in range(time, len(traj))) for time in range(len(traj))] for traj in trajectories]\n",
    "        else:\n",
    "            returns = []\n",
    "            for traj in trajectories:\n",
    "                traj_returns = []\n",
    "                for time in range(len(traj)):\n",
    "                    t_return = 0\n",
    "                    is_long_en = len(traj) > time + 5\n",
    "                    for t_prime in range(time, min(len(traj), time + 5)):\n",
    "                        t_return += discount**(t_prime - time) * traj[t_prime][2]\n",
    "\n",
    "                    if is_long_en:\n",
    "                        t_return += discount**5 * baseline(traj[time + 5][0], torch.tensor(time + 5, dtype=torch.float32))\n",
    "\n",
    "                    traj_returns.append(t_return)\n",
    "\n",
    "                returns.append(traj_returns)\n",
    "\n",
    "        return returns\n",
    "\n",
    "    def calc_baselines():\n",
    "       return [[baseline(traj[time][0], torch.tensor(time, dtype=torch.float32)) for time in range(len(traj))] for traj in trajectories]\n",
    "\n",
    "    def calc_adv_ests(returns, log=False):\n",
    "        baselines = calc_baselines()\n",
    "        return [[(returns[traj_ind][step_ind] - baselines[traj_ind][step_ind]) for step_ind in range(len(trajectories[traj_ind]))] for traj_ind in range(len(trajectories))]\n",
    "\n",
    "    def normalize_adv_ests(adv_ests):\n",
    "        flattened = torch.stack([a for t in adv_ests for a in t])\n",
    "        return [[(ae - flattened.mean()) / (flattened.std() + 1e-8) for ae in t_ae] for t_ae in adv_ests]\n",
    "\n",
    "    def calc_baseline_loss():\n",
    "        returns = calc_returns()\n",
    "        adv_ests = calc_adv_ests(returns)\n",
    "        adv_ests = [a for traj_a in adv_ests for a in traj_a]\n",
    "        return (torch.stack(adv_ests) ** 2).sum() / len(trajectories)\n",
    "\n",
    "    def collect_state_act_prob():\n",
    "        vals = []\n",
    "        for traj in trajectories:\n",
    "            for state, action, _ in traj:\n",
    "                if policy.is_continuous:\n",
    "                    mean, covar = policy(state)\n",
    "                    d = dist.MultivariateNormal(mean, covar)\n",
    "                else:\n",
    "                    d = dist.Categorical(policy(state))\n",
    "                # Add log pdf val for now for numerical stability and consistency\n",
    "                vals.append(d.log_prob(action))\n",
    "        return vals\n",
    "\n",
    "     # Update policy by maximizing PPO objective\n",
    "    with torch.no_grad():\n",
    "        returns = calc_returns()\n",
    "        adv_ests = calc_adv_ests(returns, log=True)\n",
    "        flattened_adv_ests = torch.stack([a for adv_est in adv_ests for a in adv_est])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        old_policy_dist_vals = torch.stack(collect_state_act_prob())\n",
    "\n",
    "    policy_losses = []\n",
    "    for i in range(5):\n",
    "        policy_dist_vals = torch.stack(collect_state_act_prob())\n",
    "        ratios = torch.exp(policy_dist_vals - old_policy_dist_vals)\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - eps, 1 + eps)\n",
    "\n",
    "        temp = []\n",
    "        for m in range(len(ratios)):\n",
    "            temp.append(torch.min(ratios[m] * flattened_adv_ests[m], clipped_ratios[m] * flattened_adv_ests[m]))\n",
    "\n",
    "        temp = torch.stack(temp)\n",
    "        policy_loss = -temp.mean()\n",
    "        policy_losses.append(policy_loss)\n",
    "        policy_opt.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_opt.step()\n",
    "\n",
    "\n",
    "    # Re-fit the baseline\n",
    "    last_baseline_loss = 0\n",
    "    for i in range(baseline_opt_steps):\n",
    "        baseline_loss = calc_baseline_loss()\n",
    "        last_baseline_loss = baseline_loss\n",
    "        baseline_opt.zero_grad()\n",
    "        baseline_loss.backward()\n",
    "        baseline_opt.step()\n",
    "\n",
    "\n",
    "    print(\"--------------------\")\n",
    "    print(\"Iteration:\", iter)\n",
    "    print(\"Avg policy loss:\", torch.stack(policy_losses).mean())\n",
    "    print(\"Last baseline loss:\", last_baseline_loss)\n",
    "    print(\"Avg return:\", sum(r[0] for r in returns) / len(returns))\n",
    "    print(\"Avg steps:\", sum(len(t) for t in trajectories) / len(trajectories))\n",
    "    print(\"--------------------\\n\\n\")\n"
   ],
   "id": "7891443b43b74fd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Iteration: 0\n",
      "Avg policy loss: tensor(-4.3689, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(430.2012, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([5.1840])\n",
      "Avg steps: 21.6\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 1\n",
      "Avg policy loss: tensor(-4.3169, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(410.7805, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([5.3130])\n",
      "Avg steps: 21.26\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 2\n",
      "Avg policy loss: tensor(-4.2887, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(421.9746, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([5.4716])\n",
      "Avg steps: 22.32\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 3\n",
      "Avg policy loss: tensor(-4.2046, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(403.2169, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([5.7169])\n",
      "Avg steps: 22.22\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 4\n",
      "Avg policy loss: tensor(-4.0333, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(350.7293, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([6.2031])\n",
      "Avg steps: 20.86\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 5\n",
      "Avg policy loss: tensor(-3.9822, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(380.7395, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([6.5247])\n",
      "Avg steps: 23.0\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 6\n",
      "Avg policy loss: tensor(-3.8445, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(382.1777, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([7.2211])\n",
      "Avg steps: 23.82\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 7\n",
      "Avg policy loss: tensor(-3.6541, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(354.9719, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([7.9601])\n",
      "Avg steps: 23.88\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 8\n",
      "Avg policy loss: tensor(-3.6526, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(424.8258, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([8.6053])\n",
      "Avg steps: 28.06\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 9\n",
      "Avg policy loss: tensor(-3.4990, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(454.9876, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([9.7308])\n",
      "Avg steps: 29.94\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 10\n",
      "Avg policy loss: tensor(-3.1535, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(363.5567, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([10.5794])\n",
      "Avg steps: 26.36\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 11\n",
      "Avg policy loss: tensor(-3.3451, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(501.3881, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([10.9688])\n",
      "Avg steps: 33.4\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 12\n",
      "Avg policy loss: tensor(-3.0160, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(392.4839, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([11.5660])\n",
      "Avg steps: 28.98\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 13\n",
      "Avg policy loss: tensor(-3.1066, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(425.8163, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([11.8117])\n",
      "Avg steps: 31.66\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 14\n",
      "Avg policy loss: tensor(-3.3125, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(532.3974, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([12.5933])\n",
      "Avg steps: 38.96\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 15\n",
      "Avg policy loss: tensor(-3.2093, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(521.0048, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([13.3254])\n",
      "Avg steps: 39.74\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 16\n",
      "Avg policy loss: tensor(-3.3346, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(672.3879, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([14.4486])\n",
      "Avg steps: 49.1\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 17\n",
      "Avg policy loss: tensor(-3.2035, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(669.2736, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([16.1031])\n",
      "Avg steps: 51.8\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 18\n",
      "Avg policy loss: tensor(-2.8963, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(550.8843, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([17.5906])\n",
      "Avg steps: 48.8\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 19\n",
      "Avg policy loss: tensor(-3.2098, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(788.8964, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([19.2627])\n",
      "Avg steps: 68.02\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 20\n",
      "Avg policy loss: tensor(-2.9130, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(859.5291, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([22.8779])\n",
      "Avg steps: 73.88\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 21\n",
      "Avg policy loss: tensor(-2.8839, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(887.9308, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([24.7780])\n",
      "Avg steps: 79.62\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 22\n",
      "Avg policy loss: tensor(-2.6991, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(927.6288, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([27.4266])\n",
      "Avg steps: 82.48\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 23\n",
      "Avg policy loss: tensor(-2.5711, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1040.3955, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([29.6027])\n",
      "Avg steps: 88.46\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 24\n",
      "Avg policy loss: tensor(-2.6462, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1156.0283, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([29.9613])\n",
      "Avg steps: 93.66\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 25\n",
      "Avg policy loss: tensor(-2.8526, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1240.9125, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([31.1940])\n",
      "Avg steps: 110.6\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 26\n",
      "Avg policy loss: tensor(-2.7570, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1568.3091, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([34.9801])\n",
      "Avg steps: 123.02\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 27\n",
      "Avg policy loss: tensor(-2.6644, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1430.0808, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([36.4555])\n",
      "Avg steps: 122.0\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 28\n",
      "Avg policy loss: tensor(-3.0402, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1722.4382, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([37.3563])\n",
      "Avg steps: 160.26\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 29\n",
      "Avg policy loss: tensor(-2.5184, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2030.5676, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([45.3269])\n",
      "Avg steps: 175.32\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 30\n",
      "Avg policy loss: tensor(-2.7484, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2303.3301, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([46.8289])\n",
      "Avg steps: 202.16\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 31\n",
      "Avg policy loss: tensor(-2.2212, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2154.0688, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([52.2423])\n",
      "Avg steps: 191.46\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 32\n",
      "Avg policy loss: tensor(-2.3314, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1469.4897, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([51.6947])\n",
      "Avg steps: 201.38\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 33\n",
      "Avg policy loss: tensor(-1.9817, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1746.6075, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([57.6946])\n",
      "Avg steps: 204.22\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 34\n",
      "Avg policy loss: tensor(-1.8054, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1063.3910, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([60.0354])\n",
      "Avg steps: 199.36\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 35\n",
      "Avg policy loss: tensor(-1.5919, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(986.6658, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([63.8300])\n",
      "Avg steps: 199.66\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 36\n",
      "Avg policy loss: tensor(-1.5877, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2145.2546, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([69.6528])\n",
      "Avg steps: 232.2\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 37\n",
      "Avg policy loss: tensor(-1.7246, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1961.8638, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([69.9697])\n",
      "Avg steps: 249.72\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 38\n",
      "Avg policy loss: tensor(-1.3820, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2012.0774, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([71.5957])\n",
      "Avg steps: 234.74\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 39\n",
      "Avg policy loss: tensor(-2.2181, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1954.7622, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([67.5577])\n",
      "Avg steps: 301.64\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 40\n",
      "Avg policy loss: tensor(-1.5872, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1921.2428, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([76.2225])\n",
      "Avg steps: 311.92\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 41\n",
      "Avg policy loss: tensor(-1.4976, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1602.8153, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([78.2046])\n",
      "Avg steps: 305.78\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 42\n",
      "Avg policy loss: tensor(-1.3101, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2516.0640, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([81.7501])\n",
      "Avg steps: 302.94\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 43\n",
      "Avg policy loss: tensor(-1.1364, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1997.0507, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([82.1664])\n",
      "Avg steps: 278.0\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 44\n",
      "Avg policy loss: tensor(-1.3151, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1961.5181, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([79.1470])\n",
      "Avg steps: 277.16\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 45\n",
      "Avg policy loss: tensor(-1.7263, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1570.3517, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([77.0347])\n",
      "Avg steps: 323.08\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 46\n",
      "Avg policy loss: tensor(-1.3523, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1328.5168, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([81.7849])\n",
      "Avg steps: 320.14\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 47\n",
      "Avg policy loss: tensor(-0.5011, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(581.6833, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([85.7717])\n",
      "Avg steps: 242.88\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 48\n",
      "Avg policy loss: tensor(-0.2490, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(824.3417, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([87.6896])\n",
      "Avg steps: 220.18\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 49\n",
      "Avg policy loss: tensor(-0.6001, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(815.9096, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([86.6417])\n",
      "Avg steps: 243.8\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 50\n",
      "Avg policy loss: tensor(-0.9536, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1543.7292, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([85.6804])\n",
      "Avg steps: 291.46\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 51\n",
      "Avg policy loss: tensor(-1.3764, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1138.4689, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([85.0745])\n",
      "Avg steps: 367.9\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 52\n",
      "Avg policy loss: tensor(-0.9617, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2894.0967, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([89.8519])\n",
      "Avg steps: 367.32\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 53\n",
      "Avg policy loss: tensor(-1.5532, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2230.3813, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([86.1756])\n",
      "Avg steps: 417.72\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 54\n",
      "Avg policy loss: tensor(-1.1102, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2314.3928, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([89.5645])\n",
      "Avg steps: 389.5\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 55\n",
      "Avg policy loss: tensor(-1.3566, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2088.8899, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([88.7197])\n",
      "Avg steps: 411.98\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 56\n",
      "Avg policy loss: tensor(-1.1559, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1480.3562, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([90.1693])\n",
      "Avg steps: 392.48\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 57\n",
      "Avg policy loss: tensor(-0.9182, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(889.7166, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([93.0872])\n",
      "Avg steps: 370.7\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 58\n",
      "Avg policy loss: tensor(-0.4124, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(629.6968, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([96.5534])\n",
      "Avg steps: 323.64\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 59\n",
      "Avg policy loss: tensor(-0.2115, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(763.8156, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([97.7899])\n",
      "Avg steps: 299.46\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 60\n",
      "Avg policy loss: tensor(-0.4078, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(431.5543, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([97.1460])\n",
      "Avg steps: 323.8\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 61\n",
      "Avg policy loss: tensor(-0.7047, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1274.2179, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([97.2741])\n",
      "Avg steps: 383.26\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 62\n",
      "Avg policy loss: tensor(-0.6046, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(784.6622, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([97.6070])\n",
      "Avg steps: 379.56\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 63\n",
      "Avg policy loss: tensor(-0.5722, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1221.9155, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([97.9580])\n",
      "Avg steps: 378.52\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 64\n",
      "Avg policy loss: tensor(-0.8564, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1219.8754, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([97.2246])\n",
      "Avg steps: 425.1\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 65\n",
      "Avg policy loss: tensor(-1.0348, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1744.5547, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([97.9059])\n",
      "Avg steps: 447.48\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 66\n",
      "Avg policy loss: tensor(-0.9635, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1862.7881, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([100.7579])\n",
      "Avg steps: 468.46\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[73]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     16\u001B[39m     d = dist.Categorical(actions_dist)\n\u001B[32m     17\u001B[39m     action = d.sample()\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m next_state, reward, terminated, truncated, _ = \u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m new_step = (current_state, action, reward)\n\u001B[32m     21\u001B[39m traj.append(new_step)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:125\u001B[39m, in \u001B[36mTimeLimit.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    113\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[32m    114\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[ObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    115\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[32m    116\u001B[39m \n\u001B[32m    117\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    123\u001B[39m \n\u001B[32m    124\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     observation, reward, terminated, truncated, info = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    126\u001B[39m     \u001B[38;5;28mself\u001B[39m._elapsed_steps += \u001B[32m1\u001B[39m\n\u001B[32m    128\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._elapsed_steps >= \u001B[38;5;28mself\u001B[39m._max_episode_steps:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001B[39m, in \u001B[36mOrderEnforcing.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    391\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._has_reset:\n\u001B[32m    392\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[33m\"\u001B[39m\u001B[33mCannot call env.step() before calling env.reset()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/core.py:327\u001B[39m, in \u001B[36mWrapper.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    323\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    324\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[32m    325\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    326\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:285\u001B[39m, in \u001B[36mPassiveEnvChecker.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    283\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m.env, action)\n\u001B[32m    284\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:223\u001B[39m, in \u001B[36mCartPoleEnv.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    220\u001B[39m     reward = -\u001B[32m1.0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sutton_barto_reward \u001B[38;5;28;01melse\u001B[39;00m \u001B[32m0.0\u001B[39m\n\u001B[32m    222\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mhuman\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m223\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    225\u001B[39m \u001B[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001B[39;00m\n\u001B[32m    226\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m np.array(\u001B[38;5;28mself\u001B[39m.state, dtype=np.float32), reward, terminated, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:337\u001B[39m, in \u001B[36mCartPoleEnv.render\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    335\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mhuman\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    336\u001B[39m     pygame.event.pump()\n\u001B[32m--> \u001B[39m\u001B[32m337\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrender_fps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    338\u001B[39m     pygame.display.flip()\n\u001B[32m    340\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mrgb_array\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Reflections\n",
    "\n",
    "For the most part, since I had a working vpg + a2c algorithm, implementing ppo didn't take too long. One thing that shocked me though was how fast and large the improvements would be! By 40 iterations, ppo had already solved Cartpole (averaging 300 steps). Also at times, the steps would increase by +80 between iterations. For contrast, with vpg the learning was very chaotic and it took ~200 steps for it to reach 300 avg steps."
   ],
   "id": "df309c7f38a81b00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
