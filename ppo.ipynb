{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Proximal policy optimization algorithm from scratch",
   "id": "883836f2a581bf5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:16:33.449619Z",
     "start_time": "2025-08-12T02:16:33.443824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import torch.distributions as dist\n",
    "from enum import Enum\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, action_space_dim, is_continuous):\n",
    "        super().__init__()\n",
    "        self.is_continuous = is_continuous\n",
    "        self.state_dim = state_space_dim\n",
    "        self.action_space_dim = action_space_dim\n",
    "        self.fc1 = nn.Linear(state_space_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        # For continuous action dim, return mean and only diagonal entries on covariance matrix since action states are probably independent\n",
    "        self.fc3 = nn.Linear(64, 2 * action_space_dim if is_continuous else action_space_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        if self.is_continuous:\n",
    "            mean = x[..., :self.action_space_dim]\n",
    "            covar = torch.exp(x[..., self.action_space_dim:])\n",
    "            return mean, torch.diag(covar)\n",
    "        else:\n",
    "            return torch.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class BaselineVNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, max_timesteps):\n",
    "        super().__init__()\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.fc1 = nn.Linear(state_space_dim + 1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, timestep):\n",
    "        timestep = timestep.float() / self.max_timesteps\n",
    "        x = F.relu(self.fc1(torch.cat([state, timestep.unsqueeze(-1)])))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class AdvantageEstimation(Enum):\n",
    "    VPG = 0\n",
    "    # Use k=5 step lookahead\n",
    "    A2C = 1\n",
    "\n",
    "\n",
    "# Implement vpg alg\n",
    "iterations = 100\n",
    "max_timesteps = 500\n",
    "t_per_iter = 50\n",
    "env = gym.make('CartPole-v1', render_mode='human', max_episode_steps=max_timesteps)\n",
    "state_space_dim = 4\n",
    "action_space_dim = 2\n",
    "policy = PolicyNetwork(state_space_dim, action_space_dim, False)\n",
    "policy_opt = torch.optim.AdamW(policy.parameters(), lr=5e-4)\n",
    "policy_opt_steps = 5\n",
    "baseline = BaselineVNetwork(state_space_dim, max_timesteps)\n",
    "baseline_opt = torch.optim.AdamW(baseline.parameters(), lr=1e-3)\n",
    "baseline_opt_steps = 10\n",
    "discount = 0.99\n",
    "adv_est_scheme = AdvantageEstimation.A2C\n",
    "eps = 0.2"
   ],
   "id": "dd13a1fe9faa4edf",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-12T02:17:41.080349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for iter in range(100):\n",
    "    # Collect a set of trajectories by executing the current policy\n",
    "    trajectories = []\n",
    "    with torch.no_grad():\n",
    "        for t_ind in range(t_per_iter):\n",
    "            current_state, _ = env.reset()\n",
    "            current_state = torch.tensor(current_state, dtype=torch.float32)\n",
    "            traj = []\n",
    "            for _ in range(max_timesteps):\n",
    "                actions_dist = policy(current_state)\n",
    "                if policy.is_continuous:\n",
    "                    mean, covar = actions_dist\n",
    "                    mgd = dist.MultivariateNormal(mean, covar)\n",
    "                    action = mgd.sample()\n",
    "                else:\n",
    "                    d = dist.Categorical(actions_dist)\n",
    "                    action = d.sample()\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                new_step = (current_state, action, reward)\n",
    "                traj.append(new_step)\n",
    "                current_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "            trajectories.append(traj)\n",
    "\n",
    "\n",
    "    # At each timestep in each trajectory, compute the return and advantage estimate\n",
    "    def calc_returns():\n",
    "        if adv_est_scheme == AdvantageEstimation.VPG:\n",
    "            returns = [[sum(discount**(t_prime - time) * traj[t_prime][2] for t_prime in range(time, len(traj))) for time in range(len(traj))] for traj in trajectories]\n",
    "        else:\n",
    "            returns = []\n",
    "            for traj in trajectories:\n",
    "                traj_returns = []\n",
    "                for time in range(len(traj)):\n",
    "                    t_return = 0\n",
    "                    is_long_en = len(traj) > time + 5\n",
    "                    for t_prime in range(time, min(len(traj), time + 5)):\n",
    "                        t_return += discount**(t_prime - time) * traj[t_prime][2]\n",
    "\n",
    "                    if is_long_en:\n",
    "                        t_return += discount**5 * baseline(traj[time + 5][0], torch.tensor(time + 5, dtype=torch.float32))\n",
    "\n",
    "                    traj_returns.append(t_return)\n",
    "\n",
    "                returns.append(traj_returns)\n",
    "\n",
    "        return returns\n",
    "\n",
    "    def calc_baselines():\n",
    "       return [[baseline(traj[time][0], torch.tensor(time, dtype=torch.float32)) for time in range(len(traj))] for traj in trajectories]\n",
    "\n",
    "    def calc_adv_ests(returns, log=False):\n",
    "        baselines = calc_baselines()\n",
    "        # if log:\n",
    "        #     print(\"PRINTING BASELINES\")\n",
    "        #     for b in baselines:\n",
    "        #         print(\"Baselines are: \\n\", torch.stack(b).tolist())\n",
    "        return [[(returns[traj_ind][step_ind] - baselines[traj_ind][step_ind]) for step_ind in range(len(trajectories[traj_ind]))] for traj_ind in range(len(trajectories))]\n",
    "\n",
    "    def normalize_adv_ests(adv_ests):\n",
    "        flattened = torch.stack([a for t in adv_ests for a in t])\n",
    "        return [[(ae - flattened.mean()) / (flattened.std() + 1e-8) for ae in t_ae] for t_ae in adv_ests]\n",
    "\n",
    "    def calc_baseline_loss():\n",
    "        returns = calc_returns()\n",
    "        adv_ests = calc_adv_ests(returns)\n",
    "        adv_ests = [a for traj_a in adv_ests for a in traj_a]\n",
    "        return (torch.stack(adv_ests) ** 2).sum() / len(trajectories)\n",
    "\n",
    "    def collect_state_act_prob():\n",
    "        vals = []\n",
    "        for traj in trajectories:\n",
    "            for state, action, _ in traj:\n",
    "                if policy.is_continuous:\n",
    "                    mean, covar = policy(state)\n",
    "                    d = dist.MultivariateNormal(mean, covar)\n",
    "                else:\n",
    "                    d = dist.Categorical(policy(state))\n",
    "                # Add log pdf val for now for numerical stability and consistency\n",
    "                # print(\"PROB IS:\", d.log_prob(action))\n",
    "                vals.append(d.log_prob(action))\n",
    "        return vals\n",
    "\n",
    "     # Update policy by maximizing PPO objective\n",
    "    with torch.no_grad():\n",
    "        returns = calc_returns()\n",
    "        adv_ests = calc_adv_ests(returns, log=True)\n",
    "        flattened_adv_ests = torch.stack([a for adv_est in adv_ests for a in adv_est])\n",
    "\n",
    "\n",
    "    # TESTED TILL HERE, ADV ESTIMATES ARE LOOKING GOOD\n",
    "\n",
    "    with torch.no_grad():\n",
    "        old_policy_dist_vals = torch.stack(collect_state_act_prob())\n",
    "\n",
    "    policy_losses = []\n",
    "    for i in range(5):\n",
    "        # print(\"Starting policy update:\", i)\n",
    "        policy_dist_vals = torch.stack(collect_state_act_prob())\n",
    "        ratios = torch.exp(policy_dist_vals - old_policy_dist_vals)\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - eps, 1 + eps)\n",
    "\n",
    "        # print(\"Before doing optimization step:\", i)\n",
    "        # print(\"On iteration:\", i, \"ratio mean:\", ratios.mean())\n",
    "        # print(\"On iteration:\", i, \"clipped ratio mean:\", clipped_ratios.mean())\n",
    "\n",
    "        temp = []\n",
    "        for m in range(len(ratios)):\n",
    "            # if i == 1:\n",
    "            #     print(\"For ratio:\", ratios[m].item(), \" * \", \"adv_est:\", flattened_adv_ests[m].item())\n",
    "            #     print(\"For clipped ratio:\", clipped_ratios[m].item(), \" * \", \"adv_est:\", flattened_adv_ests[m].item())\n",
    "            #     print(\"Choosing:\", torch.min(ratios[m] * flattened_adv_ests[m], clipped_ratios[m] * flattened_adv_ests[m]).item())\n",
    "            temp.append(torch.min(ratios[m] * flattened_adv_ests[m], clipped_ratios[m] * flattened_adv_ests[m]))\n",
    "\n",
    "        temp = torch.stack(temp)\n",
    "        policy_loss = -temp.mean()\n",
    "\n",
    "        # policy_loss = -torch.min(ratios * flattened_adv_ests, clipped_ratios * flattened_adv_ests).mean()\n",
    "        policy_losses.append(policy_loss)\n",
    "        policy_opt.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_opt.step()\n",
    "\n",
    "        # print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "    # Re-fit the baseline\n",
    "    last_baseline_loss = 0\n",
    "    for i in range(baseline_opt_steps):\n",
    "        baseline_loss = calc_baseline_loss()\n",
    "        last_baseline_loss = baseline_loss\n",
    "        # print(\"On baseline value opt step:\", i, \" with loss:\", baseline_loss)\n",
    "        baseline_opt.zero_grad()\n",
    "        baseline_loss.backward()\n",
    "        baseline_opt.step()\n",
    "\n",
    "\n",
    "    print(\"--------------------\")\n",
    "    print(\"Iteration:\", iter)\n",
    "    print(\"Avg policy loss:\", torch.stack(policy_losses).mean())\n",
    "    print(\"Last baseline loss:\", last_baseline_loss)\n",
    "    print(\"Avg return:\", sum(r[0] for r in returns) / len(returns))\n",
    "    print(\"Avg steps:\", sum(len(t) for t in trajectories) / len(trajectories))\n",
    "    print(\"--------------------\\n\\n\")\n"
   ],
   "id": "7891443b43b74fd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Iteration: 0\n",
      "Avg policy loss: tensor(-4.3689, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(430.2012, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([5.1840])\n",
      "Avg steps: 21.6\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 1\n",
      "Avg policy loss: tensor(-4.3169, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(410.7805, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([5.3130])\n",
      "Avg steps: 21.26\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 2\n",
      "Avg policy loss: tensor(-4.2887, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(421.9746, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([5.4716])\n",
      "Avg steps: 22.32\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 3\n",
      "Avg policy loss: tensor(-4.2046, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(403.2169, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([5.7169])\n",
      "Avg steps: 22.22\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 4\n",
      "Avg policy loss: tensor(-4.0333, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(350.7293, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([6.2031])\n",
      "Avg steps: 20.86\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 5\n",
      "Avg policy loss: tensor(-3.9822, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(380.7395, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([6.5247])\n",
      "Avg steps: 23.0\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 6\n",
      "Avg policy loss: tensor(-3.8445, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(382.1777, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([7.2211])\n",
      "Avg steps: 23.82\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 7\n",
      "Avg policy loss: tensor(-3.6541, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(354.9719, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([7.9601])\n",
      "Avg steps: 23.88\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 8\n",
      "Avg policy loss: tensor(-3.6526, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(424.8258, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([8.6053])\n",
      "Avg steps: 28.06\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 9\n",
      "Avg policy loss: tensor(-3.4990, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(454.9876, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([9.7308])\n",
      "Avg steps: 29.94\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 10\n",
      "Avg policy loss: tensor(-3.1535, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(363.5567, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([10.5794])\n",
      "Avg steps: 26.36\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 11\n",
      "Avg policy loss: tensor(-3.3451, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(501.3881, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([10.9688])\n",
      "Avg steps: 33.4\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 12\n",
      "Avg policy loss: tensor(-3.0160, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(392.4839, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([11.5660])\n",
      "Avg steps: 28.98\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 13\n",
      "Avg policy loss: tensor(-3.1066, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(425.8163, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([11.8117])\n",
      "Avg steps: 31.66\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 14\n",
      "Avg policy loss: tensor(-3.3125, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(532.3974, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([12.5933])\n",
      "Avg steps: 38.96\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 15\n",
      "Avg policy loss: tensor(-3.2093, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(521.0048, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([13.3254])\n",
      "Avg steps: 39.74\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 16\n",
      "Avg policy loss: tensor(-3.3346, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(672.3879, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([14.4486])\n",
      "Avg steps: 49.1\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 17\n",
      "Avg policy loss: tensor(-3.2035, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(669.2736, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([16.1031])\n",
      "Avg steps: 51.8\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 18\n",
      "Avg policy loss: tensor(-2.8963, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(550.8843, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([17.5906])\n",
      "Avg steps: 48.8\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 19\n",
      "Avg policy loss: tensor(-3.2098, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(788.8964, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([19.2627])\n",
      "Avg steps: 68.02\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 20\n",
      "Avg policy loss: tensor(-2.9130, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(859.5291, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([22.8779])\n",
      "Avg steps: 73.88\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 21\n",
      "Avg policy loss: tensor(-2.8839, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(887.9308, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([24.7780])\n",
      "Avg steps: 79.62\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 22\n",
      "Avg policy loss: tensor(-2.6991, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(927.6288, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([27.4266])\n",
      "Avg steps: 82.48\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 23\n",
      "Avg policy loss: tensor(-2.5711, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1040.3955, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([29.6027])\n",
      "Avg steps: 88.46\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 24\n",
      "Avg policy loss: tensor(-2.6462, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1156.0283, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([29.9613])\n",
      "Avg steps: 93.66\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 25\n",
      "Avg policy loss: tensor(-2.8526, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1240.9125, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([31.1940])\n",
      "Avg steps: 110.6\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 26\n",
      "Avg policy loss: tensor(-2.7570, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1568.3091, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([34.9801])\n",
      "Avg steps: 123.02\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 27\n",
      "Avg policy loss: tensor(-2.6644, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1430.0808, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([36.4555])\n",
      "Avg steps: 122.0\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 28\n",
      "Avg policy loss: tensor(-3.0402, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1722.4382, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([37.3563])\n",
      "Avg steps: 160.26\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 29\n",
      "Avg policy loss: tensor(-2.5184, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2030.5676, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([45.3269])\n",
      "Avg steps: 175.32\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 30\n",
      "Avg policy loss: tensor(-2.7484, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2303.3301, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([46.8289])\n",
      "Avg steps: 202.16\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 31\n",
      "Avg policy loss: tensor(-2.2212, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(2154.0688, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([52.2423])\n",
      "Avg steps: 191.46\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 32\n",
      "Avg policy loss: tensor(-2.3314, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1469.4897, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([51.6947])\n",
      "Avg steps: 201.38\n",
      "--------------------\n",
      "\n",
      "\n",
      "--------------------\n",
      "Iteration: 33\n",
      "Avg policy loss: tensor(-1.9817, grad_fn=<MeanBackward0>)\n",
      "Last baseline loss: tensor(1746.6075, grad_fn=<DivBackward0>)\n",
      "Avg return: tensor([57.6946])\n",
      "Avg steps: 204.22\n",
      "--------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c36b565dfd8ab44b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
