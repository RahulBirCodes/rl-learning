{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MLP setup and hyperparameters\n",
    "Setup hyperparameters and state tracking with simple MLP definition"
   ],
   "id": "9f0747e19293744a"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-04T22:23:35.374703Z",
     "start_time": "2025-08-04T22:23:35.367606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steps:\n",
    "# 1. Create pytorch simple mlp model\n",
    "# 2. Implement dqn algorithm with mlp model\n",
    "# 3. Test algorithm in certain environment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, in_dim, out_dim):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(in_dim, 64)\n",
    "    self.fc2 = nn.Linear(64, 64)\n",
    "    self.fc3 = nn.Linear(64, out_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "\n",
    "# Using CartPole-v1 env, so 4 state vars + actions\n",
    "# We have discrete actions so for efficiently our MLP will output [Q(state, action_1), Q(state, action_2)]\n",
    "# We will one-hot-encode the input state\n",
    "\n",
    "# Implement DQN algorithm\n",
    "\n",
    "# Constants\n",
    "replay_cap = 10000\n",
    "obs_space_dim = 4\n",
    "action_space_dim = 2\n",
    "episodes = 600\n",
    "horizon = 500\n",
    "eps = 0.1\n",
    "discount = 0.99\n",
    "C = 100\n",
    "minibatch_size = 64\n",
    "env = gym.make('CartPole-v1', render_mode='human', max_episode_steps=horizon)\n",
    "\n",
    "# Init replay memory with capacity replay_cap\n",
    "replay_mem = deque(maxlen=replay_cap)\n",
    "# Init action-value func Q w/ random weights\n",
    "q = MLP(4, 2)\n",
    "# Init target action-value func w/ random weights (separate selecting best action and updating val for training stabilization)\n",
    "q_hat = MLP(4, 2)\n",
    "# Init optimizer\n",
    "optimizer = torch.optim.AdamW(q.parameters(), lr=3e-4)\n",
    "# Init c counter\n",
    "c_step = 0\n",
    "# Init episode steps tracking\n",
    "episode_steps = []"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training run",
   "id": "8de9a214111d949a"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-04T22:23:42.526922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Each episode represents how long we run through the env each time. This is different from the horizon which signifies how far ahead we're optimizing the reward path for (we assume infinite horizon)\n",
    "for episode in range(episodes):\n",
    "    # Init state\n",
    "    obs, _ = env.reset()\n",
    "    episode_over = False\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not episode_over:\n",
    "        step += 1\n",
    "        # Get random action with prob eps or act greedily\n",
    "        obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        if torch.rand(1) < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_out = q(obs)\n",
    "            # We use q network to select action\n",
    "            action = torch.argmax(q(obs)).item()\n",
    "\n",
    "        # Execute action in emulator and observe reward and next state\n",
    "        obs_next, reward, terminated, truncated, _ = env.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Store new experience in replay memory\n",
    "        replay_mem.append((obs, action, reward, obs_next, episode_over))\n",
    "\n",
    "        # Sample random minibatch from replay mem\n",
    "        minibatch = random.sample(replay_mem, len(replay_mem) if len(replay_mem) < minibatch_size else minibatch_size)\n",
    "        y_j = torch.tensor([float(exp[2]) if exp[4] else float(exp[2]) + discount * torch.max(q_hat(torch.tensor(exp[3], dtype=torch.float32))).item() for exp in minibatch], dtype=torch.float32)\n",
    "\n",
    "        # Calculate loss and perform gradient descent step\n",
    "        q_vals = q(torch.stack([torch.tensor(exp[0]) for exp in minibatch]))\n",
    "        actions = torch.tensor([exp[1] for exp in minibatch])\n",
    "        q_vals = q_vals[torch.arange(len(minibatch)), actions]\n",
    "        loss = F.mse_loss(q_vals, y_j)\n",
    "\n",
    "        # Do only one gradient descent step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update state\n",
    "        obs = obs_next\n",
    "        c_step += 1\n",
    "\n",
    "        # Every C steps, copy weights from Q to Q_hat\n",
    "        if c_step % C == 0:\n",
    "            q_hat.load_state_dict(q.state_dict())\n",
    "\n",
    "    # Store episode steps\n",
    "    episode_steps.append(step)\n",
    "    print(f\"Finished episode {episode}, steps: {step}\")\n",
    "\n",
    "print(\"Finished training\")\n",
    "\n",
    "env.close()"
   ],
   "id": "77e5a1dc037c21b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episode_steps, 'o-')\n",
    "plt.title(f'Episode Steps Alive (Episodes 1-{episode + 1})')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps Alive')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "d454c72c5e045027"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Demo agent running in test environment\n",
    "\n",
    "input(\"Press Enter to start demo...\")\n",
    "print(\"Demo agent running in test environment\")\n",
    "\n",
    "test_env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "q.eval()\n",
    "\n",
    "obs, _ = test_env.reset()\n",
    "steps = 0\n",
    "lost = False\n",
    "while not lost:\n",
    "    action = torch.argmax(q(torch.tensor(obs))).item()\n",
    "    obs_next, reward, terminated, truncated, _ = test_env.step(action)\n",
    "    lost = terminated or truncated\n",
    "    obs = obs_next\n",
    "    steps += 1\n",
    "\n",
    "print(\"Steps:\", steps)\n",
    "print(\"Finished demo\")\n",
    "test_env.close()"
   ],
   "id": "58bd4f624b8f65c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
