{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vanilla policy gradient algorithm from scratch",
   "id": "44c341a36a312921"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-09T00:15:13.917794Z",
     "start_time": "2025-08-09T00:15:13.900684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steps:\n",
    "# 1. Create an env wrapper for continuous and discrete environments. For continuous environments, policy network needs to output parameters for a multivariate gaussian distribution.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import torch.distributions as dist\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, action_space_dim, is_continuous):\n",
    "        super().__init__()\n",
    "        self.is_continuous = is_continuous\n",
    "        self.state_dim = state_space_dim\n",
    "        self.action_space_dim = action_space_dim\n",
    "        self.fc1 = nn.Linear(state_space_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        # For continuous action dim, return mean and only diagonal entries on covariance matrix since action states are probably independent\n",
    "        self.fc3 = nn.Linear(64, 2 * action_space_dim if is_continuous else action_space_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        if self.is_continuous:\n",
    "            mean = x[..., :self.action_space_dim]\n",
    "            covar = torch.exp(x[..., self.action_space_dim:])\n",
    "            return mean, torch.diag(covar)\n",
    "        else:\n",
    "            return torch.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class BaselineVNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, max_timesteps):\n",
    "        super().__init__()\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.fc1 = nn.Linear(state_space_dim + 1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, timestep):\n",
    "        timestep = timestep.float() / self.max_timesteps\n",
    "        x = F.relu(self.fc1(torch.cat([state, timestep.unsqueeze(-1)])))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "# Implement vpg alg\n",
    "iterations = 100\n",
    "max_timesteps = 500\n",
    "t_per_iter = 50\n",
    "env = gym.make('CartPole-v1', render_mode='human', max_episode_steps=max_timesteps)\n",
    "state_space_dim = 4\n",
    "action_space_dim = 2\n",
    "policy = PolicyNetwork(state_space_dim, action_space_dim, False)\n",
    "policy_opt = torch.optim.AdamW(policy.parameters(), lr=5e-4)\n",
    "baseline = BaselineVNetwork(state_space_dim, max_timesteps)\n",
    "baseline_opt = torch.optim.AdamW(baseline.parameters(), lr=1e-2)\n",
    "baseline_opt_steps = 10\n",
    "discount = 0.99"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training run",
   "id": "467ecbf12c62b3d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T02:01:00.850266Z",
     "start_time": "2025-08-09T02:58:23.744278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for iter in range(iterations):\n",
    "    # Collect a set of trajectories by executing the current policy\n",
    "    trajectories = []\n",
    "    for t_ind in range(t_per_iter):\n",
    "        # print(\"Collecting trajectory:\", t_ind)\n",
    "        current_state, _ = env.reset()\n",
    "        current_state = torch.tensor(current_state, dtype=torch.float32)\n",
    "        traj = []\n",
    "        for _ in range(max_timesteps):\n",
    "            actions_dist = policy(current_state)\n",
    "            if policy.is_continuous:\n",
    "                mean, covar = actions_dist\n",
    "                mgd = dist.MultivariateNormal(mean, covar)\n",
    "                action = mgd.sample()\n",
    "            else:\n",
    "                d = dist.Categorical(actions_dist)\n",
    "                action = d.sample()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            new_step = (current_state, action, reward)\n",
    "            traj.append(new_step)\n",
    "            # print(\"Added step:\", new_step)\n",
    "            current_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        trajectories.append(traj)\n",
    "        # print(\"Added trajectory:\", traj)\n",
    "\n",
    "    # print(\"Finished collecting trajectories\\n\\n\")\n",
    "\n",
    "    # At each timestep in each trajectory, compute the return and advantage estimate\n",
    "    returns = [[sum(discount**(t_prime - time) * traj[t_prime][2] for t_prime in range(time, len(traj))) for time in range(len(traj))] for traj in trajectories]\n",
    "    # for traj_return in returns[:1]:\n",
    "    #     print(\"\\n\\nTrajectory returns:\", traj_return)\n",
    "\n",
    "    def calc_baselines():\n",
    "       return [[baseline(traj[time][0], torch.tensor(time, dtype=torch.float32)) for time in range(len(traj))] for traj in trajectories]\n",
    "\n",
    "    def calc_adv_ests():\n",
    "        baselines = calc_baselines()\n",
    "        return [[(returns[traj_ind][step_ind] - baselines[traj_ind][step_ind]) for step_ind in range(len(trajectories[traj_ind]))] for traj_ind in range(len(trajectories))]\n",
    "\n",
    "    def normalize_adv_ests(adv_ests):\n",
    "        flattened = torch.stack([a for t in adv_ests for a in t])\n",
    "        return [[(ae - flattened.mean()) / (flattened.std() + 1e-8) for ae in t_ae] for t_ae in adv_ests]\n",
    "\n",
    "    def calc_baseline_loss():\n",
    "        adv_ests = calc_adv_ests()\n",
    "        adv_ests = [a for traj_a in adv_ests for a in traj_a]\n",
    "        return (torch.stack(adv_ests) ** 2).sum() / len(trajectories)\n",
    "\n",
    "    # Re-fit the baseline\n",
    "    last_baseline_loss = 0\n",
    "    # print(\"Refitting the baseline\")\n",
    "    for i in range(baseline_opt_steps):\n",
    "        baseline_loss = calc_baseline_loss()\n",
    "        last_baseline_loss = baseline_loss\n",
    "        # print(\"On baseline value opt step:\", i, \" with loss:\", baseline_loss)\n",
    "        baseline_opt.zero_grad()\n",
    "        baseline_loss.backward()\n",
    "        baseline_opt.step()\n",
    "    # print(\"Finished refitting the baseline\\n\\n\")\n",
    "\n",
    "    # Update the policy using a policy gradient estimate\n",
    "    adv_ests = normalize_adv_ests(calc_adv_ests())\n",
    "    g = []\n",
    "    for traj_ind in range(len(trajectories)):\n",
    "        traj = trajectories[traj_ind]\n",
    "        for step_ind in range(len(traj)):\n",
    "            step_state, step_action, step_reward = traj[step_ind]\n",
    "            if policy.is_continuous:\n",
    "                mean, covar = policy(torch.tensor(step_state))\n",
    "                mvg = dist.MultivariateNormal(mean, covar)\n",
    "                log_pdf_val = mvg.log_prob(step_action)\n",
    "                g.append(log_pdf_val * adv_ests[traj_ind][step_ind])\n",
    "            else:\n",
    "                action_dist = policy(torch.tensor(step_state))\n",
    "                g.append(torch.log(action_dist[step_action]) * adv_ests[traj_ind][step_ind])\n",
    "    # Take only one gradient step for policy to avoid overfitting\n",
    "    policy_loss = -torch.stack(g).sum()\n",
    "    print(\"--------------------\")\n",
    "    print(\"Iteration:\", iter)\n",
    "    print(\"Policy loss:\", policy_loss)\n",
    "    print(\"Last baseline loss:\", last_baseline_loss)\n",
    "    print(\"Avg return:\", sum(r[0] for r in returns) / len(returns))\n",
    "    print(\"Avg steps:\", sum(len(t) for t in trajectories) / len(trajectories))\n",
    "    print(\"--------------------\")\n",
    "    policy_opt.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_opt.step()"
   ],
   "id": "b1fdc00cc63981f3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/sx805f6s2llg753msgs16xfr0000gn/T/ipykernel_65421/2564214388.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_dist = policy(torch.tensor(step_state))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Iteration: 0\n",
      "Policy loss: tensor(-382.1570, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(58739.0859, grad_fn=<DivBackward0>)\n",
      "Avg return: 89.6942119985307\n",
      "Avg steps: 286.78\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 1\n",
      "Policy loss: tensor(-407.3205, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(63817.8711, grad_fn=<DivBackward0>)\n",
      "Avg return: 90.65410852387194\n",
      "Avg steps: 285.12\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 2\n",
      "Policy loss: tensor(-381.2319, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(74894.1562, grad_fn=<DivBackward0>)\n",
      "Avg return: 92.6073974685978\n",
      "Avg steps: 307.44\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 3\n",
      "Policy loss: tensor(-333.5525, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(57231.9805, grad_fn=<DivBackward0>)\n",
      "Avg return: 93.03779050503027\n",
      "Avg steps: 328.7\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 4\n",
      "Policy loss: tensor(-268.2674, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(60664.8398, grad_fn=<DivBackward0>)\n",
      "Avg return: 91.49812389775323\n",
      "Avg steps: 305.5\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 5\n",
      "Policy loss: tensor(-325.8524, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(79617.6328, grad_fn=<DivBackward0>)\n",
      "Avg return: 93.16633211438129\n",
      "Avg steps: 336.62\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 6\n",
      "Policy loss: tensor(-395.8839, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(70890.0469, grad_fn=<DivBackward0>)\n",
      "Avg return: 92.63446696780007\n",
      "Avg steps: 349.12\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 7\n",
      "Policy loss: tensor(-460.6984, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(97409.6016, grad_fn=<DivBackward0>)\n",
      "Avg return: 93.56258170021246\n",
      "Avg steps: 348.64\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 8\n",
      "Policy loss: tensor(-410.6444, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(69547.8203, grad_fn=<DivBackward0>)\n",
      "Avg return: 94.20765115055903\n",
      "Avg steps: 359.88\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 9\n",
      "Policy loss: tensor(-199.7409, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(61821.1641, grad_fn=<DivBackward0>)\n",
      "Avg return: 95.21210523804002\n",
      "Avg steps: 384.26\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 10\n",
      "Policy loss: tensor(-359.7375, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(83524.3516, grad_fn=<DivBackward0>)\n",
      "Avg return: 94.95256257061857\n",
      "Avg steps: 342.6\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 11\n",
      "Policy loss: tensor(-396.8050, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(77106.3203, grad_fn=<DivBackward0>)\n",
      "Avg return: 96.22350030566162\n",
      "Avg steps: 396.82\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 12\n",
      "Policy loss: tensor(-288.8070, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(67313.5234, grad_fn=<DivBackward0>)\n",
      "Avg return: 96.84159158120073\n",
      "Avg steps: 402.94\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 13\n",
      "Policy loss: tensor(-364.7137, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(63245.3867, grad_fn=<DivBackward0>)\n",
      "Avg return: 95.7820271540026\n",
      "Avg steps: 373.14\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 14\n",
      "Policy loss: tensor(-318.2195, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(54002.4297, grad_fn=<DivBackward0>)\n",
      "Avg return: 97.81308109359446\n",
      "Avg steps: 438.14\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 15\n",
      "Policy loss: tensor(-249.5221, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(52145.5469, grad_fn=<DivBackward0>)\n",
      "Avg return: 96.8097380426203\n",
      "Avg steps: 417.5\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 16\n",
      "Policy loss: tensor(-319.9221, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(70767.2188, grad_fn=<DivBackward0>)\n",
      "Avg return: 95.97819761415951\n",
      "Avg steps: 389.32\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 17\n",
      "Policy loss: tensor(-401.0816, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(59364.2500, grad_fn=<DivBackward0>)\n",
      "Avg return: 97.1251563953849\n",
      "Avg steps: 405.88\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 18\n",
      "Policy loss: tensor(-393.8658, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(52836.7383, grad_fn=<DivBackward0>)\n",
      "Avg return: 97.42965767257891\n",
      "Avg steps: 426.6\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 19\n",
      "Policy loss: tensor(-282.3206, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(42595.9492, grad_fn=<DivBackward0>)\n",
      "Avg return: 97.95512374280938\n",
      "Avg steps: 445.14\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 20\n",
      "Policy loss: tensor(-247.6917, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(41498.8711, grad_fn=<DivBackward0>)\n",
      "Avg return: 97.46750484989666\n",
      "Avg steps: 426.88\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 21\n",
      "Policy loss: tensor(-485.5230, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(53518.0547, grad_fn=<DivBackward0>)\n",
      "Avg return: 97.3737911707237\n",
      "Avg steps: 426.82\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 22\n",
      "Policy loss: tensor(-379.2408, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(67162.8828, grad_fn=<DivBackward0>)\n",
      "Avg return: 96.78426989731497\n",
      "Avg steps: 406.68\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 23\n",
      "Policy loss: tensor(-367.8100, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(45728.0469, grad_fn=<DivBackward0>)\n",
      "Avg return: 97.00767917858114\n",
      "Avg steps: 417.74\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 24\n",
      "Policy loss: tensor(-344.7966, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(55716.9102, grad_fn=<DivBackward0>)\n",
      "Avg return: 97.76057497528241\n",
      "Avg steps: 438.46\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 25\n",
      "Policy loss: tensor(-285.2365, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(39435.1914, grad_fn=<DivBackward0>)\n",
      "Avg return: 97.64247491891436\n",
      "Avg steps: 428.78\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 26\n",
      "Policy loss: tensor(-298.7502, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(45197.1289, grad_fn=<DivBackward0>)\n",
      "Avg return: 97.83470296805355\n",
      "Avg steps: 441.8\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 27\n",
      "Policy loss: tensor(-259.4846, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(28774.7441, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.85568821468418\n",
      "Avg steps: 471.9\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 28\n",
      "Policy loss: tensor(-250.3040, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(32499.4297, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.90407470722246\n",
      "Avg steps: 471.06\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 29\n",
      "Policy loss: tensor(-284.2502, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(24430.9004, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.74228034858315\n",
      "Avg steps: 463.32\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 30\n",
      "Policy loss: tensor(-285.8522, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(29798.3730, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.34035309159007\n",
      "Avg steps: 461.6\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 31\n",
      "Policy loss: tensor(-257.0317, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(29333.4355, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.3747900758324\n",
      "Avg steps: 467.86\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 32\n",
      "Policy loss: tensor(-256.4803, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(43019.0312, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.31115265598088\n",
      "Avg steps: 455.28\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 33\n",
      "Policy loss: tensor(-212.2829, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(37642.0352, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.23864245102577\n",
      "Avg steps: 463.04\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 34\n",
      "Policy loss: tensor(-239.1117, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(19842.4746, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.91771994156183\n",
      "Avg steps: 476.72\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 35\n",
      "Policy loss: tensor(-287.7454, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(20416.8145, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.89883799949901\n",
      "Avg steps: 476.82\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 36\n",
      "Policy loss: tensor(-282.6796, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(28509.4121, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.71307805544191\n",
      "Avg steps: 463.48\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 37\n",
      "Policy loss: tensor(-243.7803, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(18912.0293, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.9402284182778\n",
      "Avg steps: 478.98\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 38\n",
      "Policy loss: tensor(-267.5919, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(20755.3027, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.67639474895672\n",
      "Avg steps: 479.9\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 39\n",
      "Policy loss: tensor(-242.6302, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(7229.9619, grad_fn=<DivBackward0>)\n",
      "Avg return: 99.1495842201266\n",
      "Avg steps: 487.68\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 40\n",
      "Policy loss: tensor(-140.0914, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(8817.4707, grad_fn=<DivBackward0>)\n",
      "Avg return: 98.75853525954103\n",
      "Avg steps: 482.8\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[112]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     16\u001B[39m     d = dist.Categorical(actions_dist)\n\u001B[32m     17\u001B[39m     action = d.sample()\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m next_state, reward, terminated, truncated, _ = \u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m new_step = (current_state, action, reward)\n\u001B[32m     21\u001B[39m traj.append(new_step)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:125\u001B[39m, in \u001B[36mTimeLimit.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    113\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[32m    114\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[ObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    115\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[32m    116\u001B[39m \n\u001B[32m    117\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    123\u001B[39m \n\u001B[32m    124\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     observation, reward, terminated, truncated, info = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    126\u001B[39m     \u001B[38;5;28mself\u001B[39m._elapsed_steps += \u001B[32m1\u001B[39m\n\u001B[32m    128\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._elapsed_steps >= \u001B[38;5;28mself\u001B[39m._max_episode_steps:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001B[39m, in \u001B[36mOrderEnforcing.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    391\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._has_reset:\n\u001B[32m    392\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[33m\"\u001B[39m\u001B[33mCannot call env.step() before calling env.reset()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/core.py:327\u001B[39m, in \u001B[36mWrapper.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    323\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    324\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[32m    325\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    326\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:285\u001B[39m, in \u001B[36mPassiveEnvChecker.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    283\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m.env, action)\n\u001B[32m    284\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:223\u001B[39m, in \u001B[36mCartPoleEnv.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    220\u001B[39m     reward = -\u001B[32m1.0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sutton_barto_reward \u001B[38;5;28;01melse\u001B[39;00m \u001B[32m0.0\u001B[39m\n\u001B[32m    222\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mhuman\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m223\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    225\u001B[39m \u001B[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001B[39;00m\n\u001B[32m    226\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m np.array(\u001B[38;5;28mself\u001B[39m.state, dtype=np.float32), reward, terminated, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:337\u001B[39m, in \u001B[36mCartPoleEnv.render\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    335\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mhuman\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    336\u001B[39m     pygame.event.pump()\n\u001B[32m--> \u001B[39m\u001B[32m337\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrender_fps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    338\u001B[39m     pygame.display.flip()\n\u001B[32m    340\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mrgb_array\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T22:21:21.994724Z",
     "start_time": "2025-08-08T06:42:48.081611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test implementation\n",
    "current_state, _ = env.reset()\n",
    "current_state = torch.tensor(current_state, dtype=torch.float32)\n",
    "for _ in range(max_timesteps):\n",
    "    actions_dist = policy(current_state)\n",
    "    if policy.is_continuous:\n",
    "        mean, covar = actions_dist\n",
    "        mgd = dist.MultivariateNormal(mean, covar)\n",
    "        action = mgd.sample()\n",
    "    else:\n",
    "        d = dist.Categorical(actions_dist)\n",
    "        action = d.sample()\n",
    "\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    current_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break"
   ],
   "id": "e303f76cb01fddd4",
   "outputs": [],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
