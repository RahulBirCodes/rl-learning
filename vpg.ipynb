{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vanilla policy gradient algorithm from scratch",
   "id": "44c341a36a312921"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-08T08:20:15.334866Z",
     "start_time": "2025-08-08T08:20:15.323947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steps:\n",
    "# 1. Create an env wrapper for continuous and discrete environments. For continuous environments, policy network needs to output parameters for a multivariate gaussian distribution.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import torch.distributions as dist\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, action_space_dim, is_continuous):\n",
    "        super().__init__()\n",
    "        self.is_continuous = is_continuous\n",
    "        self.state_dim = state_space_dim\n",
    "        self.action_space_dim = action_space_dim\n",
    "        self.fc1 = nn.Linear(state_space_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        # For continuous action dim, return mean and only diagonal entries on covariance matrix since action states are probably independent\n",
    "        self.fc3 = nn.Linear(64, 2 * action_space_dim if is_continuous else action_space_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        if self.is_continuous:\n",
    "            mean = x[..., :self.action_space_dim]\n",
    "            covar = torch.exp(x[..., self.action_space_dim:])\n",
    "            return mean, torch.diag(covar)\n",
    "        else:\n",
    "            return torch.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class BaselineVNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, max_timesteps):\n",
    "        super().__init__()\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.fc1 = nn.Linear(state_space_dim + 1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, timestep):\n",
    "        timestep = timestep.float() / self.max_timesteps\n",
    "        x = F.relu(self.fc1(torch.cat([state, timestep.unsqueeze(-1)])))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "# Implement vpg alg\n",
    "iterations = 150\n",
    "max_timesteps = 500\n",
    "t_per_iter = 50\n",
    "env = gym.make('CartPole-v1', render_mode='human', max_episode_steps=max_timesteps)\n",
    "state_space_dim = 4\n",
    "action_space_dim = 2\n",
    "policy = PolicyNetwork(state_space_dim, action_space_dim, False)\n",
    "policy_opt = torch.optim.AdamW(policy.parameters(), lr=3e-4)\n",
    "baseline = BaselineVNetwork(state_space_dim, max_timesteps)\n",
    "baseline_opt = torch.optim.AdamW(baseline.parameters(), lr=1e-3)\n",
    "baseline_opt_steps = 10\n",
    "discount = 0.99"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training run",
   "id": "467ecbf12c62b3d4"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-08T08:20:16.478370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for iter in range(iterations):\n",
    "    print(\"Starting iteration:\", iter)\n",
    "    # Collect a set of trajectories by executing the current policy\n",
    "    trajectories = []\n",
    "    for t_ind in range(t_per_iter):\n",
    "        # print(\"Collecting trajectory:\", t_ind)\n",
    "        current_state, _ = env.reset()\n",
    "        current_state = torch.tensor(current_state, dtype=torch.float32)\n",
    "        traj = []\n",
    "        for _ in range(max_timesteps):\n",
    "            actions_dist = policy(current_state)\n",
    "            if policy.is_continuous:\n",
    "                mean, covar = actions_dist\n",
    "                mgd = dist.MultivariateNormal(mean, covar)\n",
    "                action = mgd.sample()\n",
    "            else:\n",
    "                d = dist.Categorical(actions_dist)\n",
    "                action = d.sample()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            new_step = (current_state, action, reward)\n",
    "            traj.append(new_step)\n",
    "            # print(\"Added step:\", new_step)\n",
    "            current_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        trajectories.append(traj)\n",
    "        # print(\"Added trajectory:\", traj)\n",
    "\n",
    "    print(\"Finished collecting trajectories\\n\\n\")\n",
    "\n",
    "    # At each timestep in each trajectory, compute the return and advantage estimate\n",
    "    returns = [[sum(discount**(t_prime - time) * traj[t_prime][2] for t_prime in range(time, len(traj))) for time in range(len(traj))] for traj in trajectories]\n",
    "    # for traj_return in returns[:1]:\n",
    "    #     print(\"\\n\\nTrajectory returns:\", traj_return)\n",
    "\n",
    "    def calc_baselines():\n",
    "       return [[baseline(traj[time][0], torch.tensor(time, dtype=torch.float32)) for time in range(len(traj))] for traj in trajectories]\n",
    "\n",
    "    def calc_adv_ests():\n",
    "        baselines = calc_baselines()\n",
    "        return [[(returns[traj_ind][step_ind] - baselines[traj_ind][step_ind]) for step_ind in range(len(trajectories[traj_ind]))] for traj_ind in range(len(trajectories))]\n",
    "\n",
    "    def calc_baseline_loss():\n",
    "        adv_ests = calc_adv_ests()\n",
    "        adv_ests = [a for traj_a in adv_ests for a in traj_a]\n",
    "        return (torch.stack(adv_ests) ** 2).sum()\n",
    "\n",
    "    # Re-fit the baseline\n",
    "    print(\"Refitting the baseline\")\n",
    "    for _ in range(baseline_opt_steps):\n",
    "        baseline_loss = calc_baseline_loss()\n",
    "        print(\"Baseline loss:\", baseline_loss)\n",
    "        baseline_opt.zero_grad()\n",
    "        baseline_loss.backward()\n",
    "        baseline_opt.step()\n",
    "    print(\"Finished refitting the baseline\\n\\n\")\n",
    "\n",
    "    # Update the policy using a policy gradient estimate\n",
    "    adv_ests = calc_adv_ests()\n",
    "    g = []\n",
    "    for traj_ind in range(len(trajectories)):\n",
    "        traj = trajectories[traj_ind]\n",
    "        for step_ind in range(len(traj)):\n",
    "            step_state, step_action, step_reward = traj[step_ind]\n",
    "            if policy.is_continuous:\n",
    "                mean, covar = policy(torch.tensor(step_state))\n",
    "                mvg = dist.MultivariateNormal(mean, covar)\n",
    "                log_pdf_val = mvg.log_prob(step_action)\n",
    "                g.append(log_pdf_val * adv_ests[traj_ind][step_ind])\n",
    "            else:\n",
    "                action_dist = policy(torch.tensor(step_state))\n",
    "                g.append(torch.log(action_dist[step_action]) * adv_ests[traj_ind][step_ind])\n",
    "    # Take only one gradient step for policy to avoid overfitting\n",
    "    policy_loss = -torch.stack(g).sum()\n",
    "    print(\"Policy loss:\", policy_loss)\n",
    "    policy_opt.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_opt.step()"
   ],
   "id": "b1fdc00cc63981f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration: 0\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T06:42:48.294866Z",
     "start_time": "2025-08-08T06:42:48.081611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test implementation\n",
    "current_state, _ = env.reset()\n",
    "current_state = torch.tensor(current_state, dtype=torch.float32)\n",
    "for _ in range(max_timesteps):\n",
    "    actions_dist = policy(current_state)\n",
    "    if policy.is_continuous:\n",
    "        mean, covar = actions_dist\n",
    "        mgd = dist.MultivariateNormal(mean, covar)\n",
    "        action = mgd.sample()\n",
    "    else:\n",
    "        d = dist.Categorical(actions_dist)\n",
    "        action = d.sample()\n",
    "\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    current_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break"
   ],
   "id": "e303f76cb01fddd4",
   "outputs": [],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
