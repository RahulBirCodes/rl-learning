{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vanilla policy gradient algorithm from scratch",
   "id": "44c341a36a312921"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-09T00:15:13.917794Z",
     "start_time": "2025-08-09T00:15:13.900684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Steps:\n",
    "# 1. Create an env wrapper for continuous and discrete environments. For continuous environments, policy network needs to output parameters for a multivariate gaussian distribution.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import torch.distributions as dist\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, action_space_dim, is_continuous):\n",
    "        super().__init__()\n",
    "        self.is_continuous = is_continuous\n",
    "        self.state_dim = state_space_dim\n",
    "        self.action_space_dim = action_space_dim\n",
    "        self.fc1 = nn.Linear(state_space_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        # For continuous action dim, return mean and only diagonal entries on covariance matrix since action states are probably independent\n",
    "        self.fc3 = nn.Linear(64, 2 * action_space_dim if is_continuous else action_space_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        if self.is_continuous:\n",
    "            mean = x[..., :self.action_space_dim]\n",
    "            covar = torch.exp(x[..., self.action_space_dim:])\n",
    "            return mean, torch.diag(covar)\n",
    "        else:\n",
    "            return torch.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class BaselineVNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, max_timesteps):\n",
    "        super().__init__()\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.fc1 = nn.Linear(state_space_dim + 1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, timestep):\n",
    "        timestep = timestep.float() / self.max_timesteps\n",
    "        x = F.relu(self.fc1(torch.cat([state, timestep.unsqueeze(-1)])))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "# Implement vpg alg\n",
    "iterations = 100\n",
    "max_timesteps = 500\n",
    "t_per_iter = 50\n",
    "env = gym.make('CartPole-v1', render_mode='human', max_episode_steps=max_timesteps)\n",
    "state_space_dim = 4\n",
    "action_space_dim = 2\n",
    "policy = PolicyNetwork(state_space_dim, action_space_dim, False)\n",
    "policy_opt = torch.optim.AdamW(policy.parameters(), lr=5e-4)\n",
    "baseline = BaselineVNetwork(state_space_dim, max_timesteps)\n",
    "baseline_opt = torch.optim.AdamW(baseline.parameters(), lr=1e-2)\n",
    "baseline_opt_steps = 10\n",
    "discount = 0.99"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training run",
   "id": "467ecbf12c62b3d4"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-09T00:15:15.775687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for iter in range(iterations):\n",
    "    # Collect a set of trajectories by executing the current policy\n",
    "    trajectories = []\n",
    "    for t_ind in range(t_per_iter):\n",
    "        # print(\"Collecting trajectory:\", t_ind)\n",
    "        current_state, _ = env.reset()\n",
    "        current_state = torch.tensor(current_state, dtype=torch.float32)\n",
    "        traj = []\n",
    "        for _ in range(max_timesteps):\n",
    "            actions_dist = policy(current_state)\n",
    "            if policy.is_continuous:\n",
    "                mean, covar = actions_dist\n",
    "                mgd = dist.MultivariateNormal(mean, covar)\n",
    "                action = mgd.sample()\n",
    "            else:\n",
    "                d = dist.Categorical(actions_dist)\n",
    "                action = d.sample()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            new_step = (current_state, action, reward)\n",
    "            traj.append(new_step)\n",
    "            # print(\"Added step:\", new_step)\n",
    "            current_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        trajectories.append(traj)\n",
    "        # print(\"Added trajectory:\", traj)\n",
    "\n",
    "    # print(\"Finished collecting trajectories\\n\\n\")\n",
    "\n",
    "    # At each timestep in each trajectory, compute the return and advantage estimate\n",
    "    returns = [[sum(discount**(t_prime - time) * traj[t_prime][2] for t_prime in range(time, len(traj))) for time in range(len(traj))] for traj in trajectories]\n",
    "    # for traj_return in returns[:1]:\n",
    "    #     print(\"\\n\\nTrajectory returns:\", traj_return)\n",
    "\n",
    "    def calc_baselines():\n",
    "       return [[baseline(traj[time][0], torch.tensor(time, dtype=torch.float32)) for time in range(len(traj))] for traj in trajectories]\n",
    "\n",
    "    def calc_adv_ests():\n",
    "        baselines = calc_baselines()\n",
    "        return [[(returns[traj_ind][step_ind] - baselines[traj_ind][step_ind]) for step_ind in range(len(trajectories[traj_ind]))] for traj_ind in range(len(trajectories))]\n",
    "\n",
    "    def normalize_adv_ests(adv_ests):\n",
    "        flattened = torch.stack([a for t in adv_ests for a in t])\n",
    "        return [[(ae - flattened.mean()) / (flattened.std() + 1e-8) for ae in t_ae] for t_ae in adv_ests]\n",
    "\n",
    "    def calc_baseline_loss():\n",
    "        adv_ests = calc_adv_ests()\n",
    "        adv_ests = [a for traj_a in adv_ests for a in traj_a]\n",
    "        return (torch.stack(adv_ests) ** 2).sum() / len(trajectories)\n",
    "\n",
    "    # Re-fit the baseline\n",
    "    last_baseline_loss = 0\n",
    "    # print(\"Refitting the baseline\")\n",
    "    for i in range(baseline_opt_steps):\n",
    "        baseline_loss = calc_baseline_loss()\n",
    "        last_baseline_loss = baseline_loss\n",
    "        # print(\"On baseline value opt step:\", i, \" with loss:\", baseline_loss)\n",
    "        baseline_opt.zero_grad()\n",
    "        baseline_loss.backward()\n",
    "        baseline_opt.step()\n",
    "    # print(\"Finished refitting the baseline\\n\\n\")\n",
    "\n",
    "    # Update the policy using a policy gradient estimate\n",
    "    adv_ests = normalize_adv_ests(calc_adv_ests())\n",
    "    g = []\n",
    "    for traj_ind in range(len(trajectories)):\n",
    "        traj = trajectories[traj_ind]\n",
    "        for step_ind in range(len(traj)):\n",
    "            step_state, step_action, step_reward = traj[step_ind]\n",
    "            if policy.is_continuous:\n",
    "                mean, covar = policy(torch.tensor(step_state))\n",
    "                mvg = dist.MultivariateNormal(mean, covar)\n",
    "                log_pdf_val = mvg.log_prob(step_action)\n",
    "                g.append(log_pdf_val * adv_ests[traj_ind][step_ind])\n",
    "            else:\n",
    "                action_dist = policy(torch.tensor(step_state))\n",
    "                g.append(torch.log(action_dist[step_action]) * adv_ests[traj_ind][step_ind])\n",
    "    # Take only one gradient step for policy to avoid overfitting\n",
    "    policy_loss = -torch.stack(g).sum()\n",
    "    print(\"--------------------\")\n",
    "    print(\"Iteration:\", iter)\n",
    "    print(\"Policy loss:\", policy_loss)\n",
    "    print(\"Last baseline loss:\", last_baseline_loss)\n",
    "    print(\"Avg return:\", sum(r[0] for r in returns) / len(returns))\n",
    "    print(\"Avg steps:\", sum(len(t) for t in trajectories) / len(trajectories))\n",
    "    print(\"--------------------\")\n",
    "    policy_opt.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_opt.step()"
   ],
   "id": "b1fdc00cc63981f3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/sx805f6s2llg753msgs16xfr0000gn/T/ipykernel_65421/2564214388.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_dist = policy(torch.tensor(step_state))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Iteration: 0\n",
      "Policy loss: tensor(-2.4556, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(4033.0081, grad_fn=<DivBackward0>)\n",
      "Avg return: 19.284301741493184\n",
      "Avg steps: 22.04\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 1\n",
      "Policy loss: tensor(-1.5156, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(1580.8778, grad_fn=<DivBackward0>)\n",
      "Avg return: 18.410955332812215\n",
      "Avg steps: 20.56\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 2\n",
      "Policy loss: tensor(-4.1875, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(1657.7120, grad_fn=<DivBackward0>)\n",
      "Avg return: 18.88438876393039\n",
      "Avg steps: 21.34\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 3\n",
      "Policy loss: tensor(-4.8079, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(2092.9199, grad_fn=<DivBackward0>)\n",
      "Avg return: 21.421476837201467\n",
      "Avg steps: 24.66\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 4\n",
      "Policy loss: tensor(-5.1314, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(1812.2322, grad_fn=<DivBackward0>)\n",
      "Avg return: 20.330143021856735\n",
      "Avg steps: 23.24\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 5\n",
      "Policy loss: tensor(-6.8840, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(2093.1768, grad_fn=<DivBackward0>)\n",
      "Avg return: 21.011650105479067\n",
      "Avg steps: 24.48\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 6\n",
      "Policy loss: tensor(-10.8456, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(846.0967, grad_fn=<DivBackward0>)\n",
      "Avg return: 19.25443940338819\n",
      "Avg steps: 21.6\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 7\n",
      "Policy loss: tensor(-7.9578, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(2733.0867, grad_fn=<DivBackward0>)\n",
      "Avg return: 21.27266377861215\n",
      "Avg steps: 24.86\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 8\n",
      "Policy loss: tensor(-7.0607, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(3645.4900, grad_fn=<DivBackward0>)\n",
      "Avg return: 20.56309921124651\n",
      "Avg steps: 24.22\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 9\n",
      "Policy loss: tensor(-11.3352, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(2268.6433, grad_fn=<DivBackward0>)\n",
      "Avg return: 20.811519892521165\n",
      "Avg steps: 24.12\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 10\n",
      "Policy loss: tensor(-12.2974, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(1159.2601, grad_fn=<DivBackward0>)\n",
      "Avg return: 20.852718084435537\n",
      "Avg steps: 23.84\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 11\n",
      "Policy loss: tensor(-16.8947, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(1219.6240, grad_fn=<DivBackward0>)\n",
      "Avg return: 21.38118257026853\n",
      "Avg steps: 24.6\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 12\n",
      "Policy loss: tensor(-16.5857, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(2365.4666, grad_fn=<DivBackward0>)\n",
      "Avg return: 22.57763775877519\n",
      "Avg steps: 26.54\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 13\n",
      "Policy loss: tensor(-17.8642, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(1665.7404, grad_fn=<DivBackward0>)\n",
      "Avg return: 21.45288495444441\n",
      "Avg steps: 24.8\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 14\n",
      "Policy loss: tensor(-15.4135, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(1858.6938, grad_fn=<DivBackward0>)\n",
      "Avg return: 21.79988808973883\n",
      "Avg steps: 25.36\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 15\n",
      "Policy loss: tensor(-13.1034, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(6083.0879, grad_fn=<DivBackward0>)\n",
      "Avg return: 23.081021190165778\n",
      "Avg steps: 28.42\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 16\n",
      "Policy loss: tensor(-21.4627, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(2376.0269, grad_fn=<DivBackward0>)\n",
      "Avg return: 23.396626389106576\n",
      "Avg steps: 27.56\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 17\n",
      "Policy loss: tensor(-25.0780, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(1778.0004, grad_fn=<DivBackward0>)\n",
      "Avg return: 22.134551456736173\n",
      "Avg steps: 25.9\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 18\n",
      "Policy loss: tensor(-20.7495, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(1564.7852, grad_fn=<DivBackward0>)\n",
      "Avg return: 21.990192415037367\n",
      "Avg steps: 25.46\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 19\n",
      "Policy loss: tensor(-19.9323, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(3201.8013, grad_fn=<DivBackward0>)\n",
      "Avg return: 24.00757319608796\n",
      "Avg steps: 28.6\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 20\n",
      "Policy loss: tensor(-25.8868, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(1192.3148, grad_fn=<DivBackward0>)\n",
      "Avg return: 20.323900660179913\n",
      "Avg steps: 23.2\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 21\n",
      "Policy loss: tensor(-26.7371, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(2322.3357, grad_fn=<DivBackward0>)\n",
      "Avg return: 22.071773169135614\n",
      "Avg steps: 25.88\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 22\n",
      "Policy loss: tensor(-30.4011, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(2356.3071, grad_fn=<DivBackward0>)\n",
      "Avg return: 22.392348565060644\n",
      "Avg steps: 26.22\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 23\n",
      "Policy loss: tensor(-31.6378, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(5362.2974, grad_fn=<DivBackward0>)\n",
      "Avg return: 24.87210374655686\n",
      "Avg steps: 30.56\n",
      "--------------------\n",
      "--------------------\n",
      "Iteration: 24\n",
      "Policy loss: tensor(-35.2746, grad_fn=<NegBackward0>)\n",
      "Last baseline loss: tensor(4444.0654, grad_fn=<DivBackward0>)\n",
      "Avg return: 26.245788875938096\n",
      "Avg steps: 32.18\n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T22:21:21.994724Z",
     "start_time": "2025-08-08T06:42:48.081611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test implementation\n",
    "current_state, _ = env.reset()\n",
    "current_state = torch.tensor(current_state, dtype=torch.float32)\n",
    "for _ in range(max_timesteps):\n",
    "    actions_dist = policy(current_state)\n",
    "    if policy.is_continuous:\n",
    "        mean, covar = actions_dist\n",
    "        mgd = dist.MultivariateNormal(mean, covar)\n",
    "        action = mgd.sample()\n",
    "    else:\n",
    "        d = dist.Categorical(actions_dist)\n",
    "        action = d.sample()\n",
    "\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    current_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break"
   ],
   "id": "e303f76cb01fddd4",
   "outputs": [],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
