{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vanilla policy gradient algorithm from scratch",
   "id": "44c341a36a312921"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Steps:\n",
    "# 1. Create an env wrapper for continuous and discrete environments. For continuous environments, policy network needs to output parameters for a multivariate gaussian distribution.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import torch.distributions as dist\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, action_space_dim, is_continuous):\n",
    "        super().__init__()\n",
    "        self.is_continuous = is_continuous\n",
    "        self.state_dim = state_space_dim\n",
    "        self.action_space_dim = action_space_dim\n",
    "        self.fc1 = nn.Linear(state_space_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        # For continuous action dim, return mean and only diagonal entries on covariance matrix since action states are probably independent\n",
    "        self.fc3 = nn.Linear(64, 2 * action_space_dim if is_continuous else action_space_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        if self.is_continuous:\n",
    "            mean = x[..., :self.action_space_dim]\n",
    "            covar = torch.exp(x[..., self.action_space_dim:])\n",
    "            return mean, torch.diag(covar)\n",
    "        else:\n",
    "            return torch.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class BaselineVNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, max_timesteps):\n",
    "        super().__init__()\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.fc1 = nn.Linear(state_space_dim + 1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, timestep):\n",
    "        timestep = timestep.float() / self.max_timesteps\n",
    "        x = F.relu(self.fc1(torch.cat([state, timestep])))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "# Implement vpg alg\n",
    "iterations = 100\n",
    "max_timesteps = 500\n",
    "t_per_iter = 35\n",
    "env = gym.make('CartPole-v0')\n",
    "state_space_dim = 4\n",
    "action_space_dim = 2\n",
    "policy = PolicyNetwork(state_space_dim, action_space_dim, False)\n",
    "policy_opt = torch.optim.AdamW(policy.parameters(), lr=5e-4)\n",
    "baseline = BaselineVNetwork(state_space_dim, max_timesteps)\n",
    "baseline_opt = torch.optim.AdamW(baseline.parameters(), lr=1e-3)\n",
    "baseline_opt_steps = 10\n",
    "discount = 0.99"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training run",
   "id": "467ecbf12c62b3d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for _ in range(iterations):\n",
    "    # Collect a set of trajectories by executing the current policy\n",
    "    trajectories = []\n",
    "    for _ in range(t_per_iter):\n",
    "        current_state, _ = env.reset()\n",
    "        traj = torch.tensor([])\n",
    "        for _ in range(max_timesteps):\n",
    "            actions_dist = policy(torch.tensor(current_state))\n",
    "            if policy.is_continuous:\n",
    "                mean, covar = actions_dist\n",
    "                mgd = dist.MultivariateNormal(mean, covar)\n",
    "                action = mgd.sample()\n",
    "            else:\n",
    "                d = dist.Categorical(actions_dist)\n",
    "                action = d.sample()\n",
    "\n",
    "            next_state, reward, terminated, truncated = env.step(action)\n",
    "            traj.append((current_state, action, reward))\n",
    "            current_state = next_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        trajectories.append(traj)\n",
    "\n",
    "    # At each timestep in each trajectory, compute the return and advantage estimate\n",
    "    adv_ests = []\n",
    "    for t in range(t_per_iter):\n",
    "        traj = trajectories[t]\n",
    "        traj_adv_ests = torch.tensor([])\n",
    "        for time in range(len(traj)):\n",
    "            return_t = sum(discount**(t_prime - time) * traj[t_prime][2] for t_prime in range(t, len(traj)))\n",
    "            adv_est = return_t - baseline(traj[time][0], time)\n",
    "            traj_adv_ests.append(adv_est)\n",
    "        adv_ests.append(traj_adv_ests)\n",
    "\n",
    "    # Re-fit the baseline\n",
    "    for _ in range(baseline_opt_steps):\n",
    "        baseline_loss = (torch.cat(adv_ests) ** 2).sum()\n",
    "        baseline_opt.zero_grad()\n",
    "        baseline_loss.backward()\n",
    "        baseline_opt.step()\n",
    "\n",
    "    # Update the policy using a policy gradient estimate\n",
    "    g = torch.tensor([])\n",
    "    for traj_ind in range(len(trajectories)):\n",
    "        traj = trajectories[traj_ind]\n",
    "        for step_ind in range(len(traj)):\n",
    "            step_state, step_action, step_reward = traj[step_ind]\n",
    "            if policy.is_continuous:\n",
    "                mean, covar = policy(torch.tensor(step_state))\n",
    "                mvg = dist.MultivariateNormal(mean, covar)\n",
    "                log_pdf_val = mvg.log_prob(step_action)\n",
    "                g.append(log_pdf_val * adv_ests[traj_ind][step_ind])\n",
    "            else:\n",
    "                action_dist = policy(torch.tensor(step_state))\n",
    "                g.append(action_dist[step_action] * adv_ests[traj_ind][step_ind])\n",
    "    # Take only one gradient step for policy to avoid overfitting\n",
    "    policy_loss = g.sum()\n",
    "    policy_opt.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_opt.step()"
   ],
   "id": "b1fdc00cc63981f3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
