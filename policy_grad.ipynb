{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vanilla policy gradient algorithm from scratch (with a2c advantage estimation)",
   "id": "44c341a36a312921"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-11T23:51:13.593433Z",
     "start_time": "2025-08-11T23:51:13.576226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import torch.distributions as dist\n",
    "from enum import Enum\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, action_space_dim, is_continuous):\n",
    "        super().__init__()\n",
    "        self.is_continuous = is_continuous\n",
    "        self.state_dim = state_space_dim\n",
    "        self.action_space_dim = action_space_dim\n",
    "        self.fc1 = nn.Linear(state_space_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        # For continuous action dim, return mean and only diagonal entries on covariance matrix since action states are probably independent\n",
    "        self.fc3 = nn.Linear(64, 2 * action_space_dim if is_continuous else action_space_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        if self.is_continuous:\n",
    "            mean = x[..., :self.action_space_dim]\n",
    "            covar = torch.exp(x[..., self.action_space_dim:])\n",
    "            return mean, torch.diag(covar)\n",
    "        else:\n",
    "            return torch.softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "class BaselineVNetwork(nn.Module):\n",
    "    def __init__(self, state_space_dim, max_timesteps):\n",
    "        super().__init__()\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.fc1 = nn.Linear(state_space_dim + 1, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, timestep):\n",
    "        timestep = timestep.float() / self.max_timesteps\n",
    "        x = F.relu(self.fc1(torch.cat([state, timestep.unsqueeze(-1)])))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class AdvantageEstimation(Enum):\n",
    "    VPG = 0\n",
    "    # Use k=5 step lookahead\n",
    "    A2C = 1\n",
    "\n",
    "\n",
    "# Implement vpg alg\n",
    "iterations = 100\n",
    "max_timesteps = 500\n",
    "t_per_iter = 50\n",
    "env = gym.make('CartPole-v1', render_mode='human', max_episode_steps=max_timesteps)\n",
    "state_space_dim = 4\n",
    "action_space_dim = 2\n",
    "policy = PolicyNetwork(state_space_dim, action_space_dim, False)\n",
    "policy_opt = torch.optim.AdamW(policy.parameters(), lr=5e-4)\n",
    "baseline = BaselineVNetwork(state_space_dim, max_timesteps)\n",
    "baseline_opt = torch.optim.AdamW(baseline.parameters(), lr=1e-2)\n",
    "baseline_opt_steps = 10\n",
    "discount = 0.99\n",
    "adv_est_scheme = AdvantageEstimation.A2C"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training run",
   "id": "467ecbf12c62b3d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:51:18.947151Z",
     "start_time": "2025-08-11T23:51:14.343060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for iter in range(iterations):\n",
    "    # Collect a set of trajectories by executing the current policy\n",
    "    trajectories = []\n",
    "    for t_ind in range(t_per_iter):\n",
    "        current_state, _ = env.reset()\n",
    "        current_state = torch.tensor(current_state, dtype=torch.float32)\n",
    "        traj = []\n",
    "        for _ in range(max_timesteps):\n",
    "            actions_dist = policy(current_state)\n",
    "            if policy.is_continuous:\n",
    "                mean, covar = actions_dist\n",
    "                mgd = dist.MultivariateNormal(mean, covar)\n",
    "                action = mgd.sample()\n",
    "            else:\n",
    "                d = dist.Categorical(actions_dist)\n",
    "                action = d.sample()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            new_step = (current_state, action, reward)\n",
    "            traj.append(new_step)\n",
    "            current_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        trajectories.append(traj)\n",
    "\n",
    "\n",
    "    # At each timestep in each trajectory, compute the return and advantage estimate\n",
    "    def calc_returns():\n",
    "        if adv_est_scheme == AdvantageEstimation.VPG:\n",
    "            returns = [[sum(discount**(t_prime - time) * traj[t_prime][2] for t_prime in range(time, len(traj))) for time in range(len(traj))] for traj in trajectories]\n",
    "        else:\n",
    "            returns = []\n",
    "            for traj in trajectories:\n",
    "                traj_returns = []\n",
    "                for time in range(len(traj)):\n",
    "                    t_return = 0\n",
    "                    is_long_en = len(traj) > time + 5\n",
    "                    for t_prime in range(time, min(len(traj), time + 5)):\n",
    "                        t_return += discount**(t_prime - time) * traj[t_prime][2]\n",
    "\n",
    "                    if is_long_en:\n",
    "                        t_return += discount**5 * baseline(traj[time + 5][0], torch.tensor(time + 5, dtype=torch.float32))\n",
    "\n",
    "                    traj_returns.append(t_return)\n",
    "\n",
    "                returns.append(traj_returns)\n",
    "\n",
    "        return returns\n",
    "\n",
    "    def calc_baselines():\n",
    "       return [[baseline(traj[time][0], torch.tensor(time, dtype=torch.float32)) for time in range(len(traj))] for traj in trajectories]\n",
    "\n",
    "    def calc_adv_ests(returns):\n",
    "        baselines = calc_baselines()\n",
    "        return [[(returns[traj_ind][step_ind] - baselines[traj_ind][step_ind]) for step_ind in range(len(trajectories[traj_ind]))] for traj_ind in range(len(trajectories))]\n",
    "\n",
    "    def normalize_adv_ests(adv_ests):\n",
    "        flattened = torch.stack([a for t in adv_ests for a in t])\n",
    "        return [[(ae - flattened.mean()) / (flattened.std() + 1e-8) for ae in t_ae] for t_ae in adv_ests]\n",
    "\n",
    "    def calc_baseline_loss():\n",
    "        returns = calc_returns()\n",
    "        adv_ests = calc_adv_ests(returns)\n",
    "        adv_ests = [a for traj_a in adv_ests for a in traj_a]\n",
    "        return (torch.stack(adv_ests) ** 2).sum() / len(trajectories)\n",
    "\n",
    "    # Re-fit the baseline\n",
    "    last_baseline_loss = 0\n",
    "    for i in range(baseline_opt_steps):\n",
    "        baseline_loss = calc_baseline_loss()\n",
    "        last_baseline_loss = baseline_loss\n",
    "        # print(\"On baseline value opt step:\", i, \" with loss:\", baseline_loss)\n",
    "        baseline_opt.zero_grad()\n",
    "        baseline_loss.backward()\n",
    "        baseline_opt.step()\n",
    "\n",
    "    # Update the policy using a policy gradient estimate\n",
    "    returns = calc_returns()\n",
    "    adv_ests = normalize_adv_ests(calc_adv_ests(returns))\n",
    "    g = []\n",
    "    for traj_ind in range(len(trajectories)):\n",
    "        traj = trajectories[traj_ind]\n",
    "        for step_ind in range(len(traj)):\n",
    "            step_state, step_action, step_reward = traj[step_ind]\n",
    "            if policy.is_continuous:\n",
    "                mean, covar = policy(torch.tensor(step_state))\n",
    "                mvg = dist.MultivariateNormal(mean, covar)\n",
    "                log_pdf_val = mvg.log_prob(step_action)\n",
    "                g.append(log_pdf_val * adv_ests[traj_ind][step_ind])\n",
    "            else:\n",
    "                action_dist = policy(torch.tensor(step_state))\n",
    "                g.append(torch.log(action_dist[step_action]) * adv_ests[traj_ind][step_ind])\n",
    "    # Take only one gradient step for policy to avoid overfitting\n",
    "    policy_loss = -torch.stack(g).sum()\n",
    "    print(\"--------------------\")\n",
    "    print(\"Iteration:\", iter)\n",
    "    print(\"Policy loss:\", policy_loss)\n",
    "    print(\"Last baseline loss:\", last_baseline_loss)\n",
    "    print(\"Avg return:\", sum(r[0] for r in returns) / len(returns))\n",
    "    print(\"Avg steps:\", sum(len(t) for t in trajectories) / len(trajectories))\n",
    "    print(\"--------------------\")\n",
    "    policy_opt.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_opt.step()"
   ],
   "id": "b1fdc00cc63981f3",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[119]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     15\u001B[39m     d = dist.Categorical(actions_dist)\n\u001B[32m     16\u001B[39m     action = d.sample()\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m next_state, reward, terminated, truncated, _ = \u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m new_step = (current_state, action, reward)\n\u001B[32m     20\u001B[39m traj.append(new_step)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:125\u001B[39m, in \u001B[36mTimeLimit.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    113\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[32m    114\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[ObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    115\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[32m    116\u001B[39m \n\u001B[32m    117\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    123\u001B[39m \n\u001B[32m    124\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     observation, reward, terminated, truncated, info = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    126\u001B[39m     \u001B[38;5;28mself\u001B[39m._elapsed_steps += \u001B[32m1\u001B[39m\n\u001B[32m    128\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._elapsed_steps >= \u001B[38;5;28mself\u001B[39m._max_episode_steps:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:393\u001B[39m, in \u001B[36mOrderEnforcing.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    391\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._has_reset:\n\u001B[32m    392\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[33m\"\u001B[39m\u001B[33mCannot call env.step() before calling env.reset()\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m393\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/core.py:327\u001B[39m, in \u001B[36mWrapper.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    323\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstep\u001B[39m(\n\u001B[32m    324\u001B[39m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[32m    325\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[32m    326\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m327\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:285\u001B[39m, in \u001B[36mPassiveEnvChecker.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    283\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m.env, action)\n\u001B[32m    284\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m285\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:223\u001B[39m, in \u001B[36mCartPoleEnv.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m    220\u001B[39m     reward = -\u001B[32m1.0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sutton_barto_reward \u001B[38;5;28;01melse\u001B[39;00m \u001B[32m0.0\u001B[39m\n\u001B[32m    222\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mhuman\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m223\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    225\u001B[39m \u001B[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001B[39;00m\n\u001B[32m    226\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m np.array(\u001B[38;5;28mself\u001B[39m.state, dtype=np.float32), reward, terminated, \u001B[38;5;28;01mFalse\u001B[39;00m, {}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/rl-learning/venv/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:337\u001B[39m, in \u001B[36mCartPoleEnv.render\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    335\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mhuman\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    336\u001B[39m     pygame.event.pump()\n\u001B[32m--> \u001B[39m\u001B[32m337\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrender_fps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    338\u001B[39m     pygame.display.flip()\n\u001B[32m    340\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.render_mode == \u001B[33m\"\u001B[39m\u001B[33mrgb_array\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T22:21:21.994724Z",
     "start_time": "2025-08-08T06:42:48.081611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test implementation\n",
    "current_state, _ = env.reset()\n",
    "current_state = torch.tensor(current_state, dtype=torch.float32)\n",
    "for _ in range(max_timesteps):\n",
    "    actions_dist = policy(current_state)\n",
    "    if policy.is_continuous:\n",
    "        mean, covar = actions_dist\n",
    "        mgd = dist.MultivariateNormal(mean, covar)\n",
    "        action = mgd.sample()\n",
    "    else:\n",
    "        d = dist.Categorical(actions_dist)\n",
    "        action = d.sample()\n",
    "\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    current_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break"
   ],
   "id": "e303f76cb01fddd4",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Reflections\n",
    "\n",
    "**Normalizing advantage estimates**\n",
    "While training on cart pole, one problem I started encountering was that the policy was learning very very slowly and at one point seemed to be stagnating. After some investigation, I realized that when the policy would take trajectories that had a huge advantage estimate (positive or negative), that value would dominate the gradient step and the policy would be focused on fitting itself to that trajectory only, often overfitting due to very large gradients. To solve this, I normalized the advantage estimates and even though this mathematically isn't exactly the same anymore as the originall loss's gradient, it worked very well in practice.\n"
   ],
   "id": "9567f6c6ba1bebd5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
